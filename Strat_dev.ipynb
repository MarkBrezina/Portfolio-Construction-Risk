{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5553e3c7-bd0f-4fa2-adf6-dbd051ba9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Backtest.backtest as bb\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import pandas as pd\n",
    "import Backtest.account as acc\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdae8d-9fa3-4989-b325-24591756c246",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f3807d-b0de-4605-a8cd-22711dcd85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA_BUY(data):\n",
    "    # if short > long\n",
    "    # if RSI < 30\n",
    "    if data['RSI'] < 30:\n",
    "        if data['Close'] < data['MA20'] and data['Close'] < data['MA40']:\n",
    "            if data['Close'] > 1.01 * data['MA5']:\n",
    "                return(True)\n",
    "            elif data['Open'] > 1.01 * data['MA5']:\n",
    "                return(True)\n",
    "            elif data['High'] > 1.01 * data['MA5']:\n",
    "                return(True)\n",
    "            elif data['Low'] > 1.01 * data['MA5']:\n",
    "                return(True)\n",
    "            else:\n",
    "                return(False)\n",
    "        else:\n",
    "            return(False)\n",
    "    else:\n",
    "        return(False)\n",
    "    \n",
    "def MA_SELL(data):\n",
    "    # if short < long\n",
    "    if data['Close'] > data['MA20'] and data['Close'] > data['MA40']:\n",
    "        if data['Close'] < 0.99 * data['MA5']:\n",
    "            return(True)\n",
    "        elif data['Open'] < 0.99 * data['MA5']:\n",
    "            return(True)\n",
    "        elif data['High'] < 0.99 * data['MA5']:\n",
    "            return(True)\n",
    "        elif data['Low'] < 0.99 * data['MA5']:\n",
    "            return(True)\n",
    "        else:\n",
    "            return(False)\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e96417-5cac-4880-b830-023b2a36783e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Statisticals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625db8f-ff1f-4209-9af6-6ab62efbe4e5",
   "metadata": {},
   "source": [
    "* Which period are we in? Mean reverting or Trending?\n",
    "* With CLT, what are the probabilities of +1 or -1 for all the scenarios\n",
    "* What means and stds are there for each period?\n",
    "* Does entropy increase? Can it be measured?\n",
    "* Does supply/demand increase?\n",
    "\n",
    "a. probs på 3, 5, 7 dage\n",
    "b. levetid, entropi, general trend\n",
    "c. perioder og statistikker\n",
    "\n",
    "\n",
    "1. List of cryptocurrency to be checked with tests.\n",
    "2. Selection of start and end data to be used as period for checking with tests.\n",
    "3. Correlation of cryptocurrencies in pairs BTC to ETH, BTC to XRP and so on.\n",
    "4. Cointegration test of cryptocurrency pairs.\n",
    "---------------------------------------------------------------------------------\n",
    "5. Stationarity test on BTC, ETH, XRP etc. singularly \n",
    "6. Randomwalk test on BTC, ETH, etc. singularly\n",
    "7. Mean reversion test 1(Hurst exponent)\n",
    "8. Mean reversion test 2(ADfuller test)\n",
    "9. Linear trend test (MK test)\n",
    "10. Linear trend test(Cox Stuart test)\n",
    "11. Exponential Trend test (Abbe criterion)\n",
    "12. Exponential Trend test (Auto-correlation test?)\n",
    "---------------------------------------------------------------------------------\n",
    "13. Trend detection test x\n",
    "14. Mean reversion test y\n",
    "15. resistance levels test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dee01-0cbc-452d-82f3-e932b1082de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_coint(data):\n",
    "    correlations = data.corr()\n",
    "\n",
    "\n",
    "    co_int = pd.DataFrame(\"\", columns = data.columns, index = data.columns)\n",
    "\n",
    "    for i in range(len(data.columns)):\n",
    "\n",
    "        for j in range(len(data.columns)):\n",
    "\n",
    "            if a.perform_coint_test(data.iloc[:,i], data.iloc[:,j], False)[1] == False:\n",
    "                co_int.iloc[i,j] = \"False\"\n",
    "            elif a.perform_coint_test(data.iloc[:,i], data.iloc[:,j], False)[1] == True:\n",
    "                co_int.iloc[i,j] = \"True\"\n",
    "\n",
    "    couplings = []\n",
    "    for i in range(len(co_int.columns)):\n",
    "        for j in range(len(co_int)):\n",
    "\n",
    "            if i!=j:\n",
    "                if co_int.iloc[j,i] == \"True\" and correlations.iloc[j,i] > 0.8:\n",
    "                    couplings.append([co_int.columns[i], co_int.index[j]])\n",
    "    return(couplings)\n",
    "\n",
    "def corr_coint_list(lste, fra, til):\n",
    "    prices = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(lste)):\n",
    "        prices[lste[i]] = TMRW.DATA.data(lste[i],fra,til)['Close']\n",
    "\n",
    "    prices = prices.fillna(0)\n",
    "\n",
    "    lst = []\n",
    "    for i in range(len(prices.columns)):\n",
    "        if np.mean(prices.iloc[:,i]) == 0:\n",
    "            lst.append(i)\n",
    "\n",
    "    prices = prices.drop(prices.columns[lst], axis=1)\n",
    "\n",
    "    returns = prices.pct_change()#.dropna()\n",
    "    returns = returns.iloc[1:len(returns),:]\n",
    "    returns.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    returns = returns.fillna(0)\n",
    "\n",
    "    prices_list = corr_coint(prices)\n",
    "    returns_list = corr_coint(returns)\n",
    "\n",
    "    lst = []\n",
    "    for i in returns_list:\n",
    "        for j in prices_list:\n",
    "            if i == j:\n",
    "                lst.append(i)\n",
    "    return(lst)\n",
    "\n",
    "def stat_test(fra, til, lste, typ, win):\n",
    "    \n",
    "    prices = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(lste)):\n",
    "        prices[lste[i]] = TMRW.DATA.data(lste[i],fra,til)['Close']\n",
    "\n",
    "    prices = prices.fillna(0)\n",
    "\n",
    "    lst = []\n",
    "    for i in range(len(prices.columns)):\n",
    "        if np.mean(prices.iloc[:,i]) == 0:\n",
    "            lst.append(i)\n",
    "\n",
    "    prices = prices.drop(prices.columns[lst], axis=1)\n",
    "\n",
    "    returns = prices.pct_change()#.dropna()\n",
    "    returns = returns.iloc[1:len(returns),:]\n",
    "    returns.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    returns = returns.fillna(0)\n",
    "\n",
    "    if typ == \"prices\":\n",
    "        data = prices\n",
    "    elif typ == \"returns\":\n",
    "        data = returns\n",
    "    \n",
    "    df = []\n",
    "    for i in range(len(data.columns)):\n",
    "        if a.perform_adf_test(data.iloc[:,i])[2] == True:\n",
    "            b = \"Stationary\"\n",
    "        else:\n",
    "            b = \"Non-stationary\"\n",
    "\n",
    "        if a.perform_variance_ratio_test(data.iloc[:,i])[1] == True:\n",
    "            c = \"Random walk\"\n",
    "        else:\n",
    "            c = \"Not Random walk\"\n",
    "\n",
    "        if a.perform_hurst_exp_test(data.iloc[:,i])[0] < 0.25:\n",
    "            d = \"Mean reverting\"\n",
    "        elif a.perform_hurst_exp_test(data.iloc[:,i])[0] > 0.75:\n",
    "            d = \"Trending\"\n",
    "        else:\n",
    "            d = \"None\"\n",
    "\n",
    "        adf = adfuller(data.iloc[:,i], 1)\n",
    "\n",
    "        if adf[1] < 0.2 and adf[0] > (adf[4]['10%'] - adf[4]['1%'])/adf[4]['1%'] * 20: \n",
    "            d_ = \"Mean reverting\"\n",
    "\n",
    "        elif adf[1] < 0.1 and adf[0] > adf[4]['10%']:\n",
    "            d_ = \"not mean reverting\"\n",
    "\n",
    "        elif (adf[0] > dict(adf[4])['1%']) and (adf[0] > dict(adf[4])['5%']) and (adf[0] > dict(adf[4])['10%']):\n",
    "            d_ = \"not mean reverting\"\n",
    "\n",
    "        else:\n",
    "            d_ = \"Mean reverting\"\n",
    "\n",
    "        if mk_test(data.iloc[:,i], mode = 'simple', window = win, alpha = 0.05)[0] == False:\n",
    "            e = \"Non-Trending\"\n",
    "        elif mk_test(data.iloc[:,i], mode = 'simple', window = win, alpha = 0.05)[0] == True:\n",
    "            e = \"Trending\"\n",
    "\n",
    "        if cox_stuart(data.iloc[:,i], window = win, alpha = 0.00005)[0] == False:\n",
    "            f = \"Non-Trending\"\n",
    "        elif cox_stuart(data.iloc[:,i], window = win, alpha = 0.00005)[0] == True:\n",
    "            f = \"Trending\"\n",
    "\n",
    "        if abbe_criterion(data.iloc[:,i], window = win, alpha = 0.00005)[0] == False:\n",
    "            g = \"Non-Trending\"\n",
    "        elif abbe_criterion(data.iloc[:,i], window = win, alpha = 0.00005)[0] == True:\n",
    "            g = \"Trending\"\n",
    "\n",
    "        if autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == False:\n",
    "            h = \"Non-Trending\"\n",
    "        elif autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == True:\n",
    "            h = \"Trending\"\n",
    "\n",
    "\n",
    "\n",
    "        df.append([prices.columns[i],b,c,d,d_,e,f,g,h])\n",
    "    df = pd.DataFrame(df, columns = ['Symbol', 'Stationarity', 'Randomwalk(yes/no)', 'Hurst Exponent', 'ADfuller test', 'MK test', 'Cox Stuart', 'Abbe Criterion', 'Autocorrelation'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fed709-80ae-4fc5-9670-7273f8f71efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation og cointegration liste\n",
    "m1_c = pd.DataFrame(corr_coint_list(lste, m1, today))\n",
    "m3_c = pd.DataFrame(corr_coint_list(lste, m3, today))\n",
    "m6_c = pd.DataFrame(corr_coint_list(lste, m6, today))\n",
    "\n",
    "# stationarity, randomwalk, mean reversion check 1, mean reversion check 2, Linear trend, Linear Trend, Exponential trend?, auto-correlation?\n",
    "m1_s = stat_test(m1, today, lste, \"prices\", 15)\n",
    "m3_s = stat_test(m3, today, lste, \"prices\", 15)\n",
    "m6_s = stat_test(m6, today, lste, \"prices\", 15)\n",
    "\n",
    "\n",
    "\n",
    "# linear pairing mellem a og b currency\n",
    "# Are there resistance levels?\n",
    "# trend detection with returns mean\n",
    "# Trend detection with autocorrelation loss?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"E:/Investering/stats.xlsx\") as writer:\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            m1_c.to_excel(writer, sheet_name=\"1m corr coint\")\n",
    "        elif i == 1:\n",
    "            m1_s.to_excel(writer, sheet_name=\"1m stats\")\n",
    "        elif i == 2:\n",
    "            m3_c.to_excel(writer, sheet_name=\"3m corr coint\")\n",
    "        elif i == 3:\n",
    "            m3_s.to_excel(writer, sheet_name=\"3m stats\")\n",
    "        elif i == 4:\n",
    "            m6_c.to_excel(writer, sheet_name=\"6m corr coint\")\n",
    "        elif i == 5:\n",
    "            m6_s.to_excel(writer, sheet_name=\"6m stats\")\n",
    "            \n",
    "            \n",
    "# round(a.half_life_v2(x)/PERIOD_PER_DAY)\n",
    "\n",
    "#detect overall markov chain\n",
    "\n",
    "# detect overall mean reversion times\n",
    "\n",
    "# detect overall mean reversion sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfe769-d1d9-40dc-9bde-e57f999c3caf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ed3d1-f023-4a5b-8179-592024192bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "  \n",
    "# Create object \n",
    "driver = webdriver.Chrome() \n",
    "  \n",
    "# Assign URL \n",
    "url = \"https://x.com/VitalikButerin\"\n",
    "  \n",
    "# Fetching the Url \n",
    "driver.get(url) \n",
    "\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "#driver.find_element(By.ID, \"id__pho3k8o135\")\n",
    "element = driver.find_elements(By.XPATH, '/html/body/div[1]/div/div/div[2]/main/div/div/div/div/div/div[3]/div/div/section/div/div/div[1]/div/div/article/div/div/div[2]/div[2]/div[2]/div/span')\n",
    "element[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3fc8f6-dc95-4954-b87d-ad03f218c7ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mean reversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf7754-f07b-4935-bc80-a24a69465724",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8bb035-67f6-4fe6-8437-98707b91a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hver invertering af MA5 og MA21. Hvor lang tid går der så indtil vi rammer samme pris som inverteringsprisen?\n",
    "def mr_times(data):\n",
    "\n",
    "    lstma5_times = []\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if (data.iloc[i,17] > data.iloc[i,3] and data.iloc[i-1,17] <= data.iloc[i-1,3]):\n",
    "            lstma5_times.append(i)\n",
    "        elif (data.iloc[i,17] < data.iloc[i,3] and data.iloc[i-1,17] >= data.iloc[i-1,3]):\n",
    "            lstma5_times.append(i)\n",
    "\n",
    "    lst1 = []\n",
    "    for i in range(1, len(lstma5_times)):\n",
    "        lst1.append(lstma5_times[i] - lstma5_times[i-1])\n",
    "\n",
    "\n",
    "    lstma21_times = []\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if (data.iloc[i,27] > data.iloc[i,3] and data.iloc[i-1,27] < data.iloc[i-1,3]):\n",
    "            lstma21_times.append(i)\n",
    "        elif (data.iloc[i,27] < data.iloc[i,3] and data.iloc[i-1,27] > data.iloc[i-1,3]):\n",
    "            lstma21_times.append(i)\n",
    "\n",
    "    lst2 = []\n",
    "    for i in range(1, len(lstma21_times)):\n",
    "        lst2.append(lstma21_times[i] - lstma21_times[i-1])\n",
    "        \n",
    "    return([lst1,lst2])\n",
    "\n",
    "def ed(data, bins = 10):\n",
    "\n",
    "    bine = np.linspace(min(data), max(data), bins)\n",
    "    counts = []\n",
    "\n",
    "    for i in range(0,len(bine)):\n",
    "\n",
    "        bine[i] = round(bine[i],3)\n",
    "        count = 0\n",
    "\n",
    "        for j in range(len(data)):\n",
    "\n",
    "            if data.iloc[j] >= bine[i-1] and data.iloc[j] < bine[i]:\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "        counts.append(count/len(data))\n",
    "\n",
    "    counts_df = pd.DataFrame(counts, index = bine)\n",
    "    return(counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b0d6e1-e8e0-4d6b-812a-9c489c65359e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'edge'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01msns\u001b[39;00m\n\u001b[32m     29\u001b[39m sns.set()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01medge\u001b[39;00m\n\u001b[32m     32\u001b[39m help(edge)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01medge\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01medge_mean_reversion\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01ma\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'edge'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "# from tqdm import tnrange, notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Import the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.dates import MonthLocator\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import edge\n",
    "help(edge)\n",
    "\n",
    "import edge.edge_mean_reversion as a\n",
    "import edge.edge_risk_kit as erk\n",
    "\n",
    "def draw_pair_plot(data, figsize=(10,6)):\n",
    "\n",
    "    ts1 = data.iloc[:,0]\n",
    "    ts2 = data.iloc[:,1]\n",
    "    ts3 = data.iloc[:,2]\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=figsize)\n",
    "\n",
    "    ax1.plot(ts1.index, ts1.values, label = ts1.name)\n",
    "    ax2.plot(ts2.index, ts2.values, label = ts2.name, color = 'r')\n",
    "    ax3.plot(ts3.index, ts3.values, label = ts3.name, color = 'g')\n",
    "\n",
    "    ax1.set_ylabel(ts1.name)\n",
    "    ax2.set_ylabel(ts2.name)\n",
    "    ax3.set_ylabel(ts3.name)\n",
    "    \n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax3.grid()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_reg_line(x, y):\n",
    "    reg = np.polyfit(x, y, deg=1)\n",
    "    y_fitted = np.polyval(reg, x)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, 'bo', label='data')\n",
    "    plt.plot(x, y_fitted, 'r', lw=2.5, label='linear regression')\n",
    "    plt.legend(loc=0);\n",
    "    \n",
    "\n",
    "import yfinance as yf\n",
    "import TMRW\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.dates import MonthLocator\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) #today\n",
    "one = datetime(today.year-1,today.month,today.day) #one year ago\n",
    "three = datetime(today.year-3,today.month,today.day) #three years ago\n",
    "five = datetime(today.year-5,today.month,today.day) #five years ago\n",
    "ten = datetime(today.year-10,today.month,today.day) #ten years ago\n",
    "twenty = datetime(today.year-20,today.month,today.day) #twenty years ago\n",
    "\n",
    "PERIOD_PER_YEAR = 252 * 1\n",
    "PERIOD_PER_DAY = 1\n",
    "\n",
    "CUR = pd.read_excel(open('E:/Investering/list.xlsx', 'rb'),sheet_name='CUR')\n",
    "CUR = CUR[CUR['Type'] == \"CRYPTOCURRENCY\"]\n",
    "\n",
    "\n",
    "prices = pd.DataFrame()\n",
    "prices['ETH'] = TMRW.DATA.data(\"ETH-USD\",one,today)['Close']\n",
    "prices['ARB'] = TMRW.DATA.data(\"ARB11841-USD\",one,today)['Close']\n",
    "prices['BTC'] = TMRW.DATA.data(\"BTC-USD\",one,today)['Close']\n",
    "prices['ADA'] = TMRW.DATA.data(\"ADA-USD\",one,today)['Close']\n",
    "prices['SOL'] = TMRW.DATA.data(\"SOL-USD\",one,today)['Close']\n",
    "prices['TRON'] = TMRW.DATA.data(\"TRX-USD\",one,today)['Close']\n",
    "prices['NEAR'] = TMRW.DATA.data(\"NEAR-USD\",one,today)['Close']\n",
    "prices['XRP'] = TMRW.DATA.data(\"XRP-USD\",one,today)['Close']\n",
    "prices['LTC'] = TMRW.DATA.data(\"LTC-USD\",one,today)['Close']\n",
    "prices = prices.dropna()\n",
    "\n",
    "draw_pair_plot(prices[['BTC','ETH','XRP']], figsize=(12,6))    \n",
    "\n",
    "returns = prices.pct_change().dropna()\n",
    "returns.corr()\n",
    "\n",
    "sns.pairplot(data=returns, plot_kws = {'alpha': 0.5, 's': 2, 'edgecolor': 'b'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0eee4-d6c4-40fd-9740-745974d11035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.apply(lambda x: a.perform_adf_test(x), axis=0)\n",
    "prices.apply(lambda x: a.perform_hurst_exp_test(x), axis=0)\n",
    "prices.apply(lambda x: a.perform_variance_ratio_test(x), axis=0)\n",
    "prices.apply(lambda x: round(a.half_life_v2(x)/PERIOD_PER_DAY), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8b20c-3cd3-4303-8b81-1853d716552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.perform_coint_test(prices.BTC, prices.SOL, True) # this one\n",
    "\n",
    "\n",
    "X = prices.SOL\n",
    "Y = prices.ETH\n",
    "\n",
    "plot_reg_line(X, Y)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "def plot_reg_pair(independent, dependent, showplot=False):\n",
    "\n",
    "    model = sm.OLS(dependent, independent)\n",
    "    coeff = model.fit().params\n",
    "\n",
    "    if len(coeff) == 2:\n",
    "        hedge_ratio = coeff[1]\n",
    "        intercept = coeff[0]\n",
    "    else:\n",
    "        hedge_ratio = coeff[0]\n",
    "        intercept = 0\n",
    "\n",
    "    if showplot:\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "        ax.plot(independent, dependent, 'bo', label='data')\n",
    "        ax.plot(independent, independent*hedge_ratio+intercept, 'r', lw=2.5, label='linear regression')\n",
    "        ax.set_xlabel(X.name)\n",
    "        ax.set_ylabel(Y.name)\n",
    "\n",
    "        plt.legend(loc=0)\n",
    "        plt.show()\n",
    "    \n",
    "    return hedge_ratio\n",
    "\n",
    "hedge_ratio = plot_reg_pair(X, Y)\n",
    "hedge_ratio\n",
    "\n",
    "portf_2_assets = Y - hedge_ratio * X\n",
    "\n",
    "portf_2_assets.plot(figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf2a2e-85dc-4869-a4c2-cd1850aae293",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.perform_adf_test(portf_2_assets, True)\n",
    "\n",
    "a.perform_hurst_exp_test(portf_2_assets, True)\n",
    "\n",
    "a.perform_variance_ratio_test(portf_2_assets, 2, True)\n",
    "\n",
    "a.half_life(portf_2_assets)/PERIOD_PER_DAY\n",
    "\n",
    "a.perform_coint_test(X, Y, True)\n",
    "\n",
    "lookback = round(a.half_life(portf_2_assets))\n",
    "qty = -(portf_2_assets-portf_2_assets.rolling(lookback).mean())/portf_2_assets.rolling(lookback).std()\n",
    "\n",
    "position = portf_2_assets * qty\n",
    "position.plot(figsize=(12,6), title='Portfolio value over time');\n",
    "\n",
    "pnl = position.pct_change().dropna()\n",
    "pnl.plot(figsize=(12,6), title='Daily profit and loss');\n",
    "\n",
    "erk.drawdown(pnl).Wealth.plot(figsize=(12,6), title='Wealth Ratio');\n",
    "\n",
    "erk.drawdown(pnl).Drawdown.plot(figsize=(12,6), title='Drawdown');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa44eda-b2f4-4588-bdef-52ff2a224459",
   "metadata": {},
   "outputs": [],
   "source": [
    "erk.summary_stats(pnl.to_frame(), riskfree_rate=0.02, periods_per_year=PERIOD_PER_YEAR)\n",
    "\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "jres = coint_johansen(prices, det_order=0, k_ar_diff=1)\n",
    "\n",
    "coeff = jres.evec[:,0]\n",
    "\n",
    "portf_3_assets = (prices * coeff).sum(axis=1)\n",
    "\n",
    "portf_3_assets.plot(figsize=(12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af51c5-334e-470b-974a-068d5e736d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = position.pct_change().dropna()\n",
    "pnl.plot(figsize=(12,6), title='Daily Profit and Loss');\n",
    "\n",
    "\n",
    "bt_port = portf_3_assets.to_frame()\n",
    "bt_port.columns = ['Close']\n",
    "bt_port['Open'] = bt_port['Close']\n",
    "bt_port['High'] = bt_port['Close']\n",
    "bt_port['Low'] = bt_port['Close']\n",
    "\n",
    "\n",
    "def Z_Score(values, n):\n",
    "    \"\"\"\n",
    "    Return simple moving average of `values`, at\n",
    "    each step taking into account `n` previous values.\n",
    "    \"\"\"\n",
    "    series = pd.Series(values)\n",
    "    return (series - series.rolling(n).mean())/series.rolling(lookback).std()\n",
    "\n",
    "\n",
    "from backtesting import Strategy\n",
    "from backtesting.lib import crossover\n",
    "\n",
    "from backtesting import Backtest\n",
    "\n",
    "bt = Backtest(bt_port, Z_Score_Naive, cash=100, commission=.002)\n",
    "stats = bt.run()\n",
    "stats\n",
    "\n",
    "class Z_Score_Naive(Strategy):\n",
    "    \n",
    "    lookback = 30\n",
    "    threshold = 2\n",
    "    stoploss = 0.001\n",
    "    \n",
    "    def init(self):\n",
    "        self.ZScore = self.I(Z_Score, self.data.Close, self.lookback)\n",
    "    \n",
    "    def next(self):\n",
    "        if (self.position.is_long) & (self.ZScore > 0):\n",
    "            self.position.close()\n",
    "            \n",
    "        if (self.position.is_short) & (self.ZScore < 0):\n",
    "            self.position.close()\n",
    "        \n",
    "        if self.position.pl_pct < -self.stoploss:\n",
    "            self.position.close()   \n",
    "\n",
    "        if (self.ZScore < -self.threshold) & (~self.position.is_long):\n",
    "            self.position.close()\n",
    "            self.buy()\n",
    "\n",
    "        if (self.ZScore > self.threshold) & (~self.position.is_short):\n",
    "            self.position.close()\n",
    "            self.sell()\n",
    "            \n",
    "            \n",
    "bt = Backtest(bt_port, Z_Score_Naive, cash=100, commission=.002)\n",
    "stats = bt.run()\n",
    "stats\n",
    "\n",
    "\n",
    "%%time\n",
    "\n",
    "stats = bt.optimize(lookback = range(20, 40, 5),\n",
    "                    threshold = np.arange(2, 5.5, 0.5).tolist(),\n",
    "                    stoploss = np.arange(0.001, 0.005, 0.001).tolist(),\n",
    "                    maximize='Equity Final [$]')\n",
    "stats\n",
    "\n",
    "\n",
    "stats._strategy\n",
    "stats['_trades']\n",
    "\n",
    "bt.plot(plot_volume=False)\n",
    "\n",
    "stats = bt.optimize(lookback = range(20, 40, 5),\n",
    "                    threshold = np.arange(2, 5.5, 0.5).tolist(),\n",
    "                    stoploss = np.arange(0.001, 0.005, 0.001).tolist(),\n",
    "                    maximize = 'Max. Drawdown [%]')\n",
    "stats\n",
    "\n",
    "def neg_Volatility(stats):\n",
    "    return -stats['Volatility (Ann.) [%]']\n",
    "\n",
    "stats = bt.optimize(lookback = range(20, 40, 5),\n",
    "                    threshold = np.arange(2, 5.5, 0.5).tolist(),\n",
    "                    stoploss = np.arange(0.001, 0.005, 0.001).tolist(),\n",
    "                    maximize = neg_Volatility)\n",
    "\n",
    "stats = bt.optimize(lookback = range(20, 40, 5),\n",
    "                    threshold = np.arange(2, 5.5, 0.5).tolist(),\n",
    "                    stoploss = np.arange(0.001, 0.005, 0.001).tolist(),\n",
    "                    maximize = 'Sharpe Ratio')\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdeccf8-4cac-4f8a-a73e-40a04e28ae67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db74fec8-04ac-4fa6-970e-d78e014787b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    data = bb.data_intake(data)\n",
    "    signal = []\n",
    "    inv = 0\n",
    "    inv_cap = 1\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i < 20:\n",
    "            inv = 0\n",
    "            inv_cap = 1\n",
    "            pos = 0\n",
    "            signal.append([\"\", pos])\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            vol_pos = np.std(data['returns'][i-20:i])\n",
    "\n",
    "            if vol_pos < 0.02:\n",
    "                # inventory capacity should be 1\n",
    "                inv_cap = 1\n",
    "                pos = (inv_cap - inv)/10\n",
    "\n",
    "            elif vol_pos >= 0.02 and vol_pos < 0.06:\n",
    "                # inventory capacity should be 0.5\n",
    "                inv_cap = 0.5\n",
    "                pos = (inv_cap - inv)/6\n",
    "\n",
    "            elif vol_pos >= 0.06 and vol_pos < 0.09:\n",
    "                # inventory capacity should be 0.1\n",
    "                inv_cap = 0.25\n",
    "                pos = (inv_cap - inv)/2\n",
    "\n",
    "            else:\n",
    "                # inventory capacity should be 0\n",
    "                inv_cap = 0.1\n",
    "                pos = inv_cap - inv\n",
    "                \n",
    "            \n",
    "            \n",
    "            if MA_BUY(data.iloc[i]) == True:\n",
    "                if pos > 0:\n",
    "                    signal.append([\"buy\", pos])\n",
    "                elif pos <= 0:\n",
    "                    signal.append([\"\",0])\n",
    "                \n",
    "                inv = inv + pos\n",
    "\n",
    "            elif MA_SELL(data.iloc[i]) == True:\n",
    "                if pos > 0:\n",
    "                    signal.append([\"sell\",abs(pos)])\n",
    "                else:\n",
    "                    signal.append([\"\",0])\n",
    "                \n",
    "                inv = inv + pos\n",
    "\n",
    "            else:\n",
    "                signal.append([\"\",0])  \n",
    "                inv = inv\n",
    "    \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\",\"Position\"])\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810084b-1a23-4014-859a-cee829005d26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Trend following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc5a68-4318-46da-ad23-9f690a00ec62",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9c3c8-8a76-47fc-b2d5-c9758d4f4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.dates import MonthLocator\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import yfinance as yf\n",
    "from scipy import integrate\n",
    "import random\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# from tqdm import tnrange, notebook\n",
    "from tqdm.notebook import tqdm\n",
    "from dateutil import parser\n",
    "\n",
    "import edge\n",
    "import edge.edge_mean_reversion as a\n",
    "import edge.edge_risk_kit as erk\n",
    "import TMRW\n",
    "\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#import talib.abstract as ta\n",
    "#import freqtrade.vendor.qtpylib.indicators as qtpylib\n",
    "\n",
    "\n",
    "# dates in correct format for yahoo finance.\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) #today\n",
    "m1 = datetime(today.year,today.month-1,today.day)\n",
    "m3 = datetime(today.year,today.month-3,today.day)\n",
    "m6 = datetime(today.year,today.month-5,today.day)\n",
    "one = datetime(today.year-1,today.month,today.day) #one year ago\n",
    "three = datetime(today.year-3,today.month,today.day) #three years ago\n",
    "five = datetime(today.year-5,today.month,today.day) #five years ago\n",
    "ten = datetime(today.year-10,today.month,today.day) #ten years ago\n",
    "twenty = datetime(today.year-20,today.month,today.day) #twenty years ago\n",
    "\n",
    "PERIOD_PER_YEAR = 252 * 1\n",
    "PERIOD_PER_DAY = 1\n",
    "\n",
    "CUR = pd.read_excel(open('E:/Investering/Currencies.xlsx', 'rb'),sheet_name='Currencies')\n",
    "#CUR = CUR[CUR['Type'] == \"CRYPTOCURRENCY\"]\n",
    "CUR = list(CUR[CUR['BSYMBOL'].notnull()]['SYMBOL'])\n",
    "lste = CUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f236320-e0d6-4f3f-a45a-a942a53f46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.stats import norm, mstats\n",
    "\n",
    "\n",
    "def mk_test(ts, mode = 'simple', window = 50, alpha = 0.00001):\n",
    "    \"\"\"\n",
    "    Mann-Kendall test for trend\n",
    "    Detects linear trends (in pair with cox_stuart)\n",
    "    Optimal window = 50\n",
    "\n",
    "    Input:\n",
    "        x:   a vector of data\n",
    "        alpha: significance level (0.001 default)\n",
    "        window: last n values, for finding trend\n",
    "        mode: \n",
    "            - 'full' - if you need to know trend direction; \n",
    "            - 'simple' - if just trend existance.\n",
    "\n",
    "    Output:\n",
    "        trend: tells the trend (increasing, decreasing or no trend)\n",
    "        h: True (if trend is present) or False (if trend is absence)\n",
    "        p: p value of the significance test\n",
    "        z: normalized test statistics \n",
    "\n",
    "    \"\"\"\n",
    "    x = ts[-window:]\n",
    "    n = len(x)\n",
    "\n",
    "    # calculate S \n",
    "    s = 0\n",
    "    for k in range(n-1):\n",
    "        for j in range(k+1,n):\n",
    "            s += np.sign(x[j] - x[k])\n",
    "    #s = [-1 if x[j] < x[k] else 1 for j in xrange(k+1,n) for k in xrange(n-1)]\n",
    "\n",
    "    # calculate the unique data\n",
    "    unique_x = np.unique(x)\n",
    "    g = len(unique_x)\n",
    "    # calculate the var(s)\n",
    "    n = float(n)\n",
    "    if n == g: # there is no tie\n",
    "        var_s = (n*(n-1)*(2*n+5))/18\n",
    "        #print (var_s)\n",
    "    else: # there are some ties in data\n",
    "        tp = np.zeros(unique_x.shape)\n",
    "        for i in range(len(unique_x)):\n",
    "            tp[i] = sum(unique_x[i] == x)\n",
    "        var_s = (n*(n-1)*(2*n+5) + np.sum(tp*(tp-1)*(2*tp+5)))/18\n",
    "\n",
    "    if s>0:\n",
    "        z = (s - 1)/np.sqrt(var_s)\n",
    "    elif s == 0:\n",
    "        z = 0\n",
    "    elif s<0:\n",
    "        z = (s + 1)/np.sqrt(var_s)\n",
    "\n",
    "    # calculate the p_value\n",
    "    p = 2*(1-norm.cdf(abs(z))) # two tail test\n",
    "\n",
    "    h = abs(z) > norm.ppf(1-alpha/2) \n",
    "\n",
    "    if (z<0) and h:\n",
    "        trend = 'decreasing'\n",
    "    elif (z>0) and h:\n",
    "        trend = 'increasing'\n",
    "    else:\n",
    "        trend = 'no trend'\n",
    "    if mode == 'full':\n",
    "        return trend, h, p, z\n",
    "    else:\n",
    "        return h, abs(z)\n",
    "\n",
    "\n",
    "def cox_stuart(timeseries, window = 50, alpha = 0.0001, debug = False):\n",
    "    \"\"\"\n",
    "    Cox-Stuart criterion\n",
    "    H0: trend exists\n",
    "    H1: otherwise\n",
    "\n",
    "    Detects linear trends\n",
    "    Optimal window = 50\n",
    "    \"\"\"\n",
    "    n = window\n",
    "    idx = np.arange(1,n+1)\n",
    "    X = pandas.Series(timeseries[-n:], index=idx)\n",
    "    \n",
    "    S1 = [(n-2*i) if X[i] <= X[n-i+1] else 0 for i in range(1,n//2)]\n",
    "    n = float(n)\n",
    "    S1_ = (sum(S1) - n**2 / 8) / math.sqrt(n*(n**2-1)/24)\n",
    "    u = norm.ppf(1-alpha/2)\n",
    "    if debug:\n",
    "        print ('|S1*|:', abs(S1_))\n",
    "        print (\"u:\",u)\n",
    "    \n",
    "    return abs(S1_) > u, abs(S1_) #H0 accept\n",
    "\n",
    "def abbe_criterion(timeseries, window = 40, alpha = .00001, debug = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Abbe-Linnik criterion\n",
    "    Detects exponential trend (in pair with autocorrelation)\n",
    "    If window >50 => too much True\n",
    "    \"\"\"\n",
    "    if len(timeseries) < window:\n",
    "        return False\n",
    "    series = pandas.Series(timeseries)\n",
    "    a = series[-window:]\n",
    "    mean = a.mean()\n",
    "    \n",
    "\n",
    "    X = zip(a[:-1],a[1:])\n",
    "    s1 = sum([math.pow((x[0]-x[1]),2) for x in X])\n",
    "    s2 = sum([pow((x-mean),2) for x in a])\n",
    "    q = 0.5 * s1 / s2\n",
    "    q_a = 0.6814\n",
    "    n = len(a)\n",
    "    Q = -(1-q)*math.sqrt((2*n+1)/(2-pow((1-q),2)))\n",
    "    u = norm.ppf(alpha) #kobzar page 26\n",
    "    if debug:\n",
    "        print ('mean:', mean)\n",
    "        print ('q:', q)\n",
    "        print ('Q*:', Q)\n",
    "        print ('U_alpha-1:', u)\n",
    "    return Q < u, abs(Q)\n",
    "\n",
    "\n",
    "def autocorrelation(timeseries, window = 50, alpha = .00001, debug = False):\n",
    "    \"\"\"\n",
    "    Detects exponential trend\n",
    "\n",
    "    \"\"\"\n",
    "    n = window\n",
    "    idx = np.arange(1,n+1)\n",
    "    series = pandas.Series(list(timeseries[-n:]), index=idx)\n",
    "\n",
    "    X = zip(series[:-1],series[1:])\n",
    "    sqr_sum = pow(sum(series),2)\n",
    "    n_f = float(n)\n",
    "    r = (n_f * sum([x[0]*x[1] for x in X]) - sqr_sum + n_f*series[1]*series[n]) / \\\n",
    "        (n_f * sum([x**2 for x in series]) - sqr_sum)\n",
    "    \n",
    "    r_ = abs(r + 1./(n_f-1.)) / math.sqrt(n_f*(n_f-3)/(n_f+1)/pow((n_f-1),2))\n",
    "    u = norm.ppf(1-alpha/2)\n",
    "    \n",
    "    return abs(r_) > u, abs(r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54583a88-2869-478c-928f-0303341edeba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02998471-9165-48df-a798-9b171be7b592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50f42f-d799-423c-925e-1ca3dfbda485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10312f83-ccf7-479c-9ad8-bf0a48701ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbf4c0-388c-4f6d-8bd8-df2f243b8745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4e23b2b-1cf7-4b32-a3e2-18a96464a123",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afb818df-59e6-4179-b524-61165a0dd079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "\n",
    "    data = bb.data_intake(data)\n",
    "    signal = []\n",
    "    inv = 0\n",
    "    inv_cap = 1\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i < 20:\n",
    "            inv = 0\n",
    "            inv_cap = 1\n",
    "            pos = 0\n",
    "            signal.append([\"\", pos])\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            vol_pos = np.std(data['returns'][i-20:i])\n",
    "\n",
    "            if vol_pos < 0.02:\n",
    "                # inventory capacity should be 1\n",
    "                inv_cap = 1\n",
    "                pos = (inv_cap - inv)/10\n",
    "\n",
    "            elif vol_pos >= 0.02 and vol_pos < 0.06:\n",
    "                # inventory capacity should be 0.5\n",
    "                inv_cap = 0.5\n",
    "                pos = (inv_cap - inv)/6\n",
    "\n",
    "            elif vol_pos >= 0.06 and vol_pos < 0.09:\n",
    "                # inventory capacity should be 0.1\n",
    "                inv_cap = 0.25\n",
    "                pos = (inv_cap - inv)/2\n",
    "\n",
    "            else:\n",
    "                # inventory capacity should be 0\n",
    "                inv_cap = 0.1\n",
    "                pos = inv_cap - inv\n",
    "\n",
    "            \n",
    "            \n",
    "            if data['acceleration'][i] < -10:\n",
    "                if pos > 0:\n",
    "                    signal.append([\"buy\", pos])\n",
    "                elif pos <= 0:\n",
    "                    signal.append([\"sell\",abs(pos)])\n",
    "                \n",
    "                inv = inv + pos\n",
    "\n",
    "            elif data['acceleration'][i] > 10:\n",
    "                if pos > 0:\n",
    "                    signal.append([\"sell\",abs(pos)])\n",
    "                elif pos < 0:\n",
    "                    signal.append([\"sell\",abs(pos)*1.1])\n",
    "                \n",
    "                inv = inv + pos\n",
    "\n",
    "            else:\n",
    "                signal.append([\"\",0])  \n",
    "                inv = inv\n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\",\"Position\"])\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7a75e-b46c-48f4-a518-b02bee20d3b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SQRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9fcd94a-1a02-4c67-9cba-a12a0bc4b0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    data = bb.data_intake(data)\n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if np.mean(data['acceleration'][i-10:i]) > 1.5 and np.mean(data[\"returns\"][i-20:i]) > 0.001:\n",
    "            signal.append(\"buy\")\n",
    "\n",
    "        elif np.mean(data['acceleration'][i-10:i]) < -1.5 and np.mean(data[\"returns\"][i-20:i]) < -0.001:\n",
    "            signal.append(\"sell\")\n",
    "            \n",
    "        elif i == len(data)-2:\n",
    "            signal.append(\"sell\")\n",
    "\n",
    "        else:\n",
    "            signal.append(\"\")  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\"])\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3d3cf-118c-41f4-baab-3908357c8d09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Idiot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6381ab5-a922-4784-a3da-05e79ba36713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c25335-8458-4d38-9169-107bd9bfd12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    data = bb.data_intake(data)\n",
    "    signal = []\n",
    "    price = 50\n",
    "    t = 0\n",
    "    b = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if (data['Close'][i] <=  0.95 * price) or t >= 30 and b == False:\n",
    "            signal.append(\"buy\")\n",
    "            price = data['Close'][i]\n",
    "            t = 0\n",
    "            b = True\n",
    "\n",
    "        elif (data['Close'][i] >= 1.05 * price) and t >= 30 and b == True:\n",
    "            signal.append(\"sell\")\n",
    "            price = data['Close'][i]\n",
    "            t = 0\n",
    "            b = False\n",
    "            \n",
    "        elif i == len(data)-2:\n",
    "            signal.append(\"sell\")\n",
    "\n",
    "        else:\n",
    "            signal.append(\"\")\n",
    "        t = t + 1\n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\"])\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f2c7e-8f00-461a-8f63-840881477c67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4768af3e-3e36-4cfc-8697-8dd0c1bf4f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    data = bb.data_intake(data)\n",
    "    signal = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i > 250:\n",
    "        \n",
    "            if data['Close'][i] <= 1.1 * min(data['Close'][i-250:i]):\n",
    "                signal.append(\"buy\")\n",
    "\n",
    "            elif data['Close'][i] >= 0.9* max(data['Close'][i-250:i]):\n",
    "                signal.append(\"sell\")\n",
    "\n",
    "            elif i == len(data)-2:\n",
    "                signal.append(\"sell\")\n",
    "\n",
    "            else:\n",
    "                signal.append(\"\")\n",
    "                \n",
    "        else:\n",
    "            signal.append(\"\")\n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\"])\n",
    "    \n",
    "    return(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031294c-9d48-4b89-a6ca-05f03a3b7ddf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d1862-87e6-4cda-b6a7-bf3c642b2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if np.sum(data['PN_counter'][i-100:i]) > 5:\n",
    "            signal.append([\"aa\",0.004])\n",
    "            \n",
    "        elif np.sum(data['PN_counter'][i-100:i]) > 0 and np.sum(data['PN_counter'][i-100:i]) < 2:\n",
    "            signal.append([\"a\",0.004])\n",
    "            \n",
    "        elif np.sum(data['PN_counter'][i-100:i]) < 0 and np.sum(data['PN_counter'][i-100:i]) > -1:\n",
    "            signal.append([\"b\",0.004])\n",
    "            \n",
    "        elif np.sum(data['PN_counter'][i-100:i]) < -1 and np.sum(data['PN_counter'][i-100:i]) > -2:\n",
    "            signal.append([\"c\",0.004])\n",
    "\n",
    "        elif np.sum(data['PN_counter'][i-100:i]) < -2:\n",
    "            signal.append([\"cc\",0.004])\n",
    "\n",
    "        else:\n",
    "            signal.append([\"\",0])  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\", \"Position\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef147cf-bbfc-46f6-8d35-685639237b5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56dde7e-eb16-40d0-9e82-6486dae38ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if np.sum(data['PN_counter'][i-20:i]) > 5:\n",
    "            signal.append(\"buy\")\n",
    "\n",
    "        elif np.sum(data['PN_counter'][i-20:i]) < -5:\n",
    "            signal.append(\"sell\")\n",
    "\n",
    "        else:\n",
    "            signal.append(\"\")  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c895b-ee97-4d65-bb56-d02edfe2fbfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3a691-f474-43bf-a88b-578e4307fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if np.mean(data['acceleration'][i-20:i]) > 2:\n",
    "            signal.append(\"buy\")\n",
    "\n",
    "        elif np.mean(data['acceleration'][i-20:i]) < -2:\n",
    "            signal.append(\"sell\")\n",
    "\n",
    "        else:\n",
    "            signal.append(\"\")  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a34e02-a228-411c-a093-547285d23e19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8dfac-948f-4c48-8403-84b946d84487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # dy^2 > 0 and dy < 0 - upwards trend coming\n",
    "        # dy^2 > 0 and dy > 0 - on the upwards trend\n",
    "        \n",
    "        # dy^2 < 0 and dy < 0 - on the downwards trend\n",
    "        # dy^2 < 0 and dy > 0 - downwards trend coming\n",
    "        \n",
    "        # issue is false positives, noise and random movements.\n",
    "        # we need a third condition to remove \"false news\"\n",
    "        \n",
    "        if np.mean(data['acceleration'][i-5:i]) > 0 and np.mean(data['returns'][i-20:i]) < -0.0005 and data['Volume'][i] < np.mean(data['Volume'][i-20:i]):\n",
    "            signal.append([\"buy\",0.5])\n",
    "\n",
    "        elif np.mean(data['acceleration'][i-5:i]) < -0.5 and np.mean(data['returns'][i-20:i]) > 0.005 and data['Volume'][i] > np.mean(data['Volume'][i-20:i]):\n",
    "            signal.append([\"sell\",0.5])\n",
    "\n",
    "        else:\n",
    "            signal.append([\"\",0])  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\", \"Position\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099e5b9-4c4e-48e8-914c-573b1c2307e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2810337-6dc9-4d09-9ad2-db87fcdc2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # dy^2 > 0 and dy < 0 - upwards trend coming\n",
    "        # dy^2 > 0 and dy > 0 - on the upwards trend\n",
    "        \n",
    "        # dy^2 < 0 and dy < 0 - on the downwards trend\n",
    "        # dy^2 < 0 and dy > 0 - downwards trend coming\n",
    "        \n",
    "        # issue is false positives, noise and random movements.\n",
    "        # we need a third condition to remove \"false news\"\n",
    "        \n",
    "        if i == 1 or i == 330 or i == 460 or i == 620 or i == 830 or i == 900 or i == 990 or i == 1035 or i == 1070 or i == 1215 or i == 1275 or i == 1370 or i == 1435 or i == 1600:\n",
    "            signal.append([\"buy\",0.9])\n",
    "\n",
    "        elif i == 310 or i == 442 or i == 600 or i == 750 or i == 845 or i == 935 or i == 1020 or i == 1065 or i == 1170 or i == 1250 or i == 1350 or i == 1415 or i == 1550 or i == 1670:\n",
    "            signal.append([\"sell\",0.9])\n",
    "\n",
    "        else:\n",
    "            signal.append([\"\",0])  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\", \"Position\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c78f2d-d1a4-4f5b-8f58-2b1ec27e4f2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7fb13-25bf-401f-aa8b-118fedca6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strat(data):\n",
    "    \n",
    "    signal = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # dy^2 > 0 and dy < 0 - upwards trend coming\n",
    "        # dy^2 > 0 and dy > 0 - on the upwards trend\n",
    "        \n",
    "        # dy^2 < 0 and dy < 0 - on the downwards trend\n",
    "        # dy^2 < 0 and dy > 0 - downwards trend coming\n",
    "        \n",
    "        # issue is false positives, noise and random movements.\n",
    "        # we need a third condition to remove \"false news\"\n",
    "        \n",
    "        if (data['Close'][i] - np.mean(data['Close'][i-20:i]))/np.mean(data['Close'][i-20:i]) < -0.005 and np.mean(data['acceleration'][i-10:i]) > 0 and np.mean(data['returns'][i-10:i]) < 0.01 and data['returns'][i] > 0:\n",
    "            signal.append([\"buy\",0.9])\n",
    "\n",
    "        elif (data['Close'][i] - np.mean(data['Close'][i-20:i]))/np.mean(data['Close'][i-20:i]) > 0.05 and np.mean(data['acceleration'][i-10:i]) < 0 and np.mean(data['returns'][i-10:i]) > 0:\n",
    "            signal.append([\"sell\",0.9])\n",
    "\n",
    "        else:\n",
    "            signal.append([\"\",0])  \n",
    "    \n",
    "    signal = pd.DataFrame(signal, columns = [\"strategy\", \"Position\"])\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "a = _strat(data)\n",
    "strat_plot(data, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff14a2-340f-44fd-be00-ee12b22eb2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872170f-6829-4299-9f3f-7418858587a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e1dd0d-d890-4f41-95a4-8f75e8ba0a09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Østerbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2480f-4bf6-4f51-9aeb-541fcdd89881",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bollinger RSI strength\n",
    "mid band = 20 day rolling mean\n",
    "std = 20 day rolling std\n",
    "upper band = mid + 2*std\n",
    "lower band = mid - 2*std\n",
    "\n",
    "### entry\n",
    "1. version 1\n",
    "if rsi < 30\n",
    "if slowk < 20\n",
    "if bb_lowerband > close\n",
    "if CDLHAMMER == 100\n",
    "\n",
    "2. version 2\n",
    "closeprice > upper band\n",
    "\n",
    "\n",
    "### exit\n",
    "1. version 1\n",
    "if SAR > close\n",
    "if fisher_rsi > 0.3\n",
    "\n",
    "2. version 2\n",
    "if closeprice < bottom band\n",
    "\n",
    "## Mean reversion\n",
    "when the short moving average is larger than long moving average, we long and hold\n",
    "when the short moving average is smaller than long moving average, we clear positions\n",
    "the logic behind this is that the momentum has more impact on short moving average\n",
    "we can subtract short moving average from long moving average\n",
    "the difference between is sometimes positive, it sometimes becomes negative\n",
    "thats why it is named as moving average converge/diverge oscillator\n",
    "\n",
    "\n",
    "sma long - \n",
    "sma short -\n",
    "\n",
    "if statesma >0 and lastsma < 0 and position <0\n",
    "then buy\n",
    "\n",
    "if statesma <0 and lastsma >0 and position >0 then\n",
    "sell\n",
    "\n",
    "\n",
    "def signal_generation(df,method):\n",
    "    \n",
    "    signals=method(df)\n",
    "    signals['positions']=0\n",
    "\n",
    "    #positions becomes and stays one once the short moving average is above long moving average\n",
    "    signals['positions'][ma1:]=np.where(signals['ma1'][ma1:]>=signals['ma2'][ma1:],1,0)\n",
    "\n",
    "    #as positions only imply the holding\n",
    "    #we take the difference to generate real trade signal\n",
    "    signals['signals']=signals['positions'].diff()\n",
    "\n",
    "    #oscillator is the difference between two moving average\n",
    "    #when it is positive, we long, vice versa\n",
    "    signals['oscillator']=signals['ma1']-signals['ma2']\n",
    "\n",
    "    return signals\n",
    "\n",
    "\n",
    "## Breakout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Momentum/Trend\n",
    "\n",
    "### Version 1.\n",
    "Average Excess Returns is positive\n",
    "is excess return of asset over 12 months positive or negative. \n",
    "position size is inversely proportional to vlatility\n",
    "univariate garch model is used to estimate to model future volatility\n",
    "\n",
    "go long 3 currencies with highest 12 month momentum against btc\n",
    "go short 3 currencies with lowest 12 month momentum against btc\n",
    "\n",
    "filter rules and moving average rules\n",
    "filter, if we're already 1 or 2 percent up or down.\n",
    "if short-term ma crosses long-term ma.\n",
    "\n",
    "regression on excess returns with formula = r*x_t+1 = i_t - i_t - log(spot_rate_t+1)\n",
    "\n",
    "\n",
    "### Version 2.\n",
    "Hull moving average\n",
    "Fisher RSI\n",
    "CCI\n",
    "\n",
    "BUY\n",
    "If HMA < HMA.shift(1)?\n",
    "CCI < -50\n",
    "Fisher RSI < -0.5\n",
    "\n",
    "SELL\n",
    "If HMA > HMA.shift(1)?\n",
    "CCI > 100\n",
    "Fisher RSI > 0.5\n",
    "\n",
    "\n",
    "\n",
    "### Version 3.\n",
    "True range\n",
    "Average True Range = True range.ewm(alpha = 1 / period) mean\n",
    "\n",
    "basic upper bands = high + low / 2 + multiplier * ATR\n",
    "basic lower band = high + low / 2 - multiplier * ATR\n",
    "\n",
    "supertrend(dataframe, 3, 12)[\"STX\"]\n",
    "supertrend(dataframe, 1, 10)[\"STX\"]\n",
    "supertrend(dataframe, 2, 11)[\"STX\"]\n",
    "buy if all indicate up\n",
    "sell if all indicate down\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Version 4.\n",
    "has x gained 4% since previous day's close?\n",
    "is MACD positive and increasing?\n",
    "Is volume strong enough for a trade?\n",
    "\n",
    "##\n",
    "sells at stoploss or traget price?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Risks\n",
    "check for risks, exchange rate stability, check on idiosyncratic volatility HML FX factor. we find low idiosyncratic volatility currencties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f7bd5-ed6e-48ca-8c42-1922856ff9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    df = dataframe.copy()\n",
    "\n",
    "    df['TR'] = ta.TRANGE(df)\n",
    "    df['ATR'] = df['TR'].ewm(alpha=1 / period).mean()\n",
    "\n",
    "    # atr = 'ATR_' + str(period)\n",
    "    st = 'ST_' + str(period) + '_' + str(multiplier)\n",
    "    stx = 'STX_' + str(period) + '_' + str(multiplier)\n",
    "\n",
    "    # Compute basic upper and lower bands\n",
    "    df['basic_ub'] = (df['high'] + df['low']) / 2 + multiplier * df['ATR']\n",
    "    df['basic_lb'] = (df['high'] + df['low']) / 2 - multiplier * df['ATR']\n",
    "\n",
    "    # Compute final upper and lower bands\n",
    "    df['final_ub'] = 0.00\n",
    "    df['final_lb'] = 0.00\n",
    "    for i in range(period, len(df)):\n",
    "        df['final_ub'].iat[i] = df['basic_ub'].iat[i] if df['basic_ub'].iat[i] < df['final_ub'].iat[i - 1] or df['close'].iat[i - 1] > df['final_ub'].iat[i - 1] else df['final_ub'].iat[i - 1]\n",
    "        df['final_lb'].iat[i] = df['basic_lb'].iat[i] if df['basic_lb'].iat[i] > df['final_lb'].iat[i - 1] or df['close'].iat[i - 1] < df['final_lb'].iat[i - 1] else df['final_lb'].iat[i - 1]\n",
    "\n",
    "    # Set the Supertrend value\n",
    "    df[st] = 0.00\n",
    "    for i in range(period, len(df)):\n",
    "        df[st].iat[i] = df['final_ub'].iat[i] if df[st].iat[i - 1] == df['final_ub'].iat[i - 1] and df['close'].iat[i] <= df['final_ub'].iat[i] else \\\n",
    "            df['final_lb'].iat[i] if df[st].iat[i - 1] == df['final_ub'].iat[i - 1] and df['close'].iat[i] > df['final_ub'].iat[i] else \\\n",
    "                df['final_lb'].iat[i] if df[st].iat[i - 1] == df['final_lb'].iat[i - 1] and df['close'].iat[i] >= df['final_lb'].iat[i] else \\\n",
    "                    df['final_ub'].iat[i] if df[st].iat[i - 1] == df['final_lb'].iat[i - 1] and df['close'].iat[i] < df['final_lb'].iat[i] else 0.00\n",
    "\n",
    "    # Mark the trend direction up/down\n",
    "    df[stx] = np.where((df[st] > 0.00), np.where((df['close'] < df[st]), 'down', 'up'), np.NaN)\n",
    "\n",
    "    def FineSelectionFunction(self, fine):\n",
    "        fine = [x for x in fine if x.MarketCap != 0 and x.CompanyReference.IsREIT != 1 and  \\\n",
    "                    ((x.SecurityReference.ExchangeId == \"NYS\") or (x.SecurityReference.ExchangeId == \"NAS\") or (x.SecurityReference.ExchangeId == \"ASE\"))]\n",
    "                    \n",
    "        # if len(fine) > self.coarse_count:\n",
    "        #     sorted_by_market_cap = sorted(fine, key = lambda x: x.MarketCap, reverse=True)\n",
    "        #     top_by_market_cap = [x.Symbol for x in sorted_by_market_cap[:self.coarse_count]]\n",
    "        # else:\n",
    "        #     top_by_market_cap = [x.Symbol for x in fine]\n",
    "        top_by_market_cap = [x.Symbol for x in fine]\n",
    "        \n",
    "        momentum_t71_t60 = { x : (self.data[x].performance_t7t1(), self.data[x].performance_t6t0()) for x in top_by_market_cap}\n",
    "        \n",
    "        # Momentum t-7 to t-1 sorting\n",
    "        sorted_by_perf_t71 = sorted(momentum_t71_t60.items(), key = lambda x: x[1][0], reverse = True)\n",
    "        decile = int(len(sorted_by_perf_t71) / 10)\n",
    "        high_by_perf_t71 = [x[0] for x in sorted_by_perf_t71[:decile]]\n",
    "        low_by_perf_t71 = [x[0] for x in sorted_by_perf_t71[-decile:]]\n",
    "\n",
    "        # Momentum t-6 to t sorting\n",
    "        sorted_by_perf_t60 = sorted(momentum_t71_t60.items(), key = lambda x: x[1][1], reverse = True)\n",
    "        decile = int(len(sorted_by_perf_t60) / 10)\n",
    "        high_by_perf_t60 = [x[0] for x in sorted_by_perf_t60[:decile]]\n",
    "        low_by_perf_t60 = [x[0] for x in sorted_by_perf_t60[-decile:]]\n",
    "        \n",
    "        self.long = [x for x in high_by_perf_t71 if x in high_by_perf_t60]\n",
    "        self.short = [x for x in low_by_perf_t71 if x in low_by_perf_t60]\n",
    "        \n",
    "        self.selection_flag = False\n",
    "        \n",
    "        return self.long + self.short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398541d-e737-4ddb-b61c-fcbb56b0f6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def signal_generation(data,method):\n",
    "    \n",
    "    #according to investopedia\n",
    "    #for a double bottom pattern\n",
    "    #we should use 3-month horizon which is 75\n",
    "    period=75\n",
    "    \n",
    "    #alpha denotes the difference between price and bollinger bands\n",
    "    #if alpha is too small, its unlikely to trigger a signal\n",
    "    #if alpha is too large, its too easy to trigger a signal\n",
    "    #which gives us a higher probability to lose money\n",
    "    #beta denotes the scale of bandwidth\n",
    "    #when bandwidth is larger than beta, it is expansion period\n",
    "    #when bandwidth is smaller than beta, it is contraction period\n",
    "    alpha=0.0001\n",
    "    beta=0.0001\n",
    "    \n",
    "    df=method(data)\n",
    "    df['signals']=0\n",
    "    \n",
    "    #as usual, cumsum denotes the holding position\n",
    "    #coordinates store five nodes of w shape\n",
    "    #later we would use these coordinates to draw a w shape\n",
    "    df['cumsum']=0\n",
    "    df['coordinates']=''\n",
    "    \n",
    "    for i in range(period,len(df)):\n",
    "        \n",
    "        #moveon is a process control\n",
    "        #if moveon==true, we move on to verify the next condition\n",
    "        #if false, we move on to the next iteration\n",
    "        #threshold denotes the value of node k\n",
    "        #we would use it for the comparison with node m\n",
    "        #plz refer to condition 3\n",
    "        moveon=False\n",
    "        threshold=0.0\n",
    "        \n",
    "        #bottom w pattern recognition\n",
    "        #there is another signal generation method called walking the bands\n",
    "        #i personally think its too late for following the trend\n",
    "        #after confirmation of several breakthroughs\n",
    "        #maybe its good for stop and reverse\n",
    "        #condition 4\n",
    "        if (df['price'][i]>df['upper band'][i]) and \\\n",
    "        (df['cumsum'][i]==0):\n",
    "            \n",
    "            for j in range(i,i-period,-1):                \n",
    "                \n",
    "                #condition 2\n",
    "                if (np.abs(df['mid band'][j]-df['price'][j])<alpha) and \\\n",
    "                (np.abs(df['mid band'][j]-df['upper band'][i])<alpha):\n",
    "                    moveon=True\n",
    "                    break\n",
    "            \n",
    "            if moveon==True:\n",
    "                moveon=False\n",
    "                for k in range(j,i-period,-1):\n",
    "                    \n",
    "                    #condition 1\n",
    "                    if (np.abs(df['lower band'][k]-df['price'][k])<alpha):\n",
    "                        threshold=df['price'][k]\n",
    "                        moveon=True\n",
    "                        break\n",
    "                        \n",
    "            if moveon==True:\n",
    "                moveon=False\n",
    "                for l in range(k,i-period,-1):\n",
    "                    \n",
    "                    #this one is for plotting w shape\n",
    "                    if (df['mid band'][l]<df['price'][l]):\n",
    "                        moveon=True\n",
    "                        break\n",
    "                    \n",
    "            if moveon==True:\n",
    "                moveon=False        \n",
    "                for m in range(i,j,-1):\n",
    "                    \n",
    "                    #condition 3\n",
    "                    if (df['price'][m]-df['lower band'][m]<alpha) and \\\n",
    "                    (df['price'][m]>df['lower band'][m]) and \\\n",
    "                    (df['price'][m]<threshold):\n",
    "                        df.at[i,'signals']=1\n",
    "                        df.at[i,'coordinates']='%s,%s,%s,%s,%s'%(l,k,j,m,i)\n",
    "                        df['cumsum']=df['signals'].cumsum()\n",
    "                        moveon=True\n",
    "                        break\n",
    "        \n",
    "        #clear our positions when there is contraction on bollinger bands\n",
    "        #contraction on the bandwidth is easy to understand\n",
    "        #when price momentum exists, the price would move dramatically for either direction\n",
    "        #which greatly increases the standard deviation\n",
    "        #when the momentum vanishes, we clear our positions\n",
    "        \n",
    "        #note that we put moveon in the condition\n",
    "        #just in case our signal generation time is contraction period\n",
    "        #but we dont wanna clear positions right now\n",
    "        if (df['cumsum'][i]!=0) and \\\n",
    "        (df['std'][i]<beta) and \\\n",
    "        (moveon==False):\n",
    "            df.at[i,'signals']=-1\n",
    "            df['cumsum']=df['signals'].cumsum()\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "## breakout\n",
    "breakout strategy\n",
    "\n",
    "def min2day(df,column,year,month,rg):\n",
    "    \n",
    "    #lets create a dictionary \n",
    "    #we use keys to classify different info we need\n",
    "    memo={'date':[],'open':[],'close':[],'high':[],'low':[]}\n",
    "    \n",
    "    #no matter which month\n",
    "    #the maximum we can get is 31 days\n",
    "    #thus, we only need to run a traversal on 31 days\n",
    "    #nevertheless, not everyday is a workday\n",
    "    #assuming our raw data doesnt contain weekend prices\n",
    "    #we use try function to make sure we get the info of workdays without errors\n",
    "    #note that i put date at the end of the loop\n",
    "    #the date appendix doesnt depend on our raw data\n",
    "    #it only relies on the range function above\n",
    "    #we could accidentally append weekend date if we put it at the beginning of try function\n",
    "    #not until the program cant find price in raw data will the program stop\n",
    "    #by that time, we have already appended weekend date\n",
    "    #we wanna make sure the length of all lists in dictionary are the same\n",
    "    #so that we can construct a structured table in the next step\n",
    "    for i in range(1,32):\n",
    "    \n",
    "        try:\n",
    "            temp=df['%s-%s-%s 3:00:00'%(year,month,i):'%s-%s-%s 12:00:00'%(year,month,i)][column]\n",
    "\n",
    "            memo['open'].append(temp[0])\n",
    "            memo['close'].append(temp[-1])\n",
    "            memo['high'].append(max(temp))\n",
    "            memo['low'].append(min(temp))\n",
    "            memo['date'].append('%s-%s-%s'%(year,month,i))\n",
    "       \n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "    intraday=pd.DataFrame(memo)\n",
    "    intraday.set_index(pd.to_datetime(intraday['date']),inplace=True)\n",
    "    \n",
    "    \n",
    "    #preparation\n",
    "    intraday['range1']=intraday['high'].rolling(rg).max()-intraday['close'].rolling(rg).min()\n",
    "    intraday['range2']=intraday['close'].rolling(rg).max()-intraday['low'].rolling(rg).min()\n",
    "    intraday['range']=np.where(intraday['range1']>intraday['range2'],intraday['range1'],intraday['range2'])\n",
    "    \n",
    "    return intraday\n",
    "\n",
    "\n",
    "\n",
    "def signal_generation(df,intraday,param,column,rg):\n",
    "    \n",
    "    #as the lags of days have been set to 5  \n",
    "    #we should start our backtesting after 4 workdays of current month\n",
    "    #cumsum is to control the holding of underlying asset\n",
    "    #sigup and siglo are the variables to store the upper/lower threshold  \n",
    "    #upper and lower are for the purpose of tracking sigup and siglo\n",
    "    signals=df[df.index>=intraday['date'].iloc[rg-1]]\n",
    "    signals['signals']=0\n",
    "    signals['cumsum']=0\n",
    "    signals['upper']=0.0\n",
    "    signals['lower']=0.0\n",
    "    sigup=float(0)\n",
    "    siglo=float(0)\n",
    "    \n",
    "    #for traversal on time series\n",
    "    #the tricky part is the slicing\n",
    "    #we have to either use [i:i] or pd.Series\n",
    "    #first we set up thresholds at the beginning of london market\n",
    "    #which is est 3am\n",
    "    #if the price exceeds either threshold\n",
    "    #we will take long/short positions  \n",
    "    \n",
    "    for i in signals.index:\n",
    "        \n",
    "        #note that intraday and dataframe have different frequencies\n",
    "        #obviously different metrics for indexes\n",
    "        #we use variable date for index convertion\n",
    "        date='%s-%s-%s'%(i.year,i.month,i.day)\n",
    "        \n",
    "        \n",
    "        #market opening\n",
    "        #set up thresholds\n",
    "        if (i.hour==3 and i.minute==0):\n",
    "            sigup=float(param*intraday['range'][date]+pd.Series(signals[column])[i])\n",
    "            siglo=float(-(1-param)*intraday['range'][date]+pd.Series(signals[column])[i])\n",
    "\n",
    "        #thresholds got breached\n",
    "        #signals generating\n",
    "        if (sigup!=0 and pd.Series(signals[column])[i]>sigup):\n",
    "            signals.at[i,'signals']=1\n",
    "        if (siglo!=0 and pd.Series(signals[column])[i]<siglo):\n",
    "            signals.at[i,'signals']=-1\n",
    "\n",
    "\n",
    "        #check if signal has been generated\n",
    "        #if so, use cumsum to verify that we only generate one signal for each situation\n",
    "        if pd.Series(signals['signals'])[i]!=0:\n",
    "            signals['cumsum']=signals['signals'].cumsum()        \n",
    "            if (pd.Series(signals['cumsum'])[i]>1 or pd.Series(signals['cumsum'])[i]<-1):\n",
    "                signals.at[i,'signals']=0\n",
    "               \n",
    "            #if the price goes from below the lower threshold to above the upper threshold during the day\n",
    "            #we reverse our positions from short to long\n",
    "            if (pd.Series(signals['cumsum'])[i]==0):\n",
    "                if (pd.Series(signals[column])[i]>sigup):\n",
    "                    signals.at[i,'signals']=2\n",
    "                if (pd.Series(signals[column])[i]<siglo):\n",
    "                    signals.at[i,'signals']=-2\n",
    "                    \n",
    "        #by the end of london market, which is est 12pm\n",
    "        #we clear all opening positions\n",
    "        #the whole part is very similar to London Breakout strategy\n",
    "        if i.hour==12 and i.minute==0:\n",
    "            sigup,siglo=float(0),float(0)\n",
    "            signals['cumsum']=signals['signals'].cumsum()\n",
    "            signals.at[i,'signals']=-signals['cumsum'][i:i]\n",
    "            \n",
    "        #keep track of trigger levels\n",
    "        signals.at[i,'upper']=sigup\n",
    "        signals.at[i,'lower']=siglo\n",
    "\n",
    "    return signals\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def london_breakout(df):\n",
    "    \n",
    "    df['signals']=0\n",
    "\n",
    "    #cumsum is the cumulated sum of signals\n",
    "    #later we would use it to control our positions\n",
    "    df['cumsum']=0\n",
    "\n",
    "    #upper and lower are our thresholds\n",
    "    df['upper']=0.0\n",
    "    df['lower']=0.0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def signal_generation(df,method):\n",
    "    \n",
    "    #tokyo_price is a list to store average price of\n",
    "    #the last trading hour of tokyo market\n",
    "    #we use max, min to define the real threshold later\n",
    "    tokyo_price=[]\n",
    "\n",
    "    #risky_stop is a parameter set by us\n",
    "    #it is to reduce the risk exposure to volatility\n",
    "    #i am using 100 basis points\n",
    "    #for instance, we have defined our upper and lower thresholds\n",
    "    #however, when london market opens\n",
    "    #the price goes skyrocketing \n",
    "    #say 200 basis points above upper threshold\n",
    "    #i personally wouldnt get in the market as its too risky\n",
    "    #also, my stop loss and target is 50 basis points\n",
    "    #just half of my risk interval\n",
    "    #i will use this variable for later stop loss set up\n",
    "    risky_stop=0.01\n",
    "\n",
    "    #this is another parameter set by us\n",
    "    #it is about how long opening volatility would wear off\n",
    "    #for me, 30 minutes after the market opening is the boundary\n",
    "    #as long as its under 30 minutes after the market opening\n",
    "    #if the price reaches threshold level, i will trade on signals\n",
    "    open_minutes=30\n",
    "\n",
    "    #this is the price when we execute a trade\n",
    "    #we need to save it to set up the stop loss\n",
    "    executed_price=float(0)\n",
    "    \n",
    "    signals=method(df)\n",
    "    signals['date']=pd.to_datetime(signals['date'])\n",
    "    \n",
    "    #this is the core part\n",
    "    #the time complexity for this part is extremely high\n",
    "    #as there are too many constraints\n",
    "    #if u have a better idea to optimize it\n",
    "    #plz let me know\n",
    "\n",
    "    for i in range(len(signals)):\n",
    "        \n",
    "        #as mentioned before\n",
    "        #the dataset use eastern standard time\n",
    "        #so est 2am is the last hour before london starts\n",
    "        #we try to append all the price into the list called threshold\n",
    "        if signals['date'][i].hour==2:\n",
    "            tokyo_price.append(signals['price'][i])\n",
    "            \n",
    "        #est 3am which is gmt 8am\n",
    "        #thats when london market starts\n",
    "        #good morning city of london and canary wharf!\n",
    "        #right at this moment\n",
    "        #we get max and min of the price of tokyo trading hour\n",
    "        #we set up the threshold as the way it is\n",
    "        #alternatively, we can put 10 basis points above and below thresholds\n",
    "        #we also use upper and lower list to keep track of our thresholds\n",
    "        #and now we clear the list called threshold\n",
    "        elif signals['date'][i].hour==3 and signals['date'][i].minute==0:\n",
    "\n",
    "            upper=max(tokyo_price)\n",
    "            lower=min(tokyo_price)\n",
    "\n",
    "            signals.at[i,'upper']=upper\n",
    "            signals.at[i,'lower']=lower\n",
    "\n",
    "            tokyo_price=[]\n",
    "            \n",
    "        #prior to 30 minutes i have mentioned before\n",
    "        #as long as its under 30 minutes after market opening\n",
    "        #signals will be generated once conditions are met\n",
    "        #this is a relatively risky way\n",
    "        #alternatively, we can set the signal generation time at a fixed point\n",
    "        #when its gmt 8 30 am, we check the conditions to see if there is any signal\n",
    "        elif signals['date'][i].hour==3 and signals['date'][i].minute<open_minutes:\n",
    "\n",
    "            #again, we wanna keep track of thresholds during signal generation periods\n",
    "            signals.at[i,'upper']=upper\n",
    "            signals.at[i,'lower']=lower\n",
    "            \n",
    "            #this is the condition of signals generation\n",
    "            #when the price is above upper threshold\n",
    "            #we set signals to 1 which implies long\n",
    "            if signals['price'][i]-upper>0:\n",
    "                signals.at[i,'signals']=1\n",
    "\n",
    "                #we use cumsum to check the cumulated sum of signals\n",
    "                #we wanna make sure that\n",
    "                #only the first price above upper threshold triggers the signal\n",
    "                #also, if it goes skyrocketing\n",
    "                #say 200 basis points above, which is 100 above our risk tolerance\n",
    "                #we set it as a false alarm\n",
    "                signals['cumsum']=signals['signals'].cumsum()\n",
    "\n",
    "                if signals['price'][i]-upper>risky_stop:\n",
    "                    signals.at[i,'signals']=0\n",
    "\n",
    "                elif signals['cumsum'][i]>1:\n",
    "                    signals.at[i,'signals']=0\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #we also need to store the price when we execute a trade\n",
    "                    #its for stop loss calculation\n",
    "                    executed_price=signals['price'][i]\n",
    "\n",
    "            #vice versa    \n",
    "            if signals['price'][i]-lower<0:\n",
    "                signals.at[i,'signals']=-1\n",
    "\n",
    "                signals['cumsum']=signals['signals'].cumsum()\n",
    "\n",
    "                if lower-signals['price'][i]>risky_stop:\n",
    "                    signals.at[i,'signals']=0\n",
    "\n",
    "                elif signals['cumsum'][i]<-1:\n",
    "                    signals.at[i,'signals']=0\n",
    "\n",
    "                else:\n",
    "                    executed_price=signals['price'][i]\n",
    "                    \n",
    "        #when its gmt 5 pm, london market closes\n",
    "        #we use cumsum to see if there is any position left open\n",
    "        #we take -cumsum as a signal\n",
    "        #if there is no open position, -0 is still 0\n",
    "        elif signals['date'][i].hour==12:\n",
    "            signals['cumsum']=signals['signals'].cumsum()\n",
    "            signals.at[i,'signals']=-signals['cumsum'][i]\n",
    "            \n",
    "        #during london trading hour after opening but before closing\n",
    "        #we still use cumsum to check our open positions\n",
    "        #if there is any open position\n",
    "        #we set our condition at original executed price +/- half of the risk interval\n",
    "        #when it goes above or below our risk tolerance\n",
    "        #we clear positions to claim profit or loss\n",
    "        else:\n",
    "            signals['cumsum']=signals['signals'].cumsum()\n",
    "            \n",
    "            if signals['cumsum'][i]!=0:\n",
    "                if signals['price'][i]>executed_price+risky_stop/2:\n",
    "                    signals.at[i,'signals']=-signals['cumsum'][i]\n",
    "                    \n",
    "                if signals['price'][i]<executed_price-risky_stop/2:\n",
    "                    signals.at[i,'signals']=-signals['cumsum'][i]\n",
    "    \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9adad-b176-4a10-8415-d9be7770bef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Momentum \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def heikin_ashi(data):\n",
    "    \n",
    "    df=data.copy()\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "        \n",
    "    #heikin ashi close\n",
    "    df['HA close']=(df['Open']+df['Close']+df['High']+df['Low'])/4\n",
    "\n",
    "    #initialize heikin ashi open\n",
    "    df['HA open']=float(0)\n",
    "    df['HA open'][0]=df['Open'][0]\n",
    "\n",
    "    #heikin ashi open\n",
    "    for n in range(1,len(df)):\n",
    "        df.at[n,'HA open']=(df['HA open'][n-1]+df['HA close'][n-1])/2\n",
    "        \n",
    "    #heikin ashi high/low\n",
    "    temp=pd.concat([df['HA open'],df['HA close'],df['Low'],df['High']],axis=1)\n",
    "    df['HA high']=temp.apply(max,axis=1)\n",
    "    df['HA low']=temp.apply(min,axis=1)\n",
    "\n",
    "    del df['Adj Close']\n",
    "    del df['Volume']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#setting up signal generations\n",
    "#trigger conditions can be found from the website mentioned above\n",
    "#they kinda look like marubozu candles\n",
    "#there s a short strategy as well\n",
    "#the trigger condition of short strategy is the reverse of long strategy\n",
    "#you have to satisfy all four conditions to long/short\n",
    "#nevertheless, the exit signal only has three conditions\n",
    "def signal_generation(df,method,stls):\n",
    "        \n",
    "    data=method(df)\n",
    "    \n",
    "    data['signals']=0\n",
    "\n",
    "    #i use cumulated sum to check how many positions i have longed\n",
    "    #i would ignore the exit signal prior if not holding positions\n",
    "    #i also keep tracking how many long positions i have got\n",
    "    #long signals cannot exceed the stop loss limit\n",
    "    data['cumsum']=0\n",
    "\n",
    "    for n in range(1,len(data)):\n",
    "        \n",
    "        #long triggered\n",
    "        if (data['HA open'][n]>data['HA close'][n] and data['HA open'][n]==data['HA high'][n] and\n",
    "            np.abs(data['HA open'][n]-data['HA close'][n])>np.abs(data['HA open'][n-1]-data['HA close'][n-1]) and\n",
    "            data['HA open'][n-1]>data['HA close'][n-1]):\n",
    "            \n",
    "            data.at[n,'signals']=1\n",
    "            data['cumsum']=data['signals'].cumsum()\n",
    "\n",
    "\n",
    "            #accumulate too many longs\n",
    "            if data['cumsum'][n]>stls:\n",
    "                data.at[n,'signals']=0\n",
    "        \n",
    "        #exit positions\n",
    "        elif (data['HA open'][n]<data['HA close'][n] and data['HA open'][n]==data['HA low'][n] and \n",
    "        data['HA open'][n-1]<data['HA close'][n-1]):\n",
    "            \n",
    "            data.at[n,'signals']=-1\n",
    "            data['cumsum']=data['signals'].cumsum()\n",
    "        \n",
    "\n",
    "            #clear all longs\n",
    "            #if there are no long positions in my portfolio\n",
    "            #ignore the exit signal\n",
    "            if data['cumsum'][n]>0:\n",
    "                data.at[n,'signals']=-1*(data['cumsum'][n-1])\n",
    "\n",
    "            if data['cumsum'][n]<0:\n",
    "                data.at[n,'signals']=0\n",
    "                \n",
    "    return data\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#since matplotlib remove the candlestick\n",
    "#plus we dont wanna install mpl_finance\n",
    "#we implement our own version\n",
    "#simply use fill_between to construct the bar\n",
    "#use line plot to construct high and low\n",
    "def candlestick(df,ax=None,titlename='',highcol='High',lowcol='Low',\n",
    "                opencol='Open',closecol='Close',xcol='Date',\n",
    "                colorup='r',colordown='g',**kwargs):  \n",
    "    \n",
    "    #bar width\n",
    "    #use 0.6 by default\n",
    "    dif=[(-3+i)/10 for i in range(7)]\n",
    "    \n",
    "    if not ax:\n",
    "        ax=plt.figure(figsize=(10,5)).add_subplot(111)\n",
    "    \n",
    "    #construct the bars one by one\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        #width is 0.6 by default\n",
    "        #so 7 data points required for each bar\n",
    "        x=[i+j for j in dif]\n",
    "        y1=[df[opencol].iloc[i]]*7\n",
    "        y2=[df[closecol].iloc[i]]*7\n",
    "\n",
    "        barcolor=colorup if y1[0]>y2[0] else colordown\n",
    "        \n",
    "        #no high line plot if open/close is high\n",
    "        if df[highcol].iloc[i]!=max(df[opencol].iloc[i],df[closecol].iloc[i]):\n",
    "            \n",
    "            #use generic plot to viz high and low\n",
    "            #use 1.001 as a scaling factor\n",
    "            #to prevent high line from crossing into the bar\n",
    "            plt.plot([i,i],\n",
    "                     [df[highcol].iloc[i],\n",
    "                      max(df[opencol].iloc[i],\n",
    "                          df[closecol].iloc[i])*1.001],c='k',**kwargs)\n",
    "    \n",
    "        #same as high\n",
    "        if df[lowcol].iloc[i]!=min(df[opencol].iloc[i],df[closecol].iloc[i]):             \n",
    "            \n",
    "            plt.plot([i,i],\n",
    "                     [df[lowcol].iloc[i],\n",
    "                      min(df[opencol].iloc[i],\n",
    "                          df[closecol].iloc[i])*0.999],c='k',**kwargs)\n",
    "        \n",
    "        #treat the bar as fill between\n",
    "        plt.fill_between(x,y1,y2,\n",
    "                         edgecolor='k',\n",
    "                         facecolor=barcolor,**kwargs)\n",
    "\n",
    "    #only show 5 xticks\n",
    "    plt.xticks(range(0,len(df),len(df)//5),df[xcol][0::len(df)//5].dt.date)\n",
    "    plt.title(titlename)\n",
    "    \n",
    "    \n",
    "#plotting the backtesting result\n",
    "def plot(df,ticker):    \n",
    "    \n",
    "    df.set_index(df['Date'],inplace=True)\n",
    "    \n",
    "    #first plot is Heikin-Ashi candlestick\n",
    "    #use candlestick function and set Heikin-Ashi O,C,H,L\n",
    "    ax1=plt.subplot2grid((200,1), (0,0), rowspan=120,ylabel='HA price')\n",
    "    candlestick(df,ax1,titlename='',highcol='HA high',lowcol='HA low',\n",
    "                opencol='HA open',closecol='HA close',xcol='Date',\n",
    "                colorup='r',colordown='g')\n",
    "    plt.grid(True)\n",
    "    plt.xticks([])\n",
    "    plt.title('Heikin-Ashi')\n",
    "\n",
    "\n",
    "    #the second plot is the actual price with long/short positions as up/down arrows\n",
    "    ax2=plt.subplot2grid((200,1), (120,0), rowspan=80,ylabel='price',xlabel='')\n",
    "    df['Close'].plot(ax=ax2,label=ticker)\n",
    "\n",
    "    #long/short positions are attached to the real close price of the stock\n",
    "    #set the line width to zero\n",
    "    #thats why we only observe markers\n",
    "    ax2.plot(df.loc[df['signals']==1].index,df['Close'][df['signals']==1],marker='^',lw=0,c='g',label='long')\n",
    "    ax2.plot(df.loc[df['signals']<0].index,df['Close'][df['signals']<0],marker='v',lw=0,c='r',label='short')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#backtesting\n",
    "#initial capital 10k to calculate the actual pnl  \n",
    "#100 shares to buy of every position\n",
    "def portfolio(data,capital0=10000,positions=100):   \n",
    "        \n",
    "    #cumsum column is created to check the holding of the position\n",
    "    data['cumsum']=data['signals'].cumsum()\n",
    "\n",
    "    portfolio=pd.DataFrame()\n",
    "    portfolio['holdings']=data['cumsum']*data['Close']*positions\n",
    "    portfolio['cash']=capital0-(data['signals']*data['Close']*positions).cumsum()\n",
    "    portfolio['total asset']=portfolio['holdings']+portfolio['cash']\n",
    "    portfolio['return']=portfolio['total asset'].pct_change()\n",
    "    portfolio['signals']=data['signals']\n",
    "    portfolio['date']=data['Date']\n",
    "    portfolio.set_index('date',inplace=True)\n",
    "\n",
    "    return portfolio\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#plotting the asset value change of the portfolio\n",
    "def profit(portfolio):\n",
    "        \n",
    "    fig=plt.figure()\n",
    "    bx=fig.add_subplot(111)\n",
    "    \n",
    "    portfolio['total asset'].plot(label='Total Asset')\n",
    "    \n",
    "    #long/short position markers related to the portfolio\n",
    "    #the same mechanism as the previous one\n",
    "    #replace close price with total asset value\n",
    "    bx.plot(portfolio['signals'].loc[portfolio['signals']==1].index,portfolio['total asset'][portfolio['signals']==1],lw=0,marker='^',c='g',label='long')\n",
    "    bx.plot(portfolio['signals'].loc[portfolio['signals']<0].index,portfolio['total asset'][portfolio['signals']<0],lw=0,marker='v',c='r',label='short')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Asset Value')\n",
    "    plt.title('Total Asset')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#omega ratio is a variation of sharpe ratio\n",
    "#the risk free return is replaced by a given threshold\n",
    "#in this case, the return of benchmark\n",
    "#integral is needed to calculate the return above and below the threshold\n",
    "#you can use summation to do approximation as well\n",
    "#it is a more reasonable ratio to measure the risk adjusted return\n",
    "#normal distribution doesnt explain the fat tail of returns\n",
    "#so i use student T cumulated distribution function instead \n",
    "#to make our life easier, i do not use empirical distribution\n",
    "#the cdf of empirical distribution is much more complex\n",
    "#check wikipedia for more details\n",
    "# https://en.wikipedia.org/wiki/Omega_ratio\n",
    "def omega(risk_free,degree_of_freedom,maximum,minimum):\n",
    "\n",
    "    y=scipy.integrate.quad(lambda g:1-scipy.stats.t.cdf(g,degree_of_freedom),risk_free,maximum)\n",
    "    x=scipy.integrate.quad(lambda g:scipy.stats.t.cdf(g,degree_of_freedom),minimum,risk_free)\n",
    "\n",
    "    z=(y[0])/(x[0])\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "#sortino ratio is another variation of sharpe ratio\n",
    "#the standard deviation of all returns is substituted with standard deviation of negative returns\n",
    "#sortino ratio measures the impact of negative return on return\n",
    "#i am also using student T probability distribution function instead of normal distribution\n",
    "#check wikipedia for more details\n",
    "# https://en.wikipedia.org/wiki/Sortino_ratio\n",
    "def sortino(risk_free,degree_of_freedom,growth_rate,minimum):\n",
    "\n",
    "    v=np.sqrt(np.abs(scipy.integrate.quad(lambda g:((risk_free-g)**2)*scipy.stats.t.pdf(g,degree_of_freedom),risk_free,minimum)))\n",
    "    s=(growth_rate-risk_free)/v[0]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "#i use a function to calculate maximum drawdown\n",
    "#the idea is simple\n",
    "#for every day, we take the current asset value marked to market\n",
    "#to compare with the previous highest asset value\n",
    "#we get our daily drawdown\n",
    "#it is supposed to be negative if the current one is not the highest\n",
    "#we implement a temporary variable to store the minimum negative value\n",
    "#which is called maximum drawdown\n",
    "#for each daily drawdown that is smaller than our temporary value\n",
    "#we update the temp until we finish our traversal\n",
    "#in the end we return the maximum drawdown\n",
    "def mdd(series):\n",
    "\n",
    "    minimum=0\n",
    "    for i in range(1,len(series)):\n",
    "        if minimum>(series[i]/max(series[:i])-1):\n",
    "            minimum=(series[i]/max(series[:i])-1)\n",
    "\n",
    "    return minimum\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "    \n",
    "\n",
    "#stats calculation\n",
    "def stats(portfolio,trading_signals,stdate,eddate,capital0=10000):\n",
    "\n",
    "    stats=pd.DataFrame([0])\n",
    "\n",
    "    #get the min and max of return\n",
    "    maximum=np.max(portfolio['return'])\n",
    "    minimum=np.min(portfolio['return'])    \n",
    "\n",
    "    #growth_rate denotes the average growth rate of portfolio \n",
    "    #use geometric average instead of arithmetic average for percentage growth\n",
    "    growth_rate=(float(portfolio['total asset'].iloc[-1]/capital0))**(1/len(trading_signals))-1\n",
    "\n",
    "    #calculating the standard deviation\n",
    "    std=float(np.sqrt((((portfolio['return']-growth_rate)**2).sum())/len(trading_signals)))\n",
    "\n",
    "    #use S&P500 as benchmark\n",
    "    benchmark=yf.download('^GSPC',start=stdate,end=eddate)\n",
    "\n",
    "    #return of benchmark\n",
    "    return_of_benchmark=float(benchmark['Close'].iloc[-1]/benchmark['Open'].iloc[0]-1)\n",
    "\n",
    "    #rate_of_benchmark denotes the average growth rate of benchmark \n",
    "    #use geometric average instead of arithmetic average for percentage growth\n",
    "    rate_of_benchmark=(return_of_benchmark+1)**(1/len(trading_signals))-1\n",
    "\n",
    "    del benchmark\n",
    "\n",
    "    #backtesting stats\n",
    "    #CAGR stands for cumulated average growth rate\n",
    "    stats['CAGR']=stats['portfolio return']=float(0)\n",
    "    stats['CAGR'][0]=growth_rate\n",
    "    stats['portfolio return'][0]=portfolio['total asset'].iloc[-1]/capital0-1\n",
    "    stats['benchmark return']=return_of_benchmark\n",
    "    stats['sharpe ratio']=(growth_rate-rate_of_benchmark)/std\n",
    "    stats['maximum drawdown']=mdd(portfolio['total asset'])\n",
    "\n",
    "    #calmar ratio is sorta like sharpe ratio\n",
    "    #the standard deviation is replaced by maximum drawdown\n",
    "    #it is the measurement of return after worse scenario adjustment\n",
    "    #check wikipedia for more details\n",
    "    # https://en.wikipedia.org/wiki/Calmar_ratio\n",
    "    stats['calmar ratio']=growth_rate/stats['maximum drawdown']\n",
    "    stats['omega ratio']=omega(rate_of_benchmark,len(trading_signals),maximum,minimum)\n",
    "    stats['sortino ratio']=sortino(rate_of_benchmark,len(trading_signals),growth_rate,minimum)\n",
    "\n",
    "    #note that i use stop loss limit to limit the numbers of longs\n",
    "    #and when clearing positions, we clear all the positions at once\n",
    "    #so every long is always one, and short cannot be larger than the stop loss limit\n",
    "    stats['numbers of longs']=trading_signals['signals'].loc[trading_signals['signals']==1].count()\n",
    "    stats['numbers of shorts']=trading_signals['signals'].loc[trading_signals['signals']<0].count()\n",
    "    stats['numbers of trades']=stats['numbers of shorts']+stats['numbers of longs']  \n",
    "\n",
    "    #to get the total length of trades\n",
    "    #given that cumsum indicates the holding of positions\n",
    "    #we can get all the possible outcomes when cumsum doesnt equal zero\n",
    "    #then we count how many non-zero positions there are\n",
    "    #we get the estimation of total length of trades\n",
    "    stats['total length of trades']=trading_signals['signals'].loc[trading_signals['cumsum']!=0].count()\n",
    "    stats['average length of trades']=stats['total length of trades']/stats['numbers of trades']\n",
    "    stats['profit per trade']=float(0)\n",
    "    stats['profit per trade'].iloc[0]=(portfolio['total asset'].iloc[-1]-capital0)/stats['numbers of trades'].iloc[0]\n",
    "\n",
    "    del stats[0]\n",
    "    print(stats)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #initializing\n",
    "\n",
    "    #stop loss positions, the maximum long positions we can get\n",
    "    #without certain constraints, you will long indefinites times \n",
    "    #as long as the market condition triggers the signal\n",
    "    #in a whipsaw condition, it is suicidal\n",
    "    stls=3\n",
    "    ticker='NVDA'\n",
    "    stdate='2015-04-01'\n",
    "    eddate='2018-02-15'\n",
    "\n",
    "    #slicer is used for plotting\n",
    "    #a three year dataset with 750 data points would be too much\n",
    "    slicer=700\n",
    "\n",
    "    #downloading data\n",
    "    df=yf.download(ticker,start=stdate,end=eddate)\n",
    "\n",
    "    trading_signals=signal_generation(df,heikin_ashi,stls)\n",
    "\n",
    "    viz=trading_signals[slicer:]\n",
    "    plot(viz,ticker)\n",
    "\n",
    "    portfolio_details=portfolio(viz)\n",
    "    profit(portfolio_details)\n",
    "\n",
    "    stats(portfolio_details,trading_signals,stdate,eddate)\n",
    "\n",
    "    #note that this is the only py file with complete stats calculation\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e9f71-dddd-499e-b8d9-2ae2de8ce5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  6 11:57:46 2018\n",
    "\n",
    "@author: Administrator\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#grazie a my mentor Prof Giampiero M Gallo\n",
    "#ex-professor in statistics currently a governor in Italy\n",
    "#neither Lega Nord nor Movimento 5 Stelle but Partito Democratico\n",
    "#and his mentor Robert Engle, the nobel laureate!\n",
    "#for their tremendous contributions to VECM\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#pair trading is also called mean reversion trading\n",
    "#we find two cointegrated assets, normally a stock and an ETF index\n",
    "#or two stocks in the same industry or any pair that passes the test\n",
    "#we run an cointegration test on the historical data\n",
    "#we set the trigger condition for both stocks\n",
    "#theoretically these two stocks cannot drift too far from each other\n",
    "#its like a drunk man with a dog\n",
    "#the invisible dog leash would keep both assets in check\n",
    "#when one stock is getting too bullish\n",
    "#we short the bullish one and long the bearish one, vice versa\n",
    "#sooner or later, the dog would converge to the drunk man\n",
    "#nevertheless, the backtest is based on historical datasets\n",
    "#in real stock market, market conditions are dynamic\n",
    "#two assets may seem cointegrated for the past two years\n",
    "#they can completely diverge after one company launch a new product or whatsoever\n",
    "#i am talking about nvidia and amd, two gpu companies\n",
    "#after bitcoin mining boom and machine learning hype\n",
    "#stock price of nvidia went skyrocketing\n",
    "#on the contrary amd didnt change much \n",
    "#the cointegrated relationship just broke up\n",
    "#so be extremely cautious with cointegration\n",
    "#there is no such thing as riskless statistical arbitrage\n",
    "#always check the cointegration status before trading execution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#use Engle-Granger two-step method to test cointegration\n",
    "#the underlying method is straight forward and easy to implement\n",
    "#a more important thing is the method is invented by the mentor of my mentor!!!\n",
    "#the latest statsmodels package should ve included johansen test which is more common\n",
    "#check sm.tsa.var.vecm.coint_johansen\n",
    "#the malaise of two-step is the order of the cointegration\n",
    "#unlike johansen test, two-step method can only detect the first order\n",
    "#check the following material for further details\n",
    "# https://warwick.ac.uk/fac/soc/economics/staff/gboero/personal/hand2_cointeg.pdf\n",
    "def EG_method(X,Y,show_summary=False):\n",
    "    \n",
    "    #step 1\n",
    "    #estimate long run equilibrium\n",
    "    model1=sm.OLS(Y,sm.add_constant(X)).fit()\n",
    "    epsilon=model1.resid\n",
    "    \n",
    "    if show_summary:\n",
    "        print('\\nStep 1\\n')\n",
    "        print(model1.summary())\n",
    "    \n",
    "    #check p value of augmented dickey fuller test\n",
    "    #if p value is no larger than 5%, stationary test is passed\n",
    "    if sm.tsa.stattools.adfuller(epsilon)[1]>0.05:\n",
    "        return False,model1\n",
    "    \n",
    "    #take first order difference of X and Y plus the lagged residual from step 1\n",
    "    X_dif=sm.add_constant(pd.concat([X.diff(),epsilon.shift(1)],axis=1).dropna())\n",
    "    Y_dif=Y.diff().dropna()        \n",
    "    \n",
    "    #step 2\n",
    "    #estimate error correction model\n",
    "    model2=sm.OLS(Y_dif,X_dif).fit()\n",
    "    \n",
    "    if show_summary:\n",
    "        print('\\nStep 2\\n')\n",
    "        print(model2.summary())\n",
    "    \n",
    "    #adjustment coefficient must be negative\n",
    "    if list(model2.params)[-1]>0:\n",
    "        return False,model1\n",
    "    else:\n",
    "        return True,model1\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#first we verify the status of cointegration by checking historical datasets\n",
    "#bandwidth determines the number of data points for consideration\n",
    "#bandwidth is 250 by default, around one year's data points\n",
    "#if the status is valid, we check the signals\n",
    "#when z stat gets above the upper bound\n",
    "#we long the bearish one and short the bullish one, vice versa\n",
    "def signal_generation(asset1,asset2,method,bandwidth=250):    \n",
    "    \n",
    "    signals=pd.DataFrame()\n",
    "    signals['asset1']=asset1['Close']\n",
    "    signals['asset2']=asset2['Close']\n",
    "    \n",
    "    #signals only imply holding\n",
    "    signals['signals1']=0    \n",
    "    signals['signals2']=0\n",
    "    \n",
    "    #initialize\n",
    "    prev_status=False\n",
    "    signals['z']=np.nan\n",
    "    signals['z upper limit']=np.nan\n",
    "    signals['z lower limit']=np.nan\n",
    "    signals['fitted']=np.nan    \n",
    "    signals['residual']=np.nan\n",
    "    \n",
    "    #signal processing\n",
    "    for i in range(bandwidth,len(signals)):\n",
    "        \n",
    "        #cointegration test\n",
    "        coint_status,model=method(signals['asset1'].iloc[i-bandwidth:i],\n",
    "                                  signals['asset2'].iloc[i-bandwidth:i])\n",
    "                \n",
    "        #cointegration breaks\n",
    "        #clear existing positions\n",
    "        if prev_status and not coint_status:           \n",
    "            if signals.at[signals.index[i-1],'signals1']!=0:\n",
    "                signals.at[signals.index[i],'signals1']=0\n",
    "                signals.at[signals.index[i],'signals2']=0\n",
    "                signals['z'].iloc[i:]=np.nan\n",
    "                signals['z upper limit'].iloc[i:]=np.nan\n",
    "                signals['z lower limit'].iloc[i:]=np.nan\n",
    "                signals['fitted'].iloc[i:]=np.nan    \n",
    "                signals['residual'].iloc[i:]=np.nan\n",
    "        \n",
    "        #cointegration starts\n",
    "        #set the trigger conditions\n",
    "        #this is no forward bias\n",
    "        #just to minimize the calculation done in pandas\n",
    "        if not prev_status and coint_status:\n",
    "            \n",
    "            #predict the price to compute the residual       \n",
    "            signals['fitted'].iloc[i:]=model.predict(sm.add_constant(signals['asset1'].iloc[i:]))\n",
    "            signals['residual'].iloc[i:]=signals['asset2'].iloc[i:]-signals['fitted'].iloc[i:]\n",
    "            \n",
    "            #normalize the residual to get z stat\n",
    "            #z should be a white noise following N(0,1)\n",
    "            signals['z'].iloc[i:]=(signals['residual'].iloc[i:]-np.mean(model.resid))/np.std(model.resid)\n",
    "                        \n",
    "            #create thresholds\n",
    "            #conventionally one sigma is the threshold\n",
    "            #two sigma reaches 95% which is relatively difficult to trigger\n",
    "            signals['z upper limit'].iloc[i:]=signals['z'].iloc[i]+np.std(model.resid)\n",
    "            signals['z lower limit'].iloc[i:]=signals['z'].iloc[i]-np.std(model.resid)\n",
    "        \n",
    "        #as z stat cannot exceed both upper and lower bounds at the same time\n",
    "        #the lines below hold\n",
    "        if coint_status and signals['z'].iloc[i]>signals['z upper limit'].iloc[i]:            \n",
    "             signals.at[signals.index[i],'signals1']=1            \n",
    "        if coint_status and signals['z'].iloc[i]<signals['z lower limit'].iloc[i]:            \n",
    "             signals.at[signals.index[i],'signals1']=-1\n",
    "                \n",
    "        prev_status=coint_status    \n",
    "    \n",
    "    #signals only imply holding\n",
    "    #we take the first order difference to obtain the execution signal\n",
    "    signals['positions1']=signals['signals1'].diff()\n",
    "    \n",
    "    #only need to generate trading signal of one asset\n",
    "    #the other one should be the opposite direction\n",
    "    signals['signals2']=-signals['signals1']\n",
    "    signals['positions2']=signals['signals2'].diff()   \n",
    "    \n",
    "    return signals\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#position visualization\n",
    "def plot(data,ticker1,ticker2):    \n",
    "   \n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    bx=fig.add_subplot(111)   \n",
    "    bx2=bx.twinx()\n",
    "    \n",
    "    #viz two different assets\n",
    "    asset1_price,=bx.plot(data.index,data['asset1'],\n",
    "                          c='#113aac',alpha=0.7)\n",
    "    asset2_price,=bx2.plot(data.index,data['asset2'],\n",
    "                          c='#907163',alpha=0.7)\n",
    "\n",
    "    #viz positions\n",
    "    asset1_long,=bx.plot(data.loc[data['positions1']==1].index,\n",
    "                data['asset1'][data['positions1']==1],\n",
    "                lw=0,marker='^',markersize=8,\n",
    "                c='g',alpha=0.7)\n",
    "    asset1_short,=bx.plot(data.loc[data['positions1']==-1].index,\n",
    "                data['asset1'][data['positions1']==-1],\n",
    "                lw=0,marker='v',markersize=8,\n",
    "                c='r',alpha=0.7)\n",
    "    asset2_long,=bx2.plot(data.loc[data['positions2']==1].index,\n",
    "                 data['asset2'][data['positions2']==1],\n",
    "                 lw=0,marker='^',markersize=8,\n",
    "                 c='g',alpha=0.7)\n",
    "    asset2_short,=bx2.plot(data.loc[data['positions2']==-1].index,\n",
    "                 data['asset2'][data['positions2']==-1],\n",
    "                 lw=0,marker='v',markersize=8,\n",
    "                 c='r',alpha=0.7)\n",
    "\n",
    "    #set labels\n",
    "    bx.set_ylabel(ticker1,)\n",
    "    bx2.set_ylabel(ticker2,rotation=270)\n",
    "    bx.yaxis.labelpad=15\n",
    "    bx2.yaxis.labelpad=15\n",
    "    bx.set_xlabel('Date')\n",
    "    bx.xaxis.labelpad=15\n",
    "\n",
    "    plt.legend([asset1_price,asset2_price,asset1_long,asset1_short],\n",
    "               [ticker1,ticker2,\n",
    "               'LONG','SHORT'],\n",
    "               loc='lower left')\n",
    "\n",
    "    plt.title('Pair Trading')\n",
    "    plt.xlabel('Date')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#visualize overall portfolio performance\n",
    "def portfolio(data):\n",
    "\n",
    "    #initial capital to calculate the actual pnl\n",
    "    capital0=20000\n",
    "\n",
    "    #shares to buy of each position\n",
    "    #this is no forward bias\n",
    "    #just ensure we have enough €€€ to purchase shares when the price peaks\n",
    "    positions1=capital0//max(data['asset1'])\n",
    "    positions2=capital0//max(data['asset2'])\n",
    "\n",
    "    #cumsum1 column is created to check the holding of the position\n",
    "    data['cumsum1']=data['positions1'].cumsum()\n",
    "\n",
    "    #since there are two assets, we calculate each asset separately\n",
    "    #in the end we aggregate them into one portfolio\n",
    "    portfolio=pd.DataFrame()\n",
    "    portfolio['asset1']=data['asset1']\n",
    "    portfolio['holdings1']=data['cumsum1']*data['asset1']*positions1\n",
    "    portfolio['cash1']=capital0-(data['positions1']*data['asset1']*positions1).cumsum()\n",
    "    portfolio['total asset1']=portfolio['holdings1']+portfolio['cash1']\n",
    "    portfolio['return1']=portfolio['total asset1'].pct_change()\n",
    "    portfolio['positions1']=data['positions1']\n",
    "    \n",
    "    data['cumsum2']=data['positions2'].cumsum()\n",
    "    portfolio['asset2']=data['asset2']\n",
    "    portfolio['holdings2']=data['cumsum2']*data['asset2']*positions2\n",
    "    portfolio['cash2']=capital0-(data['positions2']*data['asset2']*positions2).cumsum()\n",
    "    portfolio['total asset2']=portfolio['holdings2']+portfolio['cash2']\n",
    "    portfolio['return2']=portfolio['total asset2'].pct_change()\n",
    "    portfolio['positions2']=data['positions2']\n",
    " \n",
    "    portfolio['z']=data['z']\n",
    "    portfolio['total asset']=portfolio['total asset1']+portfolio['total asset2']\n",
    "    portfolio['z upper limit']=data['z upper limit']\n",
    "    portfolio['z lower limit']=data['z lower limit']\n",
    "    \n",
    "    #plotting the asset value change of the portfolio\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax2=ax.twinx()\n",
    " \n",
    "    total_asset_performance,=ax.plot(portfolio['total asset'],c='#46344e')\n",
    "    z_stats,=ax2.plot(portfolio['z'],c='#4f4a41',alpha=0.2)\n",
    " \n",
    "    threshold=ax2.fill_between(portfolio.index,portfolio['z upper limit'],\n",
    "                       portfolio['z lower limit'],\n",
    "                       alpha=0.2,color='#ffb48f')\n",
    "     \n",
    "    #due to the opposite direction of trade for 2 assets\n",
    "    #we will not plot positions on asset performance    \n",
    "    ax.set_ylabel('Asset Value')\n",
    "    ax2.set_ylabel('Z Statistics',rotation=270)\n",
    "    ax.yaxis.labelpad=15\n",
    "    ax2.yaxis.labelpad=15\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.xaxis.labelpad=15\n",
    "    \n",
    "    plt.legend([z_stats,threshold,total_asset_performance],\n",
    "               ['Z Statistics', 'Z Statistics +-1 Sigma',\n",
    "                'Total Asset Performance'],loc='best')\n",
    "\n",
    "    plt.grid(True)   \n",
    "    plt.title('Total Asset')\n",
    "    plt.show()\n",
    "\n",
    "    return portfolio\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #the sample i am using are NVDA and AMD from 2013 to 2014\n",
    "    stdate='2013-01-01'\n",
    "    eddate='2014-12-31'\n",
    "    ticker1='NVDA'\n",
    "    ticker2='AMD'\n",
    "\n",
    "    #extract data\n",
    "    asset1=yf.download(ticker1,start=stdate,end=eddate)\n",
    "    asset2=yf.download(ticker2,start=stdate,end=eddate)  \n",
    "\n",
    "    #create signals\n",
    "    signals=signal_generation(asset1,asset2,EG_method)\n",
    "\n",
    "    #only viz the part where trading signals occur\n",
    "    ind=signals['z'].dropna().index[0]\n",
    "\n",
    "    #viz positions\n",
    "    plot(signals[ind:],ticker1,ticker2)    \n",
    "\n",
    "    #viz portfolio performance\n",
    "    portfolio_details=portfolio(signals[ind:])\n",
    "    \n",
    "    #the performance metrics of investment could be found in another strategy called Heikin-Ashi\n",
    "    # https://github.com/je-suis-tm/quant-trading/blob/master/heikin%20ashi%20backtest.py\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "## entry\n",
    "if cci - buy_ccitime.value < buy_cci.value\n",
    "if rsi - buy_rsitime.value < buy_rsi.value\n",
    "\n",
    "## exit\n",
    "if cci-sell - sell_ccitime.value > sell_cci.value\n",
    "if rsi-sell - sell_rsitime.value > sell_rsi.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228779b6-920d-4e6e-bd64-7fe3fc08a00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hl2 = close + open / 2\n",
    "ema5, ema10\n",
    "adx\n",
    "\n",
    "entry\n",
    "if rsi crossed above 50\n",
    "if ema5 crossed above ema10\n",
    "if adx > 25\n",
    "and volume > 0\n",
    "\n",
    "\n",
    "exit\n",
    "rsi crossed below 50\n",
    "if ema5 crossed below ema10\n",
    "if adx > 25\n",
    "and volume > 0\n",
    "\n",
    "entry\n",
    "if close >= max(close), then buy tomorrow\n",
    "\n",
    "exit\n",
    "average true range trailing stops. this is a derivative of the true range indicator\n",
    "find the greatest of\n",
    "today's high minus today's low.\n",
    "today's high minus yesterday's close\n",
    "yesterday's close minus today's low.\n",
    "when the 10 ATR reaches 10%, 20% or 55%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111a4f1-23e6-4530-9e1a-6e9a72ec16f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minimum distance between normalized historical prices\n",
    "\n",
    "pairs trading of downtrend with bounce on resistance\n",
    "## buy\n",
    "RMI, Momentum Pinball, PCC, MA Streak\n",
    "average day range, don't buy when near daily peak, \n",
    "\n",
    "\n",
    "informative pair? BTC/USD. we only buy alt coins, when BTC is doing well.\n",
    "alt coins do well even when BTC is doing poorly. We try to only make trades when BTC is looking bullish.\n",
    "\n",
    "\n",
    "## sell\n",
    "\n",
    "dynamic roi table?\n",
    "Csell_roi_start\n",
    "decay between csell_roi_start to csell_end over time\n",
    "step, will do a step 1 of roi start and step 2 of roi end\n",
    "\n",
    "\n",
    "    def populate_indicators(self, dataframe: DataFrame, metadata: dict) -> DataFrame:\n",
    "        if not metadata['pair'] in self.custom_trade_info:\n",
    "            self.custom_trade_info[metadata['pair']] = {}\n",
    "            if not 'had-trend' in self.custom_trade_info[metadata[\"pair\"]]:\n",
    "                self.custom_trade_info[metadata['pair']]['had-trend'] = False\n",
    "\n",
    "        ## Base Timeframe / Pair\n",
    "\n",
    "        # Kaufmann Adaptive Moving Average\n",
    "        dataframe['kama'] = ta.KAMA(dataframe, length=233)\n",
    "    \n",
    "        # RMI: https://www.tradingview.com/script/kwIt9OgQ-Relative-Momentum-Index/\n",
    "        dataframe['rmi'] = cta.RMI(dataframe, length=24, mom=5)\n",
    "\n",
    "        # Momentum Pinball: https://www.tradingview.com/script/fBpVB1ez-Momentum-Pinball-Indicator/\n",
    "        dataframe['roc-mp'] = ta.ROC(dataframe, timeperiod=1)\n",
    "        dataframe['mp']  = ta.RSI(dataframe['roc-mp'], timeperiod=3)\n",
    "\n",
    "        # MA Streak: https://www.tradingview.com/script/Yq1z7cIv-MA-Streak-Can-Show-When-a-Run-Is-Getting-Long-in-the-Tooth/\n",
    "        dataframe['mastreak'] = cta.mastreak(dataframe, period=4)\n",
    "\n",
    "        # Percent Change Channel: https://www.tradingview.com/script/6wwAWXA1-MA-Streak-Change-Channel/\n",
    "        upper, mid, lower = cta.pcc(dataframe, period=40, mult=3)\n",
    "        dataframe['pcc-lowerband'] = lower\n",
    "        dataframe['pcc-upperband'] = upper\n",
    "\n",
    "        lookup_idxs = dataframe.index.values - (abs(dataframe['mastreak'].values) + 1)\n",
    "        valid_lookups = lookup_idxs >= 0\n",
    "        dataframe['sbc'] = np.nan\n",
    "        dataframe.loc[valid_lookups, 'sbc'] = dataframe['close'].to_numpy()[lookup_idxs[valid_lookups].astype(int)]\n",
    "\n",
    "        dataframe['streak-roc'] = 100 * (dataframe['close'] - dataframe['sbc']) / dataframe['sbc']\n",
    "\n",
    "        # Trends, Peaks and Crosses\n",
    "        dataframe['candle-up'] = np.where(dataframe['close'] >= dataframe['open'],1,0)\n",
    "        dataframe['candle-up-trend'] = np.where(dataframe['candle-up'].rolling(5).sum() >= 3,1,0)\n",
    "\n",
    "        dataframe['rmi-up'] = np.where(dataframe['rmi'] >= dataframe['rmi'].shift(),1,0)      \n",
    "        dataframe['rmi-up-trend'] = np.where(dataframe['rmi-up'].rolling(5).sum() >= 3,1,0)      \n",
    "\n",
    "        dataframe['rmi-dn'] = np.where(dataframe['rmi'] <= dataframe['rmi'].shift(),1,0)\n",
    "        dataframe['rmi-dn-count'] = dataframe['rmi-dn'].rolling(8).sum()\n",
    "\n",
    "        dataframe['streak-bo'] = np.where(dataframe['streak-roc'] < dataframe['pcc-lowerband'],1,0)\n",
    "        dataframe['streak-bo-count'] = dataframe['streak-bo'].rolling(8).sum()\n",
    "\n",
    "        # Indicators used only for ROI and Custom Stoploss\n",
    "        ssldown, sslup = cta.SSLChannels_ATR(dataframe, length=21)\n",
    "        dataframe['sroc'] = cta.SROC(dataframe, roclen=21, emalen=13, smooth=21)\n",
    "        dataframe['ssl-dir'] = np.where(sslup > ssldown,'up','down')\n",
    "\n",
    "        # Base pair informative timeframe indicators\n",
    "        informative = self.dp.get_pair_dataframe(pair=metadata['pair'], timeframe=self.inf_timeframe)\n",
    "        \n",
    "        # Get the \"average day range\" between the 1d high and 1d low to set up guards\n",
    "        informative['1d-high'] = informative['close'].rolling(24).max()\n",
    "        informative['1d-low'] = informative['close'].rolling(24).min()\n",
    "        informative['adr'] = informative['1d-high'] - informative['1d-low']\n",
    "\n",
    "        dataframe = merge_informative_pair(dataframe, informative, self.timeframe, self.inf_timeframe, ffill=True)\n",
    "\n",
    "        # Other stake specific informative indicators\n",
    "        # e.g if stake is BTC and current coin is XLM (pair: XLM/BTC)\n",
    "        if self.config['stake_currency'] in ('BTC', 'ETH'):\n",
    "            coin, stake = metadata['pair'].split('/')\n",
    "            fiat = self.custom_fiat\n",
    "            coin_fiat = f\"{coin}/{fiat}\"\n",
    "            stake_fiat = f\"{stake}/{fiat}\"\n",
    "\n",
    "            # Informative COIN/FIAT e.g. XLM/USD - Base Timeframe\n",
    "            coin_fiat_tf = self.dp.get_pair_dataframe(pair=coin_fiat, timeframe=self.timeframe)\n",
    "            dataframe[f\"{fiat}_rmi\"] = cta.RMI(coin_fiat_tf, length=55, mom=5)\n",
    "\n",
    "            # Informative STAKE/FIAT e.g. BTC/USD - Base Timeframe\n",
    "            stake_fiat_tf = self.dp.get_pair_dataframe(pair=stake_fiat, timeframe=self.timeframe)\n",
    "            dataframe[f\"{stake}_rmi\"] = cta.RMI(stake_fiat_tf, length=55, mom=5)\n",
    "\n",
    "        # Informatives for BTC/STAKE if not in whitelist\n",
    "        else:\n",
    "            pairs = self.dp.current_whitelist()\n",
    "            btc_stake = f\"BTC/{self.config['stake_currency']}\"\n",
    "            if not btc_stake in pairs:\n",
    "                self.custom_btc_inf = True\n",
    "                # BTC/STAKE - Base Timeframe\n",
    "                btc_stake_tf = self.dp.get_pair_dataframe(pair=btc_stake, timeframe=self.timeframe)\n",
    "                dataframe['BTC_rmi'] = cta.RMI(btc_stake_tf, length=55, mom=5)\n",
    "                dataframe['BTC_close'] = btc_stake_tf['close']\n",
    "                dataframe['BTC_kama'] = ta.KAMA(btc_stake_tf, length=144)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    Buy Signal\n",
    "    \"\"\" \n",
    "    def populate_buy_trend(self, dataframe: DataFrame, metadata: dict) -> DataFrame:\n",
    "        conditions = []\n",
    "\n",
    "        # Informative Timeframe Guards\n",
    "        conditions.append(\n",
    "            (dataframe['close'] <= dataframe[f\"1d-low_{self.inf_timeframe}\"] + \n",
    "            (self.inf_pct_adr.value * dataframe[f\"adr_{self.inf_timeframe}\"]))\n",
    "        )\n",
    "\n",
    "        # Base Timeframe Guards\n",
    "        conditions.append(\n",
    "            (dataframe['rmi-dn-count'] >= self.base_rmi_streak.value) &\n",
    "            (dataframe['streak-bo-count'] >= self.base_ma_streak.value) &\n",
    "            (dataframe['rmi'] <= self.base_rmi_max.value) &\n",
    "            (dataframe['rmi'] >= self.base_rmi_min.value) &\n",
    "            (dataframe['mp'] <= self.base_mp.value)        \n",
    "        )\n",
    "\n",
    "        # Base Timeframe Trigger\n",
    "        if self.base_trigger.value == 'pcc':\n",
    "            conditions.append(qtpylib.crossed_above(dataframe['streak-roc'], dataframe['pcc-lowerband']))\n",
    "\n",
    "        if self.base_trigger.value == 'rmi':\n",
    "            conditions.append(dataframe['rmi-up-trend'] == 1)\n",
    "\n",
    "        # Extra conditions for */BTC and */ETH stakes on additional informative pairs\n",
    "        if self.config['stake_currency'] in ('BTC', 'ETH'):\n",
    "            conditions.append(\n",
    "                (dataframe[f\"{self.custom_fiat}_rmi\"] > self.xtra_base_fiat_rmi.value) |\n",
    "                (dataframe[f\"{self.config['stake_currency']}_rmi\"] < self.xtra_base_stake_rmi.value)\n",
    "            )\n",
    "        # Extra conditions for BTC/STAKE if not in whitelist\n",
    "        else:\n",
    "            if self.custom_btc_inf:\n",
    "                if self.xbtc_guard.value == 'strict':\n",
    "                    conditions.append(\n",
    "                        (\n",
    "                            (dataframe['BTC_rmi'] > self.xbtc_base_rmi.value) &\n",
    "                            (dataframe['BTC_close'] > dataframe['BTC_kama'])\n",
    "                        )\n",
    "                    )\n",
    "                if self.xbtc_guard.value == 'lazy':\n",
    "                    conditions.append(\n",
    "                        (dataframe['close'] > dataframe['kama']) |\n",
    "                        (\n",
    "                            (dataframe['BTC_rmi'] > self.xbtc_base_rmi.value) &\n",
    "                            (dataframe['BTC_close'] > dataframe['BTC_kama'])\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        conditions.append(dataframe['volume'].gt(0))\n",
    "\n",
    "        if conditions:\n",
    "            dataframe.loc[\n",
    "                reduce(lambda x, y: x & y, conditions),\n",
    "                'buy'] = 1\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    Sell Signal\n",
    "    \"\"\"\n",
    "    def populate_sell_trend(self, dataframe: DataFrame, metadata: dict) -> DataFrame:\n",
    "  \n",
    "        dataframe['sell'] = 0\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    Custom Stoploss\n",
    "    \"\"\"\n",
    "    def custom_stoploss(self, pair: str, trade: 'Trade', current_time: datetime, current_rate: float, current_profit: float, **kwargs) -> float:\n",
    "\n",
    "        dataframe, _ = self.dp.get_analyzed_dataframe(pair=pair, timeframe=self.timeframe)\n",
    "        last_candle = dataframe.iloc[-1].squeeze()\n",
    "        trade_dur = int((current_time.timestamp() - trade.open_date_utc.timestamp()) // 60)\n",
    "        in_trend = self.custom_trade_info[trade.pair]['had-trend']\n",
    "\n",
    "        # Determine how we sell when we are in a loss\n",
    "        if current_profit < self.cstop_loss_threshold.value:\n",
    "            if self.cstop_bail_how.value == 'roc' or self.cstop_bail_how.value == 'any':\n",
    "                # Dynamic bailout based on rate of change\n",
    "                if last_candle['sroc'] <= self.cstop_bail_roc.value:\n",
    "                    return 0.01\n",
    "            if self.cstop_bail_how.value == 'time' or self.cstop_bail_how.value == 'any':\n",
    "                # Dynamic bailout based on time, unless time_trend is true and there is a potential reversal\n",
    "                if trade_dur > self.cstop_bail_time.value:\n",
    "                    if self.cstop_bail_time_trend.value == True and in_trend == True:\n",
    "                        return 1\n",
    "                    else:\n",
    "                        return 0.01\n",
    "        return 1\n",
    "\n",
    "    \"\"\"\n",
    "    Custom Sell\n",
    "    \"\"\"\n",
    "    def custom_sell(self, pair: str, trade: 'Trade', current_time: 'datetime', current_rate: float,\n",
    "                    current_profit: float, **kwargs):\n",
    "                    \n",
    "        dataframe, _ = self.dp.get_analyzed_dataframe(pair=pair, timeframe=self.timeframe)\n",
    "        last_candle = dataframe.iloc[-1].squeeze()\n",
    "\n",
    "        trade_dur = int((current_time.timestamp() - trade.open_date_utc.timestamp()) // 60)\n",
    "        max_profit = max(0, trade.calc_profit_ratio(trade.max_rate))\n",
    "        pullback_value = max(0, (max_profit - self.csell_pullback_amount.value))\n",
    "        in_trend = False\n",
    "\n",
    "        # Determine our current ROI point based on the defined type\n",
    "        if self.csell_roi_type.value == 'static':\n",
    "            min_roi = self.csell_roi_start.value\n",
    "        elif self.csell_roi_type.value == 'decay':\n",
    "            min_roi = cta.linear_decay(self.csell_roi_start.value, self.csell_roi_end.value, 0, self.csell_roi_time.value, trade_dur)\n",
    "        elif self.csell_roi_type.value == 'step':\n",
    "            if trade_dur < self.csell_roi_time.value:\n",
    "                min_roi = self.csell_roi_start.value\n",
    "            else:\n",
    "                min_roi = self.csell_roi_end.value\n",
    "\n",
    "        # Determine if there is a trend\n",
    "        if self.csell_trend_type.value == 'rmi' or self.csell_trend_type.value == 'any':\n",
    "            if last_candle['rmi-up-trend'] == 1:\n",
    "                in_trend = True\n",
    "        if self.csell_trend_type.value == 'ssl' or self.csell_trend_type.value == 'any':\n",
    "            if last_candle['ssl-dir'] == 'up':\n",
    "                in_trend = True\n",
    "        if self.csell_trend_type.value == 'candle' or self.csell_trend_type.value == 'any':\n",
    "            if last_candle['candle-up-trend'] == 1:\n",
    "                in_trend = True\n",
    "\n",
    "        # Don't sell if we are in a trend unless the pullback threshold is met\n",
    "        if in_trend == True and current_profit > 0:\n",
    "            # Record that we were in a trend for this trade/pair for a more useful sell message later\n",
    "            self.custom_trade_info[trade.pair]['had-trend'] = True\n",
    "            # If pullback is enabled and profit has pulled back allow a sell, maybe\n",
    "            if self.csell_pullback.value == True and (current_profit <= pullback_value):\n",
    "                if self.csell_pullback_respect_roi.value == True and current_profit > min_roi:\n",
    "                    return 'intrend_pullback_roi' \n",
    "                elif self.csell_pullback_respect_roi.value == False: \n",
    "                    if current_profit > min_roi:\n",
    "                        return 'intrend_pullback_roi'\n",
    "                    else:\n",
    "                        return 'intrend_pullback_noroi'\n",
    "            # We are in a trend and pullback is disabled or has not happened or various criteria were not met, hold\n",
    "            return None\n",
    "        # If we are not in a trend, just use the roi value\n",
    "        elif in_trend == False:\n",
    "            if self.custom_trade_info[trade.pair]['had-trend']:\n",
    "                if current_profit > min_roi:\n",
    "                    self.custom_trade_info[trade.pair]['had-trend'] = False\n",
    "                    return 'trend_roi'\n",
    "                elif self.csell_endtrend_respect_roi.value == False:\n",
    "                    self.custom_trade_info[trade.pair]['had-trend'] = False\n",
    "                    return 'trend_noroi'\n",
    "            elif current_profit > min_roi: \n",
    "                return 'notrend_roi'\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bccb00-a1a9-4c1f-834d-7c8f62a6a59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#relative strength index(rsi) is another popular indicator for technical analysis\n",
    "#actually i believe its kinda bull shit\n",
    "#normally i read stuff on trading view wiki\n",
    "#its not like i work there and try to promote it\n",
    "#trading view wiki is a very detailed encyclopedia for different indicators\n",
    "#plz refer to the following link for more details\n",
    "# https://www.tradingview.com/wiki/Relative_Strength_Index_(RSI)\n",
    "\n",
    "#on trading view wiki, there are a couple of strategies to use rsi\n",
    "#the simplest one is overbought/oversold\n",
    "#that is what this script is about\n",
    "#we just set upper/lower boundaries capped at 30/70 for rsi\n",
    "#if rsi exceeds the bound, we bet the stock would go under price correction\n",
    "\n",
    "#another one is called divergence\n",
    "#rsi goes up and price actually goes down\n",
    "#the inventor of rsi called wilder believes bearish rsi divergence creates a selling opportunity \n",
    "#but his protege cardwell believes bearish divergence only occurs in a bullish trend\n",
    "#so their ideas basically contradict to each other\n",
    "#i would undoubtedly give up on this bs divergence strategy\n",
    "\n",
    "#the last one is called failure swing\n",
    "#its kinda like a double bottom pattern in price itself\n",
    "#except this strategy is a pattern recognition on rsi\n",
    "#since i have written bottom w pattern for bollinger bands\n",
    "#i would not do it here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fix_yahoo_finance as yf\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#smoothed moving average\n",
    "#for details plz refer to wikipedia\n",
    "# https://en.wikipedia.org/wiki/Moving_average#Modified_moving_average\n",
    "def smma(series,n):\n",
    "    \n",
    "    output=[series[0]]\n",
    "    \n",
    "    for i in range(1,len(series)):\n",
    "        temp=output[-1]*(n-1)+series[i]\n",
    "        output.append(temp/n)\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#calculating rsi is very simple\n",
    "#except there are several versions of moving average for rsi\n",
    "#simple moving average, exponentially weighted moving average, etc\n",
    "#in this script, we use smoothed moving average(the authentic way)\n",
    "def rsi(data,n=14):\n",
    "    \n",
    "    delta=data.diff().dropna()\n",
    "    \n",
    "    up=np.where(delta>0,delta,0)\n",
    "    down=np.where(delta<0,-delta,0)\n",
    "    \n",
    "    rs=np.divide(smma(up,n),smma(down,n))\n",
    "    \n",
    "    output=100-100/(1+rs)\n",
    "    \n",
    "    return output[n-1:]\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#signal generation\n",
    "#it is really easy\n",
    "#when rsi goes above 70, we short the stock\n",
    "#we bet the stock price would fall\n",
    "#vice versa\n",
    "def signal_generation(df,method,n=14):\n",
    "    \n",
    "    df['rsi']=0.0\n",
    "    df['rsi'][n:]=method(df['Close'],n=14)\n",
    "    \n",
    "    df['positions']=np.select([df['rsi']<30,df['rsi']>70], \\\n",
    "                              [1,-1],default=0)\n",
    "    df['signals']=df['positions'].diff()\n",
    "    \n",
    "    return df[n:]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#plotting\n",
    "def plot(new,ticker):\n",
    "    \n",
    "    #the first plot is the actual close price with long/short positions\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    ax=fig.add_subplot(211)\n",
    "    \n",
    "    new['Close'].plot(label=ticker)\n",
    "    ax.plot(new.loc[new['signals']==1].index,\n",
    "            new['Close'][new['signals']==1],\n",
    "            label='LONG',lw=0,marker='^',c='g')\n",
    "    ax.plot(new.loc[new['signals']==-1].index,\n",
    "            new['Close'][new['signals']==-1],\n",
    "            label='SHORT',lw=0,marker='v',c='r')\n",
    "\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.title('Positions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('price')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #the second plot is rsi with overbought/oversold interval capped at 30/70\n",
    "    bx=plt.figure(figsize=(10,10)).add_subplot(212,sharex=ax)\n",
    "    new['rsi'].plot(label='relative strength index',c='#522e75')\n",
    "    bx.fill_between(new.index,30,70,alpha=0.5,color='#f22f08')\n",
    "    \n",
    "    bx.text(new.index[-45],75,'overbought',color='#594346',size=12.5)\n",
    "    bx.text(new.index[-45],25,'oversold',color='#594346',size=12.5)\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('value')\n",
    "    plt.title('RSI')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#pattern recognition\n",
    "#do u really think i would write such an easy script?\n",
    "#dont be naive, here is another way of using rsi\n",
    "#unlike double bottom pattern for bollinger bands\n",
    "#this is head-shoulder pattern directly on rsi instead of price\n",
    "#well, it is actually named head and shoulders\n",
    "#but i refused to do free marketing for the shampoo\n",
    "#cuz that shampoo doesnt work at all!\n",
    "#the details of head-shoulder pattern could be found in this link\n",
    "# https://www.investopedia.com/terms/h/head-shoulders.asp\n",
    "\n",
    "#any way, this pattern recognition is similar to the one in bollinger bands\n",
    "#plz refer to bollinger bands for a detailed explanation\n",
    "# https://github.com/je-suis-tm/quant-trading/blob/master/Bollinger%20Bands%20Pattern%20Recognition%20backtest.py\n",
    "def pattern_recognition(df,method,lag=14):\n",
    "    \n",
    "    df['rsi']=0.0\n",
    "    df['rsi'][lag:]=method(df['Close'],lag)    \n",
    "    \n",
    "    #as usual, period is defined as the horizon for finding the pattern\n",
    "    period=25    \n",
    "    \n",
    "    #delta is the threshold of the difference between two prices\n",
    "    #if the difference is smaller than delta\n",
    "    #we can conclude two prices are not significantly different from each other\n",
    "    #the significant level is defined as delta\n",
    "    delta=0.2\n",
    "    \n",
    "    #these are the multipliers of delta\n",
    "    #we wanna make sure there is head and shoulders are significantly larger than other nodes\n",
    "    #the significant level is defined as head/shoulder multiplier*delta\n",
    "    head=1.1\n",
    "    shoulder=1.1\n",
    "    \n",
    "    df['signals']=0\n",
    "    df['cumsum']=0\n",
    "    df['coordinates']=''\n",
    "    \n",
    "    #now these are the parameters set by us based on experience\n",
    "    #entry_rsi is the rsi when we enter a trade\n",
    "    #we would exit the trade based on two conditions\n",
    "    #one is that we hold the stock for more than five days\n",
    "    #the variable for five days is called exit_days\n",
    "    #we use a variable called counter to keep track of it\n",
    "    #two is that rsi has increased more than 4 since the entry\n",
    "    #the variable for 4 is called exit_rsi\n",
    "    #when either condition is triggered, we exit the trade\n",
    "    #this is a lazy way to exit the trade\n",
    "    #cuz i dont wanna import indicators from other scripts\n",
    "    #i would suggest people to use other indicators such as macd or bollinger bands\n",
    "    #exiting trades based on rsi is definitely inefficient and unprofitable\n",
    "    entry_rsi=0.0\n",
    "    counter=0\n",
    "    exit_rsi=4\n",
    "    exit_days=5\n",
    "    \n",
    "    #signal generation\n",
    "    #plz refer to the following link for pattern visualization\n",
    "    # https://github.com/je-suis-tm/quant-trading/blob/master/preview/rsi%20head-shoulder%20pattern.png\n",
    "    #the idea is to start with the first node i\n",
    "    #we look backwards and find the head node j with maximum value in pattern finding period\n",
    "    #between node i and node j, we find a node k with its value almost the same as node i\n",
    "    #started from node j to left, we find a node l with its value almost the same as node i\n",
    "    #between the left beginning and node l, we find a node m with its value almost the same as node i\n",
    "    #after that, we find the shoulder node n with maximum value between node m and node l\n",
    "    #finally, we find the shoulder node o with its value almost the same as node n\n",
    "    for i in range(period+lag,len(df)):\n",
    "        \n",
    "        #this is pretty much the same idea as in bollinger bands\n",
    "        #except we have two variables\n",
    "        #one for shoulder and one for the bottom nodes\n",
    "        moveon=False\n",
    "        top=0.0\n",
    "        bottom=0.0\n",
    "        \n",
    "        #we have to make sure no holding positions\n",
    "        #and the close price is not the maximum point of pattern finding horizon\n",
    "        if (df['cumsum'][i]==0) and  \\\n",
    "        (df['Close'][i]!=max(df['Close'][i-period:i])):\n",
    "            \n",
    "            #get the head node j with maximum value in pattern finding period\n",
    "            #note that dataframe is in datetime index\n",
    "            #we wanna convert the result of idxmax to a numerical index number\n",
    "            j=df.index.get_loc(df['Close'][i-period:i].idxmax())\n",
    "            \n",
    "            #if the head node j is significantly larger than node i\n",
    "            #we would move on to the next phrase\n",
    "            if (np.abs(df['Close'][j]-df['Close'][i])>head*delta):\n",
    "                bottom=df['Close'][i]\n",
    "                moveon=True\n",
    "            \n",
    "            #we try to find node k between node j and node i\n",
    "            #if node k is not significantly different from node i\n",
    "            #we would move on to the next phrase\n",
    "            if moveon==True:\n",
    "                moveon=False\n",
    "                for k in range(j,i):    \n",
    "                    if (np.abs(df['Close'][k]-bottom)<delta):\n",
    "                        moveon=True\n",
    "                        break\n",
    "            \n",
    "            #we try to find node l between node j and the end of pattern finding horizon\n",
    "            #note that we start from node j to the left\n",
    "            #cuz we need to find another bottom node m later which would start from the left beginning\n",
    "            #this way we can make sure we would find a shoulder node n between node m and node l\n",
    "            #if node l is not significantly different from node i\n",
    "            #we would move on to the next phrase\n",
    "            if moveon==True:\n",
    "                moveon=False\n",
    "                for l in range(j,i-period+1,-1):\n",
    "                    if (np.abs(df['Close'][l]-bottom)<delta):\n",
    "                        moveon=True\n",
    "                        break\n",
    "                    \n",
    "            #we try to find node m between node l and the end of pattern finding horizon\n",
    "            #this time we start from left to right as usual\n",
    "            #if node m is not significantly different from node i\n",
    "            #we would move on to the next phrase\n",
    "            if moveon==True:\n",
    "                moveon=False        \n",
    "                for m in range(i-period,l):\n",
    "                    if (np.abs(df['Close'][m]-bottom)<delta):\n",
    "                        moveon=True\n",
    "                        break\n",
    "            \n",
    "            #get the shoulder node n with maximum value between node m and node l\n",
    "            #note that dataframe is in datetime index\n",
    "            #we wanna convert the result of idxmax to a numerical index number\n",
    "            #if node n is significantly larger than node i and significantly smaller than node j\n",
    "            #we would move on to the next phrase\n",
    "            if moveon==True:\n",
    "                moveon=False        \n",
    "                n=df.index.get_loc(df['Close'][m:l].idxmax())\n",
    "                if (df['Close'][n]-bottom>shoulder*delta) and \\\n",
    "                (df['Close'][j]-df['Close'][n]>shoulder*delta):\n",
    "                    top=df['Close'][n]\n",
    "                    moveon=True\n",
    "                    \n",
    "            #we try to find shoulder node o between node k and node i\n",
    "            #if node o is not significantly different from node n\n",
    "            #we would set up the signals and coordinates for visualization\n",
    "            #we also need to refresh cumsum and entry_rsi for exiting the trade\n",
    "            #note that moveon is still set as True\n",
    "            #it would help the algo to ignore this round of iteration for exiting the trade\n",
    "            if moveon==True:        \n",
    "                for o in range(k,i):\n",
    "                    if (np.abs(df['Close'][o]-top)<delta):\n",
    "                        df.at[df.index[i],'signals']=-1\n",
    "                        df.at[df.index[i],'coordinates']='%s,%s,%s,%s,%s,%s,%s'%(m,n,l,j,k,o,i)\n",
    "                        df['cumsum']=df['signals'].cumsum()\n",
    "                        entry_rsi=df['rsi'][i]\n",
    "                        moveon=True\n",
    "                        break\n",
    "        \n",
    "        #each time we have a holding position\n",
    "        #counter would steadily increase\n",
    "        #if either of the exit conditions is met\n",
    "        #we exit the trade with long position\n",
    "        #and we refresh counter, entry_rsi and cumsum\n",
    "        #you may wonder why do we need cumsum?\n",
    "        #well, this is for holding positions in case you wanna check on portfolio performance\n",
    "        if entry_rsi!=0 and moveon==False:\n",
    "            counter+=1\n",
    "            if (df['rsi'][i]-entry_rsi>exit_rsi) or \\\n",
    "            (counter>exit_days):\n",
    "                df.at[df.index[i],'signals']=1\n",
    "                df['cumsum']=df['signals'].cumsum()\n",
    "                counter=0\n",
    "                entry_rsi=0\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "#visualize the pattern\n",
    "def pattern_plot(new,ticker):\n",
    "    \n",
    "    #this part is to get a small slice of dataframe\n",
    "    #so we can get a clear view of head-shoulder pattern\n",
    "    a,b=list(new[new['signals']!=0].iloc[2:4].index)\n",
    "    \n",
    "    #extract coordinates for head-shoulder pattern visualization\n",
    "    temp=list(map(int,new['coordinates'][a].split(',')))\n",
    "    indexlist=list(map(lambda x:new.index[x],temp))\n",
    "    \n",
    "    #slicing\n",
    "    c=new.index.get_loc(b)\n",
    "    newbie=new[temp[0]-30:c+20]\n",
    "    \n",
    "    #first plot is always price with positions\n",
    "    ax=plt.figure(figsize=(10,10)).add_subplot(211)\n",
    "        \n",
    "    newbie['Close'].plot(label=ticker)\n",
    "    ax.plot(newbie['Close'][newbie['signals']==1],marker='^',markersize=12, \\\n",
    "            lw=0,c='g',label='LONG')\n",
    "    ax.plot(newbie['Close'][newbie['signals']==-1],marker='v',markersize=12, \\\n",
    "            lw=0,c='r',label='SHORT')\n",
    "    \n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Positions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('price')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #second plot is head-shoulder pattern on rsi\n",
    "    bx=plt.figure(figsize=(10,10)).add_subplot(212,sharex=ax)\n",
    "    \n",
    "    newbie['rsi'].plot(label='relative strength index',c='#f4ed71')\n",
    "    \n",
    "    #we plot the overbought/oversold interval, positions and pattern\n",
    "    bx.fill_between(newbie.index,30,70,alpha=0.6,label='overbought/oversold range',color='#000d29')\n",
    "    bx.plot(newbie['rsi'][indexlist], \\\n",
    "            lw=3,alpha=0.7,marker='o', \\\n",
    "            markersize=6,c='#8d2f23',label='head-shoulder pattern')\n",
    "    bx.plot(newbie['rsi'][newbie['signals']==1],marker='^',markersize=12, \\\n",
    "            lw=0,c='g',label='LONG')\n",
    "    bx.plot(newbie['rsi'][newbie['signals']==-1],marker='v',markersize=12, \\\n",
    "            lw=0,c='r',label='SHORT')\n",
    "\n",
    "    #put some captions on head and shoulders\n",
    "    for i in [(1,'Shoulder'),(3,'Head'),(5,'Shoulder')]:\n",
    "        plt.text(indexlist[i[0]], newbie['rsi'][indexlist[i[0]]]+2, \\\n",
    "             '%s'%i[1],fontsize=10,color='#e4ebf2', \\\n",
    "             horizontalalignment='center', \\\n",
    "            verticalalignment='center')\n",
    "        \n",
    "    plt.title('RSI')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('value')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    ticker='FCAU'\n",
    "    startdate='2016-01-01'\n",
    "    enddate='2018-01-01'\n",
    "    df=yf.download(ticker,start=startdate,end=enddate)\n",
    "    new=signal_generation(df,rsi,n=14)\n",
    "\n",
    "    plot(new,ticker)\n",
    "\n",
    "\n",
    "#how to calculate stats could be found from my other code called Heikin-Ashi\n",
    "# https://github.com/je-suis-tm/quant-trading/blob/master/heikin%20ashi%20backtest.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    \n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#parabolic stop and reverse is very useful for trend following\n",
    "#sar is an indicator below the price when its an uptrend \n",
    "#and above the price when its a downtrend\n",
    "#it is very painful to calculate sar, though\n",
    "#and many explanations online including wiki cannot clearly explain the process\n",
    "#hence, the good idea would be to read info on wikipedia\n",
    "#and download an excel spreadsheet made by joeu2004\n",
    "#formulas are always more straight forward than descriptions\n",
    "#links are shown below\n",
    "# https://en.wikipedia.org/wiki/Parabolic_SAR\n",
    "# https://www.box.com/s/gbtrjuoktgyag56j6lv0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fix_yahoo_finance as yf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#the calculation of sar\n",
    "#as rules are very complicated\n",
    "#plz check the links above to understand more about it\n",
    "\n",
    "def parabolic_sar(new):\n",
    "    \n",
    "    #this is common accelerating factors for forex and commodity\n",
    "    #for equity, af for each step could be set to 0.01\n",
    "    initial_af=0.02\n",
    "    step_af=0.02\n",
    "    end_af=0.2\n",
    "    \n",
    "    \n",
    "    new['trend']=0\n",
    "    new['sar']=0.0\n",
    "    new['real sar']=0.0\n",
    "    new['ep']=0.0\n",
    "    new['af']=0.0\n",
    "\n",
    "    #initial values for recursive calculation\n",
    "    new['trend'][1]=1 if new['Close'][1]>new['Close'][0] else -1\n",
    "    new['sar'][1]=new['High'][0] if new['trend'][1]>0 else new['Low'][0]\n",
    "    new.at[1,'real sar']=new['sar'][1]\n",
    "    new['ep'][1]=new['High'][1] if new['trend'][1]>0 else new['Low'][1]\n",
    "    new['af'][1]=initial_af\n",
    "\n",
    "    #calculation\n",
    "    for i in range(2,len(new)):\n",
    "        \n",
    "        temp=new['sar'][i-1]+new['af'][i-1]*(new['ep'][i-1]-new['sar'][i-1])\n",
    "        if new['trend'][i-1]<0:\n",
    "            new.at[i,'sar']=max(temp,new['High'][i-1],new['High'][i-2])\n",
    "            temp=1 if new['sar'][i]<new['High'][i] else new['trend'][i-1]-1\n",
    "        else:\n",
    "            new.at[i,'sar']=min(temp,new['Low'][i-1],new['Low'][i-2])\n",
    "            temp=-1 if new['sar'][i]>new['Low'][i] else new['trend'][i-1]+1\n",
    "        new.at[i,'trend']=temp\n",
    "    \n",
    "        \n",
    "        if new['trend'][i]<0:\n",
    "            temp=min(new['Low'][i],new['ep'][i-1]) if new['trend'][i]!=-1 else new['Low'][i]\n",
    "        else:\n",
    "            temp=max(new['High'][i],new['ep'][i-1]) if new['trend'][i]!=1 else new['High'][i]\n",
    "        new.at[i,'ep']=temp\n",
    "    \n",
    "    \n",
    "        if np.abs(new['trend'][i])==1:\n",
    "            temp=new['ep'][i-1]\n",
    "            new.at[i,'af']=initial_af\n",
    "        else:\n",
    "            temp=new['sar'][i]\n",
    "            if new['ep'][i]==new['ep'][i-1]:\n",
    "                new.at[i,'af']=new['af'][i-1]\n",
    "            else:\n",
    "                new.at[i,'af']=min(end_af,new['af'][i-1]+step_af)\n",
    "        new.at[i,'real sar']=temp\n",
    "       \n",
    "        \n",
    "    return new\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#generating signals\n",
    "#idea is the same as macd oscillator\n",
    "#check the website below to learn more\n",
    "# https://github.com/je-suis-tm/quant-trading/blob/master/MACD%20oscillator%20backtest.py\n",
    "\n",
    "def signal_generation(df,method):\n",
    "    \n",
    "        new=method(df)\n",
    "\n",
    "        new['positions'],new['signals']=0,0\n",
    "        new['positions']=np.where(new['real sar']<new['Close'],1,0)\n",
    "        new['signals']=new['positions'].diff()\n",
    "        \n",
    "        return new\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#plotting of sar and trading positions\n",
    "#still similar to macd\n",
    "\n",
    "def plot(new,ticker):\n",
    "    \n",
    "    fig=plt.figure()\n",
    "    ax=fig.add_subplot(111)\n",
    "    \n",
    "    new['Close'].plot(lw=3,label='%s'%ticker)\n",
    "    new['real sar'].plot(linestyle=':',label='Parabolic SAR',color='k')\n",
    "    ax.plot(new.loc[new['signals']==1].index,new['Close'][new['signals']==1],marker='^',color='g',label='LONG',lw=0,markersize=10)\n",
    "    ax.plot(new.loc[new['signals']==-1].index,new['Close'][new['signals']==-1],marker='v',color='r',label='SHORT',lw=0,markersize=10)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Parabolic SAR')\n",
    "    plt.ylabel('price')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #download data via fix yahoo finance library\n",
    "    stdate=('2016-01-01')\n",
    "    eddate=('2018-01-01')\n",
    "    ticker=('EA')\n",
    "\n",
    "    #slice is used for plotting\n",
    "    #a two year dataset with 500 variables would be too much for a figure\n",
    "    slicer=450\n",
    "\n",
    "    df=yf.download(ticker,start=stdate,end=eddate)\n",
    "    \n",
    "    #delete adj close and volume\n",
    "    #as we dont need them\n",
    "    del df['Adj Close']\n",
    "    del df['Volume']\n",
    "\n",
    "    #no need to iterate over timestamp index\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    new=signal_generation(df,parabolic_sar)\n",
    "\n",
    "    #convert back to time series for plotting\n",
    "    #so that we get a date x axis\n",
    "    new.set_index(new['date'],inplace=True)\n",
    "\n",
    "    #shorten our plotting horizon and plot\n",
    "    new=new[slicer:]\n",
    "    plot(new,ticker) \n",
    "\n",
    "#how to calculate stats could be found from my other code called Heikin-Ashi\n",
    "# https://github.com/je-suis-tm/quant-trading/blob/master/heikin%20ashi%20backtest.py\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ebe34-7b3e-4e4e-a39f-8cb8dcfb8ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#shooting star is my friend's fav indicator\n",
    "#the name is poetic and romantic\n",
    "#it is merely a vertical flipped hammer\n",
    "#hammer and shooting star could be confusing\n",
    "#since both of them can be inverted\n",
    "#i memorize them via a simple tune\n",
    "#if u see thor (with hammer),price shall soar\n",
    "#if u see star (shooting star),price shall fall\n",
    "#details of shooting star can be found in investopedia\n",
    "# https://www.investopedia.com/terms/s/shootingstar.asp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yfinance\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#criteria of shooting star\n",
    "def shooting_star(data,lower_bound,body_size):\n",
    "\n",
    "    df=data.copy()\n",
    "\n",
    "    #open>close,red color\n",
    "    df['condition1']=np.where(df['Open']>=df['Close'],1,0)\n",
    "\n",
    "    #a candle with little or no lower wick\n",
    "    df['condition2']=np.where(\n",
    "        (df['Close']-df['Low'])<lower_bound*abs(\n",
    "            df['Close']-df['Open']),1,0)\n",
    "\n",
    "    #a candle with a small lower body\n",
    "    df['condition3']=np.where(abs(\n",
    "        df['Open']-df['Close'])<abs(\n",
    "        np.mean(df['Open']-df['Close']))*body_size,1,0)\n",
    "\n",
    "    #a long upper wick that is at least two times the size of the lower body\n",
    "    df['condition4']=np.where(\n",
    "        (df['High']-df['Open'])>=2*(\n",
    "            df['Open']-df['Close']),1,0)\n",
    "\n",
    "    #price uptrend\n",
    "    df['condition5']=np.where(\n",
    "        df['Close']>=df['Close'].shift(1),1,0)\n",
    "    df['condition6']=np.where(\n",
    "        df['Close'].shift(1)>=df['Close'].shift(2),1,0)\n",
    "\n",
    "    #the next candle's high must stay \n",
    "    #below the high of the shooting star \n",
    "    df['condition7']=np.where(\n",
    "        df['High'].shift(-1)<=df['High'],1,0)\n",
    "\n",
    "    #the next candle's close below \n",
    "    #the close of the shooting star\n",
    "    df['condition8']=np.where(\n",
    "        df['Close'].shift(-1)<=df['Close'],1,0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#signal generation\n",
    "#there are eight criteria according to investopedia\n",
    "def signal_generation(df,method,\n",
    "                      lower_bound=0.2,body_size=0.5,\n",
    "                      stop_threshold=0.05,\n",
    "                      holding_period=7):\n",
    "\n",
    "    #get shooting star conditions\n",
    "    data=method(df,lower_bound,body_size)\n",
    "\n",
    "    #shooting star should suffice all conditions\n",
    "    #in practise,you may find the definition too rigid\n",
    "    #its important to relax a bit on the body size\n",
    "    data['signals']=data['condition1']*data[\n",
    "        'condition2']*data['condition3']*data[\n",
    "        'condition4']*data['condition5']*data[\n",
    "        'condition6']*data['condition7']*data[\n",
    "        'condition8']\n",
    "\n",
    "    #shooting star is a short signal\n",
    "    data['signals']=-data['signals']\n",
    "    \n",
    "    #find exit position\n",
    "    idxlist=data[data['signals']==-1].index\n",
    "    for ind in idxlist:\n",
    "\n",
    "        #entry point\n",
    "        entry_pos=data['Close'].loc[ind]\n",
    "\n",
    "        stop=False\n",
    "        counter=0\n",
    "        while not stop:\n",
    "            ind+=1\n",
    "            counter+=1\n",
    "\n",
    "            #set stop loss/profit at +-5%\n",
    "            if abs(data['Close'].loc[\n",
    "                ind]/entry_pos-1)>stop_threshold:\n",
    "                stop=True\n",
    "                data['signals'].loc[ind]=1\n",
    "\n",
    "            #set maximum holding period at 7 workdays\n",
    "            if counter>=holding_period:\n",
    "                stop=True\n",
    "                data['signals'].loc[ind]=1\n",
    "\n",
    "    #create positions\n",
    "    data['positions']=data['signals'].cumsum()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#since matplotlib remove the candlestick\n",
    "#plus we dont wanna install mpl_finance\n",
    "#we implement our own version\n",
    "#simply use fill_between to construct the bar\n",
    "#use line plot to construct high and low\n",
    "def candlestick(df,ax=None,highlight=None,titlename='',\n",
    "                highcol='High',lowcol='Low',\n",
    "                opencol='Open',closecol='Close',xcol='Date',\n",
    "                colorup='r',colordown='g',highlightcolor='y',\n",
    "                **kwargs):  \n",
    "    \n",
    "    #bar width\n",
    "    #use 0.6 by default\n",
    "    dif=[(-3+i)/10 for i in range(7)]\n",
    "    \n",
    "    if not ax:\n",
    "        ax=plt.figure(figsize=(10,5)).add_subplot(111)\n",
    "    \n",
    "    #construct the bars one by one\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        #width is 0.6 by default\n",
    "        #so 7 data points required for each bar\n",
    "        x=[i+j for j in dif]\n",
    "        y1=[df[opencol].iloc[i]]*7\n",
    "        y2=[df[closecol].iloc[i]]*7\n",
    "\n",
    "        barcolor=colorup if y1[0]>y2[0] else colordown\n",
    "        \n",
    "        #no high line plot if open/close is high\n",
    "        if df[highcol].iloc[i]!=max(df[opencol].iloc[i],df[closecol].iloc[i]):\n",
    "            \n",
    "            #use generic plot to viz high and low\n",
    "            #use 1.001 as a scaling factor\n",
    "            #to prevent high line from crossing into the bar\n",
    "            plt.plot([i,i],\n",
    "                     [df[highcol].iloc[i],\n",
    "                      max(df[opencol].iloc[i],\n",
    "                          df[closecol].iloc[i])*1.001],c='k',**kwargs)\n",
    "    \n",
    "        #same as high\n",
    "        if df[lowcol].iloc[i]!=min(df[opencol].iloc[i],df[closecol].iloc[i]):             \n",
    "            \n",
    "            plt.plot([i,i],\n",
    "                     [df[lowcol].iloc[i],\n",
    "                      min(df[opencol].iloc[i],\n",
    "                          df[closecol].iloc[i])*0.999],c='k',**kwargs)\n",
    "        \n",
    "        #treat the bar as fill between\n",
    "        plt.fill_between(x,y1,y2,\n",
    "                         edgecolor='k',\n",
    "                         facecolor=barcolor,**kwargs)\n",
    "        \n",
    "        if highlight:\n",
    "            if df[highlight].iloc[i]==-1:\n",
    "                plt.fill_between(x,y1,y2,\n",
    "                         edgecolor='k',\n",
    "                         facecolor=highlightcolor,**kwargs)\n",
    "\n",
    "    #only show 5 xticks\n",
    "    plt.xticks([])\n",
    "    plt.grid(True)\n",
    "    plt.title(titlename)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#plotting the backtesting result\n",
    "def plot(data,name):   \n",
    "    \n",
    "    #first plot is candlestick to showcase\n",
    "    ax1=plt.subplot2grid((250,1),(0,0),\n",
    "                         rowspan=120,\n",
    "                         ylabel='Candlestick')\n",
    "    candlestick(data,ax1,\n",
    "                highlight='signals',\n",
    "                highlightcolor='#FFFF00')\n",
    "\n",
    "    #the second plot is the actual price \n",
    "    #with long/short positions as up/down arrows\n",
    "    ax2=plt.subplot2grid((250,1),(130,0),\n",
    "                         rowspan=120,\n",
    "                         ylabel='£ per share',\n",
    "                         xlabel='Date')\n",
    "    ax2.plot(data.index,\n",
    "             data['Close'],\n",
    "             label=name)\n",
    "\n",
    "    #long/short positions are attached to \n",
    "    #the real close price of the stock\n",
    "    #set the line width to zero\n",
    "    #thats why we only observe markers\n",
    "    ax2.plot(data.loc[data['signals']==-1].index,\n",
    "             data['Close'].loc[data['signals']==-1],\n",
    "             marker='v',lw=0,c='r',label='short',\n",
    "             markersize=10)\n",
    "    ax2.plot(data.loc[data['signals']==1].index,\n",
    "             data['Close'].loc[data['signals']==1],\n",
    "             marker='^',lw=0,c='g',label='long',\n",
    "             markersize=10)\n",
    "\n",
    "    #only show five tickers\n",
    "    plt.xticks(range(0,len(data),len(data)//5),\n",
    "               data['Date'][0::len(data)//5].dt.date)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.tight_layout(pad=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #initializing\n",
    "    stdate='2000-01-01'\n",
    "    eddate='2021-11-04'\n",
    "    name='Vodafone'\n",
    "    ticker='VOD.L'\n",
    "\n",
    "    df=yfinance.download(ticker,start=stdate,end=eddate)\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Date']=pd.to_datetime(df['Date'])\n",
    "\n",
    "    #signal generation\n",
    "    new=signal_generation(df,shooting_star)\n",
    "\n",
    "    #get subset for better viz to highlight shooting star\n",
    "    subset=new.loc[5268:5283].copy()\n",
    "    subset.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    #viz\n",
    "    plot(subset,name)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d843e2-80fa-4fc0-973a-102c1a4c18ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf0b4a1-6e2c-42fa-92fe-f98e31a31b04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the length of the time horizon 2189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strat PnL</th>\n",
       "      <th>Strat Sigma</th>\n",
       "      <th>Strat Sharpe</th>\n",
       "      <th>Strat Max DD</th>\n",
       "      <th>Benchmark PnL</th>\n",
       "      <th>Benchmark Sigma</th>\n",
       "      <th>Benchmark Sharpe</th>\n",
       "      <th>Benchmark Max DD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.080659</td>\n",
       "      <td>0.419063</td>\n",
       "      <td>-0.311789</td>\n",
       "      <td>-0.978819</td>\n",
       "      <td>1.413695</td>\n",
       "      <td>0.419065</td>\n",
       "      <td>3.254141</td>\n",
       "      <td>-0.579845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Strat PnL  Strat Sigma  Strat Sharpe  Strat Max DD  Benchmark PnL  \\\n",
       "0  -0.080659     0.419063     -0.311789     -0.978819       1.413695   \n",
       "\n",
       "   Benchmark Sigma  Benchmark Sharpe  Benchmark Max DD  \n",
       "0         0.419065          3.254141         -0.579845  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = bb.backtest(initial = 1000, transaction_cost = 0, slippage_cost = 0, strategy = _strat)\n",
    "b.evaluate(_type = [\"MSFT\"], _iter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48553473-bc6d-45bf-b4a9-a66ba152c2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c3f48-ef99-4c73-8e7d-5e5e3c2e33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pnl = 0\n",
    "val = 10000\n",
    "value = []\n",
    "inv = []\n",
    "_inv = 0\n",
    "\n",
    "for i in range(len(data)-1):\n",
    "    \n",
    "    post = a[\"Position\"][i] \n",
    "    \n",
    "    if a[\"strategy\"][i] == \"buy\":\n",
    "        \n",
    "        pos = val / data['Open'][i+1]\n",
    "        val = val - pos * post * 1.01 * data['Open'][i+1]\n",
    "        \n",
    "        if val > 0:\n",
    "            _inv = _inv +  post * 1.01 * pos\n",
    "        else:\n",
    "            _inv = _inv\n",
    "        \n",
    "    if a[\"strategy\"][i] == \"sell\":\n",
    "        \n",
    "        if _inv > 0:\n",
    "            val = val + _inv * post * 0.99 * data['Open'][i+1]\n",
    "            _inv = _inv - post * _inv\n",
    "        else:\n",
    "            val = val\n",
    "            _inv = _inv\n",
    "    \n",
    "    inv.append(_inv)\n",
    "    value.append(val)\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "val = val + _inv * 0.99 * data['Open'][i]\n",
    "_inv = 0\n",
    "print(val / 10000)\n",
    "print((data['Close'][len(data)-1] - data['Close'][0]) / data['Close'][0])\n",
    "np.sum(abs(data['returns']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e168bfa-b345-4000-ad10-2446f6e68726",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Repeated testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "06d9aa45-4b30-4112-931f-422c708d0cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst = []\n",
    "lst2 = []\n",
    "\n",
    "# moving average distance\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if i < 20:\n",
    "        lst.append(data['Close'][i])\n",
    "    else:\n",
    "        lst.append(abs(data['Close'][i] - np.mean(data['Close'][i-20:i])))\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if i < 40:\n",
    "        lst2.append(data['Close'][i])\n",
    "    else:\n",
    "        lst2.append(data['Close'][i] - np.mean(data['Close'][i-40:i]))\n",
    "\n",
    "\n",
    "#plt.plot(data['Close'] - data['Close'][0])\n",
    "plt.plot(lst)\n",
    "\n",
    "#plt.plot(lst2)\n",
    "plt.xlim(40,1700)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5d0a4-9ebb-40b7-bcdf-48c08aa98d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.DataFrame()\n",
    "\n",
    "for i in range(len(lste)):\n",
    "    prices[lste[i]] = TMRW.DATA.data(lste[i],'2014-01-01','2024-05-26')['Close']\n",
    "\n",
    "prices = prices.fillna(0)\n",
    "\n",
    "lst = []\n",
    "for i in range(len(prices.columns)):\n",
    "    if np.mean(prices.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "    \n",
    "prices = prices.drop(prices.columns[lst], axis=1)\n",
    "\n",
    "returns = prices.pct_change()#.dropna()\n",
    "returns = returns.iloc[1:len(returns),:]\n",
    "returns.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "lst = []\n",
    "for i in range(len(returns.columns)):\n",
    "    if np.mean(returns.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "    \n",
    "    \n",
    "returns = returns.drop(returns.columns[lst], axis=1)\n",
    "corr = returns.corr()\n",
    "\n",
    "lst = []\n",
    "for i in range(len(corr)):\n",
    "    \n",
    "    for j in range(len(corr.columns)):\n",
    "        \n",
    "        if i!=j and corr.iloc[i,j] > 0.7:\n",
    "            lst.append([corr.index[i], corr.columns[j]])\n",
    "            \n",
    "co_int = pd.DataFrame(\"\", columns = prices.columns, index = prices.columns)\n",
    "\n",
    "lst2 = []\n",
    "\n",
    "for i in range(len(prices.columns)):\n",
    "    \n",
    "    for j in range(len(prices.columns)):\n",
    "        \n",
    "        if a.perform_coint_test(prices.iloc[:,i], prices.iloc[:,j], False)[1] == False:\n",
    "            co_int.iloc[i,j] = \"False\"\n",
    "        elif a.perform_coint_test(prices.iloc[:,i], prices.iloc[:,j], False)[1] == True:\n",
    "            co_int.iloc[i,j] = \"True\"\n",
    "            lst2.append([co_int.index[i], co_int.columns[j]])\n",
    "\n",
    "lst3 = []\n",
    "for i in lst2:\n",
    "    for j in lst:\n",
    "        \n",
    "        if i == j:\n",
    "            lst3.append(i)\n",
    "            \n",
    "lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92397894-7362-48e7-a39a-8526052ccfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import yfinance as yf\n",
    "\n",
    "fra = one\n",
    "til = today\n",
    "\n",
    "prices = pd.DataFrame()\n",
    "\n",
    "for i in range(1):\n",
    "    prices[lste[i]] = TMRW.DATA.data(lste[i],fra,til)['Close']\n",
    "\n",
    "prices = prices.fillna(0)\n",
    "\n",
    "lst = []\n",
    "for i in range(len(prices.columns)):\n",
    "    if np.mean(prices.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "\n",
    "prices = prices.drop(prices.columns[lst], axis=1)\n",
    "\n",
    "returns = prices.pct_change()#.dropna()\n",
    "returns = returns.iloc[1:len(returns),:]\n",
    "returns.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "returns = returns.fillna(0)\n",
    "\n",
    "values = pd.DataFrame(prices.values)\n",
    "dataframe = pd.concat([values.shift(-5),values.shift(-4),values.shift(-3), values.shift(-2),values.shift(-1), values], axis=1)\n",
    "dataframe.columns = ['t', 't-1', 't-2', 't-3','t-4','t-5']\n",
    "result = dataframe.corr()\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'value': values[0]\n",
    "})\n",
    "#data['autocorr_20'] = data['value'].rolling(window=20).apply(lambda x: x.autocorr())\n",
    "#data['autocorr_5'] = data['value'].rolling(window=5).apply(lambda x: x.autocorr())\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "threshold = 0.5\n",
    "\n",
    "data['long_signal'] = (data['autocorr_30'] >= threshold) & (data['value'].diff() < 0) #& (data['value'].diff() < -1000)\n",
    "# Generate short (sell) signals\n",
    "data['short_signal'] = (data['autocorr_30'] >= threshold) & (data['value'].diff() > 0) #& (data['value'].diff() > 1000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "long_signal_dates = data[data['long_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=12, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "short_signal_dates = data[data['short_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][-20:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Data')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Visualise autocorrelation using the plot_acf function\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_acf(data['value'], lags=30, ax=ax)\n",
    "\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Plot')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pacf(data['value'], lags=30, ax=ax)\n",
    "\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89d103-733f-4d9f-8409-4bbb6c9bf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov switching regression does it right, but only right now\n",
    "we need machine learning to predict the regime switching.\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "cumulated velociy empirical distribution\n",
    "\n",
    "cumulative mean reversion time distribution\n",
    "\n",
    "cumulative mean reversion size distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77891225-00fd-44fc-b573-b475f7a99521",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if k == 0:\n",
    "        v = ed(data['velocity'], 100)\n",
    "    else:\n",
    "        v1 = ed(data['velocity'], 100)\n",
    "        for i in range(len(v1)):\n",
    "            for j in range(1,len(v)-1):\n",
    "\n",
    "                if np.sqrt(v1.iloc[i,0]**2 + v.iloc[j,0]**2) > np.sqrt(v1.iloc[i,0]**2 + v.iloc[j-1,0]**2) and np.sqrt(v1.iloc[i,0]**2 + v.iloc[j+1,0]**2) > np.sqrt(v1.iloc[i,0]**2 + v.iloc[j,0]**2):\n",
    "                    v.iloc[j,0] = v.iloc[j,0] + v1.iloc[i,0]\n",
    "        \n",
    "    lstma5_times = []\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if (data.iloc[i,17] > data.iloc[i,3] and data.iloc[i-1,17] <= data.iloc[i-1,3]):\n",
    "            lstma5_times.append(i)\n",
    "        elif (data.iloc[i,17] < data.iloc[i,3] and data.iloc[i-1,17] >= data.iloc[i-1,3]):\n",
    "            lstma5_times.append(i)\n",
    "\n",
    "    lstma21_times = []\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "\n",
    "        if (data.iloc[i,27] > data.iloc[i,3] and data.iloc[i-1,27] <= data.iloc[i-1,3]):\n",
    "            lstma21_times.append(i)\n",
    "        elif (data.iloc[i,27] < data.iloc[i,3] and data.iloc[i-1,27] >= data.iloc[i-1,3]):\n",
    "            lstma21_times.append(i)\n",
    "\n",
    "    ma5_sizes = []\n",
    "\n",
    "    for i in range(1,len(lstma5_times)):\n",
    "\n",
    "        if (sum(data.iloc[i-1:i, 3]) - sum(data.iloc[i-1:i, 17])) < 0:\n",
    "            ma5_sizes.append(min((data.iloc[i-1:i, 3] - data.iloc[i-1:i, 17])))\n",
    "        elif (sum(data.iloc[i-1:i, 3]) - sum(data.iloc[i-1:i, 17])) > 0:\n",
    "            ma5_sizes.append(max((data.iloc[i-1:i, 3] - data.iloc[i-1:i, 17])))\n",
    "\n",
    "    ma21_sizes = []\n",
    "\n",
    "    for i in range(1,len(lstma21_times)):\n",
    "\n",
    "        if (sum(data.iloc[i-1:i, 3]) - sum(data.iloc[i-1:i, 27])) < 0:\n",
    "            ma21_sizes.append(min((data.iloc[i-1:i, 3] - data.iloc[i-1:i, 27])))\n",
    "        elif (sum(data.iloc[i-1:i, 3]) - sum(data.iloc[i-1:i, 27])) > 0:\n",
    "            ma21_sizes.append(max((data.iloc[i-1:i, 3] - data.iloc[i-1:i, 27])))\n",
    "\n",
    "    if k == 0:\n",
    "        mat1 = ed(pd.DataFrame([lstma5_times, lstma21_times])[0],100)\n",
    "        mat2 = ed(pd.DataFrame([lstma5_times, lstma21_times])[1],100) \n",
    "\n",
    "        mas1 = ed(pd.DataFrame([ma5_sizes, ma21_sizes])[0],100)\n",
    "        mas2 = ed(pd.DataFrame([ma5_sizes, ma21_sizes])[1],100)\n",
    "        \n",
    "    else:\n",
    "        _mat1 = ed(pd.DataFrame([lstma5_times, lstma21_times])[0],100)\n",
    "        for i in range(len(_mat1)):\n",
    "            for j in range(1,len(mat1-1)):\n",
    "\n",
    "                if np.sqrt(_mat1.iloc[i,0]**2 + mat1.iloc[j,0]**2) > np.sqrt(_mat1.iloc[i,0]**2 + mat1.iloc[j-1,0]**2) and np.sqrt(_mat1.iloc[i,0]**2 + mat1.iloc[j+1,0]**2) > np.sqrt(_mat1.iloc[i,0]**2 + mat1.iloc[j,0]**2):\n",
    "                    mat1.iloc[j,0] = mat1.iloc[j,0] + _mat1.iloc[i,0]\n",
    "        \n",
    "        \n",
    "        _mat2 = ed(pd.DataFrame([lstma5_times, lstma21_times])[1],100)\n",
    "        for i in range(len(_mat2)):\n",
    "            for j in range(1,len(mat2)-1):\n",
    "\n",
    "                if np.sqrt(_mat2.iloc[i,0]**2 + mat2.iloc[j,0]**2) > np.sqrt(_mat2.iloc[i,0]**2 + mat2.iloc[j-1,0]**2) and np.sqrt(_mat2.iloc[i,0]**2 + mat2.iloc[j+1,0]**2) > np.sqrt(_mat2.iloc[i,0]**2 + mat2.iloc[j,0]**2):\n",
    "                    mat2.iloc[j,0] = mat2.iloc[j,0] + _mat2.iloc[i,0]\n",
    "\n",
    "        _mas1 = ed(pd.DataFrame([ma5_sizes, ma21_sizes])[0],100)\n",
    "        for i in range(len(_mas1)):\n",
    "            for j in range(1,len(mas1)-1):\n",
    "\n",
    "                if np.sqrt(_mas1.iloc[i,0]**2 + mas1.iloc[j,0]**2) > np.sqrt(_mas1.iloc[i,0]**2 + mas1.iloc[j-1,0]**2) and np.sqrt(_mas1.iloc[i,0]**2 + mas1.iloc[j+1,0]**2) > np.sqrt(_mas1.iloc[i,0]**2 + mas1.iloc[j,0]**2):\n",
    "                    mas1.iloc[j,0] = mas1.iloc[j,0] + _mas1.iloc[i,0]\n",
    "        \n",
    "        _mas2 = ed(pd.DataFrame([ma5_sizes, ma21_sizes])[1],100)      \n",
    "        for i in range(len(_mas2)):\n",
    "            for j in range(1,len(mas2)-1):\n",
    "\n",
    "                if np.sqrt(_mas2.iloc[i,0]**2 + mas2.iloc[j,0]**2) > np.sqrt(_mas2.iloc[i,0]**2 + mas2.iloc[j-1,0]**2) and np.sqrt(_mas2.iloc[i,0]**2 + mas2.iloc[j+1,0]**2) > np.sqrt(_mas2.iloc[i,0]**2 + mas2.iloc[j,0]**2):\n",
    "                    mas2.iloc[j,0] = mas2.iloc[j,0] + _mas2.iloc[i,0]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "plt.plot(v.index, v[0])\n",
    "plt.title(\"returns distribution\")\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "plt.plot(e.index, e[0])\n",
    "plt.title(\"mean reversion size distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "a = pd.DataFrame(mr_times(data), index = [\"MA5\", \"MA21\"]).T\n",
    "e = ed(a['MA5'],100)\n",
    "plt.bar(e.index, e[0])\n",
    "plt.title(\"mean reversion time distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(e.index, e[0])\n",
    "plt.title(\"mean reversion size distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "a21 = list(a21['MA21'].copy())\n",
    "a21 = pd.DataFrame([x for x in a21 if str(x) != 'nan'])\n",
    "e21 = ed(a21[0],100)\n",
    "plt.bar(e21.index, e21[0], width = 0.5,linewidth = 0.5)\n",
    "plt.title(\"mean reversion time distribution\")\n",
    "plt.show()\n",
    "print([sum(e[e.index <= 5][0]), sum(e[(e.index > 5)&(e.index < 15)][0]), sum(e[e.index >= 15][0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afbc81-f69d-4480-b68b-e15f5e5f2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = np.mean(data[['MVSTD3', 'MVSTD5', 'MVSTD10', 'MVSTD21']].tail(5))\n",
    "\n",
    "m = 1+np.mean(data[['MV3', 'MV5', 'MV10', 'MV21']].tail(5))\n",
    "\n",
    "def vg(timeframe = 7, runs = 1000, mu = 1, sigma = 0.5, T = 1.0, S0 = 1.0, rate = 0.05):\n",
    "        #variance gamma\n",
    "        dt = T/timeframe\n",
    "        kappa = 1/rate\n",
    "        dG = np.random.gamma(dt/kappa, kappa, (timeframe, runs))\n",
    "        \n",
    "        dS = mu*dG+sigma*np.random.randn(timeframe, runs)*np.sqrt(dG)\n",
    "        \n",
    "        dS = np.insert(dS, 0, S0, axis=0)\n",
    "        S = np.cumsum(dS, axis=0)\n",
    "        \n",
    "        S = pd.DataFrame(S).T\n",
    "        return S\n",
    "\n",
    "def srd(timeframe = 7, runs = 1000,  x0 = 1, theta = 0.5, mu = 1 ,sigma = 0.25, typ = 'Euler'):\n",
    "    #square root diffusion\n",
    "    walkh = np.zeros((timeframe, runs))\n",
    "    walks = np.zeros((timeframe, runs))\n",
    "    walkh[0] = x0\n",
    "    walks[0] = x0\n",
    "    dt = 1.0\n",
    "\n",
    "    if typ.lower() ==\"euler\":\n",
    "        for t in range(1, timeframe):\n",
    "            walkh[t] = (walkh[t - 1] +\n",
    "                     theta * (mu - np.maximum(walkh[t - 1], 0)) * dt +\n",
    "                     sigma * np.sqrt(np.maximum(walkh[t - 1], 0)) *\n",
    "                     np.sqrt(dt) * np.random.standard_normal(runs))\n",
    "        walks = np.maximum(walkh, 0)\n",
    "\n",
    "    elif typ.lower() ==\"exact\":\n",
    "        for t in range(1, timeframe):\n",
    "            df = 4 * mu * theta / sigma ** 2\n",
    "            c = (sigma ** 2 * (1 - np.exp(-theta * dt))) / (4 * theta)\n",
    "            nc = np.exp(-theta * dt) / c * walks[t - 1] \n",
    "            walks[t] = c * np.random.noncentral_chisquare(df, nc, size=runs)\n",
    "    \n",
    "    walks = pd.DataFrame(walks).T\n",
    "    return walks\n",
    "\n",
    "def emou(timeframe = 7, runs = 1000, x0 = 1, theta = 0.5, mu = 1, sigma = 0.25):\n",
    "    dt = 1.0 / timeframe \n",
    "    walks = np.zeros((timeframe, runs))\n",
    "    walks[0] = x0\n",
    "    for t in range(1, timeframe):\n",
    "        walks[t] = walks[t-1] + theta *(mu - walks[t-1]) * dt + sigma *np.sqrt(dt)* np.random.standard_normal(runs)\n",
    "    walks = pd.DataFrame(walks).T\n",
    "    return walks  \n",
    "  \n",
    "def ctou(T = 30, runs = 10, alpha = 1, gamma = 1, beta = 1, X_0 = 1, random_state = 1,) -> np.ndarray:\n",
    "    \n",
    "    lst = []\n",
    "    for i in range(runs):\n",
    "        \n",
    "        t = np.arange(T, dtype=np.float64)\n",
    "        exp_alpha_t = np.exp(-alpha * t)\n",
    "        dW = np.random.normal(0.0, 1.0, T)\n",
    "        exp_alpha_s = np.exp(alpha * t)\n",
    "        integral_W = np.cumsum(exp_alpha_s * dW)\n",
    "        integral_W = np.insert(integral_W, 0, 0)[:-1]\n",
    "        if X_0 is not None:\n",
    "            _X_0 = X_0\n",
    "        else:\n",
    "            _X_0 = gamma\n",
    "\n",
    "        l = ( _X_0 * exp_alpha_t + gamma * (1 - exp_alpha_t) + beta * exp_alpha_t * integral_W) \n",
    "        lst.append(l)\n",
    "    \n",
    "    return pd.DataFrame(lst)\n",
    "\n",
    "def fsr(timeframe = 7, runs = 1000, alpha = 1, mu = 1, sigma = 0.25, t = 0, T = 7, S0 = 1, sim_type = \"EM\", analytic_EM = True):\n",
    "    dt = 1.0 \n",
    "    N = np.random.randn(timeframe, runs)\n",
    "    S = np.concatenate((S0*np.ones((1, runs)), np.zeros((timeframe, runs))), axis=0)\n",
    "    \n",
    "    \n",
    "    if sim_type==\"EM\":\n",
    "        if analytic_EM==True:\n",
    "            a = (sigma**2)/alpha*(np.exp(-alpha*dt)-np.exp(-2*alpha*dt))\n",
    "            b = mu*(sigma**2)/(2*alpha)*(1-np.exp(-alpha*dt))**2\n",
    "            for i in range(0, timeframe):\n",
    "                S[i+1,:] = mu + (S[i,:]-mu)*np.exp(-alpha*dt) + np.sqrt(a*S[i,:]+b)*N[i,:]\n",
    "                S[i+1,:] = np.maximum(S[i+1,:], np.zeros((1, runs)))\n",
    "        else:\n",
    "            for i in range(0, timeframe):\n",
    "                S[i+1,:] = S[i,:] + alpha*(mu-S[i,:])*dt + sigma*np.sqrt(S[i,:]*dt)*N[i,:]\n",
    "                S[i+1,:] = np.maximum(S[i+1,:], np.zeros((1, runs)))\n",
    "    elif sim_type==\"E\":\n",
    "        d = 4*alpha*mu/(sigma**2)\n",
    "        k = (sigma**2)*(1-np.exp(-alpha*dt))/(4*alpha)\n",
    "        for i in range(0, timeframe):\n",
    "            delta = 4*alpha*S[i,:]/((sigma**2)*(np.exp(alpha*dt)-1))\n",
    "            S[i+1,:] = np.random.noncentral_chisquare(d, delta, (1, runs))*k\n",
    "    else:\n",
    "        raise TypeError(\"sim_type can only take values in [EM, E]\")\n",
    "    \n",
    "    S = pd.DataFrame(S).T\n",
    "    return S\n",
    "\n",
    "def cev(timeframe = 7, runs = 1000, mu = (m-1), sigma = sp, t = 1, T = 1, S0 = 1, beta = 0.12):\n",
    "    # constant elasticity volatility\n",
    "    dt = T/timeframe\n",
    "    dW = np.sqrt(dt)*np.random.randn(timeframe, runs)\n",
    "    S = np.concatenate((S0*np.ones((1, runs)), np.zeros((timeframe, runs))), axis=0) \n",
    "\n",
    "    for i in range(0, timeframe):\n",
    "        S[i+1,:] = S[i,:] + mu*S[i,:]*dt + sigma*(S[i,:]**(beta+1))*dW[i,:]\n",
    "        S[i+1,:] = np.maximum(S[i+1,:], np.zeros((1, runs)))\n",
    "    \n",
    "    S = pd.DataFrame(S).T\n",
    "    return S\n",
    "\n",
    "\n",
    "def heston(timeframe = 7, runs = 1000, mu = m, k = 1, theta = 1, epsilon = sp, t = 1, T = 1, S0 = 1, V0 = 1, rho = 0.05):\n",
    "    \n",
    "        dt = T/timeframe\n",
    "        Nv = np.random.randn(timeframe, runs)\n",
    "        N = np.random.randn(timeframe, runs)\n",
    "        NS = rho*Nv + np.sqrt(1-rho**2)*N\n",
    "        \n",
    "        v = np.concatenate((V0*np.ones((1, runs)), np.zeros((timeframe, runs))), axis=0)\n",
    "        S = np.concatenate((V0*np.ones((1, runs)), np.zeros((timeframe, runs))), axis=0)\n",
    "        \n",
    "        a = (epsilon**2)/k*(np.exp(-k*dt)-np.exp(-2*k*dt))\n",
    "        b = theta*(epsilon**2)/(2*k)*(1-np.exp(-k*dt))**2\n",
    "        for i in range(0, timeframe):\n",
    "            v[i+1,:] = theta + (v[i,:]-theta)*np.exp(-k*dt) + np.sqrt(a*v[i,:]+b)*Nv[i,:]\n",
    "            v[i+1,:] = np.maximum(v[i+1,:], np.zeros((1, runs)))\n",
    "        \n",
    "        for j in range(0, timeframe):\n",
    "            S[j+1,:] = S[j,:] + (mu-0.5*v[j,:])*dt + epsilon*np.sqrt(v[j,:]*dt)*NS[j,:]\n",
    "            S[j+1,:] = np.maximum(S[j+1,:], np.zeros((1, runs)))\n",
    "        \n",
    "        S = pd.DataFrame(S).T\n",
    "        v = pd.DataFrame(v).T\n",
    "        return S, v\n",
    "\n",
    "mean = []\n",
    "std = []\n",
    "cvar = []\n",
    "\n",
    "\n",
    "srd(7,1000, x0 = 1, theta = 0.05, mu = m, sigma = sp, typ = \"Euler\")\n",
    "srd(7,1000, x0 = 1, theta = 0.05, mu = m, sigma = sp, typ = \"exact\")\n",
    "fsr(7, 1000, 0.06, 1,0.06, 0, 6, 1)\n",
    "emou(7, 1000, x0 = 1, theta = 0.05, mu = m, sigma = sp)\n",
    "ctou(7, 1000, beta = 0.125)\n",
    "vg(7,1000)\n",
    "cev(7, 1000, mu = 0, sigma = 0.05)\n",
    "heston(7, 1000, mu = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658bf01a-b5e1-4e40-887b-e78d1f0427cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VWV5'] = TMRW.FINANCE.volume_weighted_velocity(data, window = 5)\n",
    "data['VWV10'] = TMRW.FINANCE.volume_weighted_velocity(data, window = 10)\n",
    "data['VWV60'] = TMRW.FINANCE.volume_weighted_velocity(data, window = 60)\n",
    "\n",
    "data['VWM5'] = TMRW.FINANCE.volume_weighted_mean(data, window = 5)\n",
    "data['VWM10'] = TMRW.FINANCE.volume_weighted_mean(data, window = 10)\n",
    "data['VWM60'] = TMRW.FINANCE.volume_weighted_mean(data, window = 60)\n",
    "\n",
    "MA = data[['Close','MA5', 'MA10', 'MA21', 'MA60']] # time moving averages\n",
    "VWA = data[['Close','VWM5','VWM10','VWmean21','VWM60']] # volume moving averages\n",
    "\n",
    "MR = data[['velocity', 'MV5','MV10','MV21', 'MV60']]\n",
    "VWR = data[['velocity', 'VWV5','VWV10','VWvelocity21','VWV60']]\n",
    "\n",
    "plt.scatter((data['acceleration']/data['volume velocity'])[-100:], data['velocity'].shift(2)[-100:])\n",
    "plt.xlim(-100,100)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(MA.index[-100:-50], MA.Close[-100:-50], color = \"k\", linewidth = 2, label = \"Close\")\n",
    "\n",
    "#plt.plot(MA.index[-100:], MA.MA5[-100:], color = \"r\", linewidth = 1, label = \"MA5\")\n",
    "#plt.plot(MA.index[-100:], MA.MA10[-100:], color = \"g\", linewidth = 1, label = \"MA10\")\n",
    "#plt.plot(MA.index[-100:], MA.MA21[-100:], color = \"y\", linewidth = 1, label = \"MA21\")\n",
    "#plt.plot(MA.index[-100:], MA.MA60[-100:], color = \"b\", linewidth = 1, label = \"MA60\")\n",
    "\n",
    "plt.plot(MA.index[-100:-50], VWA.VWM10[-100:-50], color = \"r\", linewidth = 1, label = \"VA5\")\n",
    "#plt.plot(MA.index[-100:], VWA.VWM10[-100:], color = \"g\", linewidth = 1, label = \"VA10\")\n",
    "#plt.plot(MA.index[-100:], VWA.VWmean21[-100:], color = \"y\", linewidth = 1, label = \"VA21\")\n",
    "#plt.plot(MA.index[-100:], VWA.VWM60[-100:], color = \"b\", linewidth = 1, label = \"VA60\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(MA.Close, MR['MV5']/MA['MA5'], color = \"r\", linewidth = 1, label = \"MA5\")\n",
    "plt.ylim(-0.00002, 0.00002)\n",
    "\n",
    "plt.scatter(MA.Close[-300:], MA.MA5[-300:], color = \"r\", linewidth = 1, label = \"MA5\", )\n",
    "plt.scatter(MA.Close[-300:], VWA.VWM5[-300:], color = \"g\", linewidth = 2, label = \"Close vs VA5\")\n",
    "plt.scatter(MA.MA5[-300:], VWA.VWM5[-300:], color = \"b\", linewidth = 1, label = \"MA5\")\n",
    "plt.scatter(MR.MV21[-300:].shift(-1), VWR.VWvelocity21[-300:], color = \"b\", linewidth = 1, label = \"MV5 vs VV5\")\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "y = x['indicator1']\n",
    "\n",
    "X = x.loc[:, x.columns != 'indicator1']\n",
    "X = X.loc[:, X.columns != 'indicator3']\n",
    "X = X.loc[:, X.columns != 'indicator7']\n",
    "X = X.loc[:, X.columns != 'indicator21']\n",
    "#X = x[['mean velocity 3', 'mean velocity 5', 'UD3indicator', 'UD5indicator', 'volume velocity', 'mean std 3']]\n",
    "X = x[['volume velocity', 'volume acceleration', 'MSTD3', 'MSTD5', 'MSTD10', 'mean velocity 3', 'mean velocity 5', 'mean velocity 10', 'mean std 3', 'mean std 5', 'mean std 10']]\n",
    "\n",
    "clf3 = DecisionTreeClassifier(max_depth=1000)\n",
    "clf3 = clf3.fit(X, y)\n",
    "scores = cross_val_score(clf3, X, y, scoring='accuracy', cv=10)\n",
    "print(\"Accuracy:\",(scores.mean(), scores.std()))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "y = x['indicator1']\n",
    "X = x.loc[:, x.columns != 'indicator1']\n",
    "X = X.loc[:, X.columns != 'indicator3']\n",
    "X = X.loc[:, X.columns != 'indicator7']\n",
    "X = X.loc[:, X.columns != 'indicator21']\n",
    "\n",
    "#X = x[['volume velocity', 'volume acceleration', 'MSTD3', 'MSTD5', 'MSTD10', 'mean velocity 3', 'mean velocity 5', 'mean velocity 10', 'mean std 3', 'mean std 5', 'mean std 10']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=1) # random sampling\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)#, xgb_model=model.load_model('XGB.json'))\n",
    "\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print('****Train Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_train))\n",
    "print('****Test Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_test))\n",
    "\n",
    "#plot feature importance\n",
    "plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "#for at lave predictions skal modellen have alle de satte features\n",
    "\n",
    "#model.save_model('XGB.json')\n",
    "\n",
    "\n",
    "# mean std 10(3), mean std 5(2), mstd21, volume velocity(2), mean velocity 10(3), mean velocity 5(2), mean velocity 3(2). MSTD35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa53303-4639-4d56-b72f-d4307c2ab92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\"\"\"\n",
    "Created on January 30, 2022\n",
    "Heston Model Simulation\n",
    "@author: Nicholas Burgess\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import enum \n",
    "\n",
    "# This class defines puts and calls\n",
    "class OptionType(enum.Enum):\n",
    "    CALL = 1.0\n",
    "    PUT = -1.0\n",
    "\n",
    "# European Option Price evaluating Characteristic Function using COSine Method\n",
    "def CallPutOptionPriceCOSMthd(cf,CP,S0,r,tau,K,N,L):\n",
    "    # cf   - characteristic function as a functon, in the book denoted as \\varphi\n",
    "    # CP   - C for call and P for put\n",
    "    # S0   - Initial stock price\n",
    "    # r    - interest rate (constant)\n",
    "    # tau  - time to maturity\n",
    "    # K    - list of strikes\n",
    "    # N    - Number of cosine expansion terms\n",
    "    # L    - size of truncation domain (type:L=8 or L=10)  \n",
    "        \n",
    "    # reshape K to a column vector\n",
    "    if K is not np.array:\n",
    "        K = np.array(K).reshape([len(K),1])\n",
    "    \n",
    "    #assigning i=sqrt(-1)\n",
    "    i = np.complex(0.0,1.0) \n",
    "    x0 = np.log(S0 / K)   \n",
    "    \n",
    "    # truncation domain\n",
    "    a = 0.0 - L * np.sqrt(tau)\n",
    "    b = 0.0 + L * np.sqrt(tau)\n",
    "    \n",
    "    # sumation from k = 0 to k=N-1\n",
    "    k = np.linspace(0,N-1,N).reshape([N,1])  \n",
    "    u = k * np.pi / (b - a);  \n",
    "\n",
    "    # Determine coefficients for Put Prices  \n",
    "    H_k = CallPutCoefficients(CP,a,b,k)   \n",
    "    mat = np.exp(i * np.outer((x0 - a) , u))\n",
    "    temp = cf(u) * H_k \n",
    "    temp[0] = 0.5 * temp[0]    \n",
    "    value = np.exp(-r * tau) * K * np.real(mat.dot(temp))     \n",
    "    return value\n",
    "\n",
    "# Chi and Psi Evaluation Method\n",
    "def Chi_Psi(a,b,c,d,k):\n",
    "    psi = np.sin(k * np.pi * (d - a) / (b - a)) - np.sin(k * np.pi * (c - a)/(b - a))\n",
    "    psi[1:] = psi[1:] * (b - a) / (k[1:] * np.pi)\n",
    "    psi[0] = d - c\n",
    "    \n",
    "    chi = 1.0 / (1.0 + np.power((k * np.pi / (b - a)) , 2.0)) \n",
    "    expr1 = np.cos(k * np.pi * (d - a)/(b - a)) * np.exp(d)  - np.cos(k * np.pi \n",
    "                  * (c - a) / (b - a)) * np.exp(c)\n",
    "    expr2 = k * np.pi / (b - a) * np.sin(k * np.pi * \n",
    "                        (d - a) / (b - a))   - k * np.pi / (b - a) * np.sin(k \n",
    "                        * np.pi * (c - a) / (b - a)) * np.exp(c)\n",
    "    chi = chi * (expr1 + expr2)\n",
    "    \n",
    "    value = {\"chi\":chi,\"psi\":psi }\n",
    "    return value\n",
    "\n",
    "# Determine coefficients for Call or Put Prices \n",
    "def CallPutCoefficients(CP,a,b,k):\n",
    "    if CP==OptionType.CALL:                  \n",
    "        c = 0.0\n",
    "        d = b\n",
    "        coef = Chi_Psi(a,b,c,d,k)\n",
    "        Chi_k = coef[\"chi\"]\n",
    "        Psi_k = coef[\"psi\"]\n",
    "        if a < b and b < 0.0:\n",
    "            H_k = np.zeros([len(k),1])\n",
    "        else:\n",
    "            H_k      = 2.0 / (b - a) * (Chi_k - Psi_k)  \n",
    "    elif CP==OptionType.PUT:\n",
    "        c = a\n",
    "        d = 0.0\n",
    "        coef = Chi_Psi(a,b,c,d,k)\n",
    "        Chi_k = coef[\"chi\"]\n",
    "        Psi_k = coef[\"psi\"]\n",
    "        H_k      = 2.0 / (b - a) * (- Chi_k + Psi_k)                  \n",
    "    return H_k    \n",
    "\n",
    "# Evaluate Heston model using closed-form characteristic function approach\n",
    "def ChFHestonModel(r,tau,kappa,gamma,vbar,v0,rho):\n",
    "    i = np.complex(0.0,1.0)\n",
    "    D1 = lambda u: np.sqrt(np.power(kappa-gamma*rho*i*u,2)+(u*u+i*u)*gamma*gamma)\n",
    "    g  = lambda u: (kappa-gamma*rho*i*u-D1(u))/(kappa-gamma*rho*i*u+D1(u))\n",
    "    C  = lambda u: (1.0-np.exp(-D1(u)*tau))/(gamma*gamma*(1.0-g(u)*np.exp(-D1(u)*tau)))\\\n",
    "        *(kappa-gamma*rho*i*u-D1(u))\n",
    "        \n",
    "    # Note that we exclude the term -r*tau, as the discounting is performed in the COS method\n",
    "    A  = lambda u: r * i*u *tau + kappa*vbar*tau/gamma/gamma *(kappa-gamma*rho*i*u-D1(u))\\\n",
    "        - 2*kappa*vbar/gamma/gamma*np.log((1.0-g(u)*np.exp(-D1(u)*tau))/(1.0-g(u)))\n",
    "    \n",
    "    # Characteristic function for the Heston's model    \n",
    "    cf = lambda u: np.exp(A(u) + C(u)*v0)\n",
    "    return cf \n",
    "\n",
    "# European Option Price given Monte Carlo Paths\n",
    "# Here we specify our option payoff\n",
    "def EuropeanOptionPriceFromMCPaths(CP,S,K,T,r):\n",
    "    # S is a vector of Monte Carlo samples at T\n",
    "    result = np.zeros([len(K),1])\n",
    "    if CP == OptionType.CALL:\n",
    "        for (idx,k) in enumerate(K):\n",
    "            result[idx] = np.exp(-r*T)*np.mean(np.maximum(S-k,0.0))\n",
    "    elif CP == OptionType.PUT:\n",
    "        for (idx,k) in enumerate(K):\n",
    "            result[idx] = np.exp(-r*T)*np.mean(np.maximum(k-S,0.0))\n",
    "    return result\n",
    "\n",
    "# Generate Heston Paths using Euler Discretization\n",
    "def GeneratePathsHestonEuler(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0):\n",
    "    \n",
    "    # Vector Initialization\n",
    "    Z1 = np.random.normal(0.0,1.0,[NoOfPaths,NoOfSteps])\n",
    "    Z2 = np.random.normal(0.0,1.0,[NoOfPaths,NoOfSteps])\n",
    "    W1 = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    W2 = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    V = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    X = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    V[:,0]=v0\n",
    "    X[:,0]=np.log(S_0)\n",
    "    \n",
    "    time = np.zeros([NoOfSteps+1])\n",
    "    dt = T / float(NoOfSteps)\n",
    "\n",
    "    for i in range(0,NoOfSteps):\n",
    "        # Apply Central Limit Theorem: Standard Normal dist'n has better convergence\n",
    "        # Make sure that samples from normal have mean 0 and variance 1\n",
    "        if NoOfPaths > 1:\n",
    "            Z1[:,i] = (Z1[:,i] - np.mean(Z1[:,i])) / np.std(Z1[:,i])\n",
    "            Z2[:,i] = (Z2[:,i] - np.mean(Z2[:,i])) / np.std(Z2[:,i])\n",
    "        Z2[:,i] = rho * Z1[:,i] + np.sqrt(1.0-rho**2)*Z2[:,i]\n",
    "        \n",
    "        # Compute Correlated Brownian Motions using Cholesky Decomposition\n",
    "        W1[:,i+1] = W1[:,i] + np.power(dt, 0.5)*Z1[:,i]\n",
    "        W2[:,i+1] = W2[:,i] + np.power(dt, 0.5)*Z2[:,i]\n",
    "        \n",
    "        # Variance cannot be negative so we use apply the truncation boundary condition\n",
    "        # Truncated boundary condition: v((i+1) = max(v(i+1),0)\n",
    "        V[:,i+1] = V[:,i] + kappa*(vbar - V[:,i]) * dt + gamma* np.sqrt(V[:,i]) * (W1[:,i+1]-W1[:,i])\n",
    "        V[:,i+1] = np.maximum(V[:,i+1],0.0)\n",
    "        \n",
    "        # Euler Discretization of Log-Normal Asset Process\n",
    "        X[:,i+1] = X[:,i] + (r - 0.5*V[:,i])*dt + np.sqrt(V[:,i])*(W2[:,i+1]-W2[:,i])\n",
    "        time[i+1] = time[i] +dt\n",
    "        \n",
    "    #Compute exponent\n",
    "    S = np.exp(X)\n",
    "    paths = {\"time\":time,\"S\":S}\n",
    "    return paths\n",
    "\n",
    "def CIR_Sample(NoOfPaths,kappa,gamma,vbar,s,t,v_s):\n",
    "    delta = 4.0 *kappa*vbar/gamma/gamma\n",
    "    c= 1.0/(4.0*kappa)*gamma*gamma*(1.0-np.exp(-kappa*(t-s)))\n",
    "    kappaBar = 4.0*kappa*v_s*np.exp(-kappa*(t-s))/(gamma*gamma*(1.0-np.exp(-kappa*(t-s))))\n",
    "    sample = c* np.random.noncentral_chisquare(delta,kappaBar,NoOfPaths)\n",
    "    return  sample\n",
    "\n",
    "# Generate Heston Monte Carlo Paths using Almost Exact Simulation\n",
    "def GeneratePathsHestonAES(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0):    \n",
    "    \n",
    "    # Vector Initialization\n",
    "    Z1 = np.random.normal(0.0,1.0,[NoOfPaths,NoOfSteps])\n",
    "    W1 = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    V = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    X = np.zeros([NoOfPaths, NoOfSteps+1])\n",
    "    V[:,0]=v0\n",
    "    X[:,0]=np.log(S_0)\n",
    "    \n",
    "    time = np.zeros([NoOfSteps+1])\n",
    "        \n",
    "    dt = T / float(NoOfSteps)\n",
    "    for i in range(0,NoOfSteps):\n",
    "        \n",
    "        # Apply Central Limit Theorem - Standard Normal dist'n has better convergence\n",
    "        # Make sure that samples from normal have mean 0 and variance 1\n",
    "        if NoOfPaths > 1:\n",
    "            Z1[:,i] = (Z1[:,i] - np.mean(Z1[:,i])) / np.std(Z1[:,i])\n",
    "        \n",
    "        # Evaluate the Browniam Motion: W = Z.sqrt(dt)\n",
    "        W1[:,i+1] = W1[:,i] + np.power(dt, 0.5)*Z1[:,i]\n",
    "        \n",
    "        # Exact samples for the variance process\n",
    "        V[:,i+1] = CIR_Sample(NoOfPaths,kappa,gamma,vbar,0,dt,V[:,i])\n",
    "        \n",
    "        # AES Constant Terms\n",
    "        k0 = (r -rho/gamma*kappa*vbar)*dt\n",
    "        k1 = (rho*kappa/gamma -0.5)*dt - rho/gamma\n",
    "        k2 = rho / gamma\n",
    "        \n",
    "        # Almost Exact Simulation for Log-Normal Asset Process\n",
    "        X[:,i+1] = X[:,i] + k0 + k1*V[:,i] + k2 *V[:,i+1] + np.sqrt((1.0-rho**2)*V[:,i])*(W1[:,i+1]-W1[:,i])\n",
    "        \n",
    "        time[i+1] = time[i] +dt\n",
    "        \n",
    "    #Compute exponent\n",
    "    S = np.exp(X)\n",
    "    paths = {\"time\":time,\"S\":S}\n",
    "    return paths\n",
    "\n",
    "# Black-Scholes Call option price\n",
    "def BS_Call_Put_Option_Price(CP,S_0,K,sigma,t,T,r):\n",
    "    #print('Maturity T={0} and t={1}'.format(T,t))\n",
    "    #print(float(sigma * np.sqrt(T-t)))\n",
    "    #print('strike K ={0}'.format(K))\n",
    "    K = np.array(K).reshape([len(K),1])\n",
    "    d1    = (np.log(S_0 / K) + (r + 0.5 * np.power(sigma,2.0)) \n",
    "    * (T-t)) / (sigma * np.sqrt(T-t))\n",
    "    d2    = d1 - sigma * np.sqrt(T-t)\n",
    "    \n",
    "    if CP == OptionType.CALL:\n",
    "        value = st.norm.cdf(d1) * S_0 - st.norm.cdf(d2) * K * np.exp(-r * (T-t))\n",
    "     #   print(value)\n",
    "    elif CP == OptionType.PUT:\n",
    "        value = st.norm.cdf(-d2) * K * np.exp(-r * (T-t)) - st.norm.cdf(-d1)*S_0\n",
    "      #  print(value)\n",
    "    return value\n",
    "\n",
    "# Run Simulation Processes\n",
    "def RunSimulation(CP,ResultsChart):\n",
    "    # CP = CALL or PUT\n",
    "    # ChartNumber = The results chart number (use the same number to overlay charts)\n",
    "    \n",
    "    # Simulation Paths and Euler Timesteps\n",
    "    NoOfPaths = 2500\n",
    "    NoOfSteps = 1000\n",
    "    \n",
    "    # Heston model parameters from calibration process\n",
    "    gamma = 1.0     # vol of vol\n",
    "    kappa = 0.5     # speed of mean reversion\n",
    "    vbar  = 0.04    # long-term variance\n",
    "    rho   = -0.9    # correlation (negative as variance moves in opposiste direction to asset)\n",
    "    T     = 1.0     # maturity\n",
    "    r     = 0.1     # interest rate\n",
    "    S_0   = 100.0   # initial asset value\n",
    "    v0    = 0.04    # initial variance\n",
    "    \n",
    "    CP_String = \"*** CALL OPTIONS ***\"\n",
    "    if CP==OptionType.PUT:\n",
    "        CP_String = \"*** PUT OPTIONS ***\"\n",
    "    \n",
    "    # First we define a range of strikes and check the convergence\n",
    "    # min strike, max strike and number of abscissae\n",
    "    K = np.linspace(S_0*0.8, S_0*1.4, 25)\n",
    "    \n",
    "    # Exact solution with the COSine method\n",
    "    cf = ChFHestonModel(r,T,kappa,gamma,vbar,v0,rho)\n",
    "    \n",
    "    # The COSine method with 1,000 expansion terms to approx characteristic function\n",
    "    optValueExact = CallPutOptionPriceCOSMthd(cf, CP, S_0, r, T, K, 1000, 8)\n",
    "    \n",
    "    # Euler simulation\n",
    "    pathsEULER = GeneratePathsHestonEuler(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0)\n",
    "    S_Euler = pathsEULER[\"S\"]\n",
    "    \n",
    "    # Almost exact simulation\n",
    "    pathsAES = GeneratePathsHestonAES(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0)\n",
    "    S_AES = pathsAES[\"S\"]\n",
    "    \n",
    "    # Here we evaluate the option payoff - can be modified for exotics et al.\n",
    "    OptPrice_EULER = EuropeanOptionPriceFromMCPaths(CP,S_Euler[:,-1],K,T,r)\n",
    "    OptPrice_AES   = EuropeanOptionPriceFromMCPaths(CP,S_AES[:,-1],K,T,r)\n",
    "    \n",
    "    # Plot Simulated Option Prices for Exact, Euler and AES Schemes\n",
    "    plt.figure(ResultsChart)\n",
    "    plt.plot(K,optValueExact,'-r')\n",
    "    plt.plot(K,OptPrice_EULER,'--k')\n",
    "    plt.plot(K,OptPrice_AES,'.b')\n",
    "    plt.legend(['Exact (COS)','Euler','AES'])\n",
    "    plt.grid()\n",
    "    plt.xlabel('strike, K')\n",
    "    plt.ylabel('option price')\n",
    "    plt.title('Simulated Option Prices')\n",
    "    \n",
    "    # Here we will analyze the convergence a variety of variance time steps dtV\n",
    "    dtV = np.array([1.0, 1.0/4.0, 1.0/8.0,1.0/16.0,1.0/32.0,1.0/64.0])\n",
    "    NoOfStepsV = [int(T/x) for x in dtV]\n",
    "    \n",
    "    # Specify strike for analysis\n",
    "    K = np.array([140])\n",
    "    \n",
    "    # 1. Pricing using Exact Simulation\n",
    "    ###################################\n",
    "    optValueExact = CallPutOptionPriceCOSMthd(cf, CP, S_0, r, T, K, 1000, 8)\n",
    "    \n",
    "    # Initialize Standard Error Vectors\n",
    "    errorEuler = np.zeros([len(dtV),1])\n",
    "    errorAES = np.zeros([len(dtV),1])\n",
    "    \n",
    "    for (idx,NoOfSteps) in enumerate(NoOfStepsV):\n",
    "        \n",
    "        # 2. Euler Discretization\n",
    "        #########################\n",
    "        np.random.seed(3)\n",
    "        pathsEULER = GeneratePathsHestonEuler(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0)\n",
    "        S_Euler = pathsEULER[\"S\"]\n",
    "        \n",
    "        # Evaluate the option payoff - can be modified to exotics et al.\n",
    "        OptPriceEULER = EuropeanOptionPriceFromMCPaths(CP,S_Euler[:,-1],K,T,r)\n",
    "        errorEuler[idx] = OptPriceEULER-optValueExact\n",
    "        \n",
    "        # 3. Almost Exact Simulation (AES)\n",
    "        ##################################\n",
    "        np.random.seed(3)\n",
    "        pathsAES = GeneratePathsHestonAES(NoOfPaths,NoOfSteps,T,r,S_0,kappa,gamma,rho,vbar,v0)\n",
    "        S_AES = pathsAES[\"S\"]\n",
    "        \n",
    "        # Evaluate the option payoff - can be modified to exotics et al.\n",
    "        OptPriceAES   = EuropeanOptionPriceFromMCPaths(CP,S_AES[:,-1],K,T,r)\n",
    "        errorAES[idx] = OptPriceAES-optValueExact\n",
    "\n",
    "    print()\n",
    "    print(CP_String)\n",
    "\n",
    "    print()\n",
    "    print(\"Euler Scheme\")\n",
    "    print(\"------------\")\n",
    "    print(\"Strike (K), Timestep (dt), Standard Error (eps)\")\n",
    "    for i in range(0,len(NoOfStepsV)):\n",
    "        print(\"Euler Scheme, K = {0}, dt = {1}, eps = {2}\".format(K,dtV[i],errorEuler[i]))\n",
    "    \n",
    "    print()\n",
    "    print(\"Almost Exact Simulation (AES)\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Strike (K), Timestep (dt), Standard Error (eps)\")\n",
    "    for i in range(0,len(NoOfStepsV)):\n",
    "        print(\"AES Scheme, K = {0}, dt = {1}, eps = {2}\".format(K,dtV[i],errorAES[i]))\n",
    "        \n",
    "# Run Simulation for Calls then Puts        \n",
    "RunSimulation(OptionType.CALL,1)\n",
    "RunSimulation(OptionType.PUT,2)\n",
    "\n",
    "def calculate_spread_zscore(pairs, symbols, lookback=100):\n",
    "    \"\"\"\n",
    "    Creates a hedge ratio between the two symbols by calculating\n",
    "    a rolling linear regression with a defined lookback period. This\n",
    "    is then used to create a z-score of the 'spread' between the two\n",
    "    symbols based on a linear combination of the two.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : `pd.DataFrame`\n",
    "        A DataFrame containing Close price for SPY and IWM. Index is a \n",
    "        Datetime object.\n",
    "    symbols : `tup`\n",
    "        Tuple containing ticker symbols as `str`.\n",
    "    lookback : `int`, optional (default: 100)\n",
    "        Lookback preiod for rolling linear regression.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pairs : 'pd.DataFrame'\n",
    "        Updated DataFrame containing the spread and z score between\n",
    "        the two symbols based on the rolling linear regression.    \n",
    "    \"\"\"\n",
    "\n",
    "    # Use the statsmodels Rolling Ordinary Least Squares method to fit\n",
    "    # a rolling linear regression between the two closing price time series\n",
    "    print(\"Fitting the rolling Linear Regression...\")\n",
    "\n",
    "    model = RollingOLS(\n",
    "        endog=pairs['%s_close' % symbols[0].lower()],\n",
    "        exog=sm.add_constant(pairs['%s_close' % symbols[1].lower()]),\n",
    "        window=lookback\n",
    "    )\n",
    "    rres = model.fit()\n",
    "    params = rres.params.copy()\n",
    "    \n",
    "    \n",
    "    # Construct the hedge ratio and eliminate the first \n",
    "    # lookback-length empty/NaN period\n",
    "    pairs['hedge_ratio'] = params['iwm_close']\n",
    "    pairs.dropna(inplace=True)\n",
    "\n",
    "    # Create the spread and then a z-score of the spread\n",
    "    print(\"Creating the spread/zscore columns...\")\n",
    "    pairs['spread'] = (\n",
    "        pairs['spy_close'] - pairs['hedge_ratio']*pairs['iwm_close']\n",
    "    )\n",
    "    pairs['zscore'] = (\n",
    "        pairs['spread'] - np.mean(pairs['spread']))/np.std(pairs['spread']\n",
    "    )\n",
    "    return pairs\n",
    "\n",
    "def create_long_short_market_signals(\n",
    "        pairs, symbols, z_entry_threshold=2.0, z_exit_threshold=1.0\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create the entry/exit signals based on the exceeding of z_entry_threshold\n",
    "    for entering a position and falling below z_exit_threshold for exiting\n",
    "    a position.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : `pd.DataFrame`\n",
    "        Updated DataFrame containing the close price, spread and z score\n",
    "        between the two symbols.\n",
    "    symbols : `tup`\n",
    "        Tuple containing ticker symbols as `str`.\n",
    "    z_entry_threshold : `float`, optional (default:2.0)\n",
    "        Z Score threshold for market entry. \n",
    "    z_exit_threshold : `float`, optional (default:1.0)\n",
    "        Z Score threshold for market exit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pairs : `pd.DataFrame`\n",
    "        Updated DataFrame containing long, short and exit signals.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate when to be long, short and when to exit\n",
    "    pairs['longs'] = (pairs['zscore'] <= -z_entry_threshold)*1.0\n",
    "    pairs['shorts'] = (pairs['zscore'] >= z_entry_threshold)*1.0\n",
    "    pairs['exits'] = (np.abs(pairs['zscore']) <= z_exit_threshold)*1.0\n",
    "\n",
    "    # These signals are needed because we need to propagate a\n",
    "    # position forward, i.e. we need to stay long if the zscore\n",
    "    # threshold is less than z_entry_threshold by still greater\n",
    "    # than z_exit_threshold, and vice versa for shorts.\n",
    "    pairs['long_market'] = 0.0\n",
    "    pairs['short_market'] = 0.0\n",
    "\n",
    "    # These variables track whether to be long or short while\n",
    "    # iterating through the bars\n",
    "    long_market = 0\n",
    "    short_market = 0\n",
    "\n",
    "    # Calculates when to actually be \"in\" the market, i.e. to have a\n",
    "    # long or short position, as well as when not to be.\n",
    "    # Since this is using iterrows to loop over a dataframe, it will\n",
    "    # be significantly less efficient than a vectorised operation,\n",
    "    # i.e. slow!\n",
    "    print(\"Calculating when to be in the market (long and short)...\")\n",
    "    for i, b in enumerate(pairs.iterrows()):\n",
    "        # Calculate longs\n",
    "        if b[1]['longs'] == 1.0:\n",
    "            long_market = 1            \n",
    "        # Calculate shorts\n",
    "        if b[1]['shorts'] == 1.0:\n",
    "            short_market = 1\n",
    "        # Calculate exists\n",
    "        if b[1]['exits'] == 1.0:\n",
    "            long_market = 0\n",
    "            short_market = 0\n",
    "        # This directly assigns a 1 or 0 to the long_market/short_market\n",
    "        # columns, such that the strategy knows when to actually stay in!\n",
    "        pairs.iloc[i]['long_market'] = long_market\n",
    "        pairs.iloc[i]['short_market'] = short_market\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bfc94c-72a3-411a-813f-23bbbb118835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "#assuming you already know how monte carlo works\n",
    "#if not, plz click the link below\n",
    "# https://datascienceplus.com/how-to-apply-monte-carlo-simulation-to-forecast-stock-prices-using-python/\n",
    "\n",
    "#monte carlo simulation is a buzz word for people outside of financial industry\n",
    "#in the industry, everybody jokes about it but no one actually uses it\n",
    "#including my risk quant friends, they be like why the heck use that\n",
    "#you may argue its application in option pricing to monitor fat tail events\n",
    "#seriously, did anyone predict 2008 financial crisis?\n",
    "#or did anyone foresee the vix surging in early 2018?\n",
    "\n",
    "#the weakness of monte carlo, perhaps in every forecast methodology\n",
    "#is that our pseudo random number is generated via empirical distribution\n",
    "#in another word, we use the past to predict the future\n",
    "#if something has never happened in the past\n",
    "#how can you predict it with our limited imagination\n",
    "#its like muggles trying to understand the wizard world\n",
    "#laplace smoothing is actually better than monte carlo in this case\n",
    "\n",
    "#the idea presented here is very straight forward\n",
    "#we construct a model to get mean and variance of its residual (return)\n",
    "#we generate the next possible price by geometric brownian motion\n",
    "#we run this simulations as many times as possible\n",
    "#naturally we should acquire a large amount of data in the end\n",
    "#we pick the forecast that has the least std against the original data series\n",
    "#we would check if the best forecast can predict the future direction (instead of actual price)\n",
    "#and how well monte carlo catches black swans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fix_yahoo_finance as yf\n",
    "import random as rd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#this list is purely designed to generate gradient color \n",
    "global colorlist\n",
    "colorlist=['#fffb77',\n",
    " '#fffa77',\n",
    " '#fff977',\n",
    " '#fff876',\n",
    " '#fff776',\n",
    " '#fff676',\n",
    " '#fff576',\n",
    " '#fff475',\n",
    " '#fff375',\n",
    " '#fff275',\n",
    " '#fff175',\n",
    " '#fff075',\n",
    " '#ffef74',\n",
    " '#ffef74',\n",
    " '#ffee74',\n",
    " '#ffed74',\n",
    " '#ffec74',\n",
    " '#ffeb73',\n",
    " '#ffea73',\n",
    " '#ffe973',\n",
    " '#ffe873',\n",
    " '#ffe772',\n",
    " '#ffe672',\n",
    " '#ffe572',\n",
    " '#ffe472',\n",
    " '#ffe372',\n",
    " '#ffe271',\n",
    " '#ffe171',\n",
    " '#ffe071',\n",
    " '#ffdf71',\n",
    " '#ffde70',\n",
    " '#ffdd70',\n",
    " '#ffdc70',\n",
    " '#ffdb70',\n",
    " '#ffda70',\n",
    " '#ffd96f',\n",
    " '#ffd86f',\n",
    " '#ffd76f',\n",
    " '#ffd66f',\n",
    " '#ffd66f',\n",
    " '#ffd56e',\n",
    " '#ffd46e',\n",
    " '#ffd36e',\n",
    " '#ffd26e',\n",
    " '#ffd16d',\n",
    " '#ffd06d',\n",
    " '#ffcf6d',\n",
    " '#ffce6d',\n",
    " '#ffcd6d',\n",
    " '#ffcc6c',\n",
    " '#ffcb6c',\n",
    " '#ffca6c',\n",
    " '#ffc96c',\n",
    " '#ffc86b',\n",
    " '#ffc76b',\n",
    " '#ffc66b',\n",
    " '#ffc56b',\n",
    " '#ffc46b',\n",
    " '#ffc36a',\n",
    " '#ffc26a',\n",
    " '#ffc16a',\n",
    " '#ffc06a',\n",
    " '#ffbf69',\n",
    " '#ffbe69',\n",
    " '#ffbd69',\n",
    " '#ffbd69',\n",
    " '#ffbc69',\n",
    " '#ffbb68',\n",
    " '#ffba68',\n",
    " '#ffb968',\n",
    " '#ffb868',\n",
    " '#ffb768',\n",
    " '#ffb667',\n",
    " '#ffb567',\n",
    " '#ffb467',\n",
    " '#ffb367',\n",
    " '#ffb266',\n",
    " '#ffb166',\n",
    " '#ffb066',\n",
    " '#ffaf66',\n",
    " '#ffad65',\n",
    " '#ffac65',\n",
    " '#ffab65',\n",
    " '#ffa964',\n",
    " '#ffa864',\n",
    " '#ffa763',\n",
    " '#ffa663',\n",
    " '#ffa463',\n",
    " '#ffa362',\n",
    " '#ffa262',\n",
    " '#ffa062',\n",
    " '#ff9f61',\n",
    " '#ff9e61',\n",
    " '#ff9c61',\n",
    " '#ff9b60',\n",
    " '#ff9a60',\n",
    " '#ff9860',\n",
    " '#ff975f',\n",
    " '#ff965f',\n",
    " '#ff955e',\n",
    " '#ff935e',\n",
    " '#ff925e',\n",
    " '#ff915d',\n",
    " '#ff8f5d',\n",
    " '#ff8e5d',\n",
    " '#ff8d5c',\n",
    " '#ff8b5c',\n",
    " '#ff8a5c',\n",
    " '#ff895b',\n",
    " '#ff875b',\n",
    " '#ff865b',\n",
    " '#ff855a',\n",
    " '#ff845a',\n",
    " '#ff8259',\n",
    " '#ff8159',\n",
    " '#ff8059',\n",
    " '#ff7e58',\n",
    " '#ff7d58',\n",
    " '#ff7c58',\n",
    " '#ff7a57',\n",
    " '#ff7957',\n",
    " '#ff7857',\n",
    " '#ff7656',\n",
    " '#ff7556',\n",
    " '#ff7455',\n",
    " '#ff7355',\n",
    " '#ff7155',\n",
    " '#ff7054',\n",
    " '#ff6f54',\n",
    " '#ff6d54',\n",
    " '#ff6c53',\n",
    " '#ff6b53',\n",
    " '#ff6953',\n",
    " '#ff6852',\n",
    " '#ff6752',\n",
    " '#ff6552',\n",
    " '#ff6451',\n",
    " '#ff6351',\n",
    " '#ff6250',\n",
    " '#ff6050',\n",
    " '#ff5f50',\n",
    " '#ff5e4f',\n",
    " '#ff5c4f',\n",
    " '#ff5b4f',\n",
    " '#ff5a4e',\n",
    " '#ff584e',\n",
    " '#ff574e',\n",
    " '#ff564d',\n",
    " '#ff544d',\n",
    " '#ff534d',\n",
    " '#ff524c',\n",
    " '#ff514c',\n",
    " '#ff4f4b',\n",
    " '#ff4e4b',\n",
    " '#ff4d4b',\n",
    " '#ff4b4a',\n",
    " '#ff4a4a']\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#this is where the actual simulation happens\n",
    "#testsize denotes how much percentage of dataset would be used for testing\n",
    "#simulation denotes the number of simulations\n",
    "#theoretically speaking, the larger the better\n",
    "#given the constrained computing power\n",
    "#we have to take a balance point between efficiency and effectiveness\n",
    "def monte_carlo(data,testsize=0.5,simulation=100,**kwargs):    \n",
    "    \n",
    "    #train test split as usual\n",
    "    df,test=train_test_split(data,test_size=testsize,shuffle=False,**kwargs)\n",
    "    forecast_horizon=len(test)\n",
    "    \n",
    "    #we only care about close price\n",
    "    #if there has been dividend issued\n",
    "    #we use adjusted close price instead\n",
    "    df=df.loc[:,['Close']]\n",
    "        \n",
    "    #here we use log return\n",
    "    returnn=np.log(df['Close'].iloc[1:]/df['Close'].shift(1).iloc[1:])\n",
    "    drift=returnn.mean()-returnn.var()/2\n",
    "    \n",
    "    #we use dictionary to store predicted time series\n",
    "    d={}\n",
    "    \n",
    "    #we use geometric brownian motion to compute the next price\n",
    "    # https://en.wikipedia.org/wiki/Geometric_Brownian_motion\n",
    "    for counter in range(simulation):\n",
    "        d[counter]=[df['Close'].iloc[0]]\n",
    "      \n",
    "        #we dont just forecast the future\n",
    "        #we need to compare the forecast with the historical data as well\n",
    "        #thats why the data range is training horizon plus testing horizon\n",
    "        for i in range(len(df)+forecast_horizon-1):\n",
    "         \n",
    "            #we use standard normal distribution to generate pseudo random number\n",
    "            #which is sufficient for our monte carlo simulation\n",
    "            sde=drift+returnn.std()*rd.gauss(0,1)\n",
    "            temp=d[counter][-1]*np.exp(sde)\n",
    "        \n",
    "            d[counter].append(temp.item())\n",
    "    \n",
    "    #to determine which simulation is the best fit\n",
    "    #we use simple criterias, the smallest standard deviation\n",
    "    #we iterate through every simulation and compare it with actual data\n",
    "    #the one with the least standard deviation wins\n",
    "    std=float('inf')\n",
    "    pick=0\n",
    "    for counter in range(simulation):\n",
    "    \n",
    "        temp=np.std(np.subtract(\n",
    "                    d[counter][:len(df)],df['Close']))\n",
    "        if temp<std:\n",
    "            std=temp\n",
    "            pick=counter\n",
    "    \n",
    "    return forecast_horizon,d,pick\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#result plotting\n",
    "def plot(df,forecast_horizon,d,pick,ticker):\n",
    "    \n",
    "    #the first plot is to plot every simulation\n",
    "    #and highlight the best fit with the actual dataset\n",
    "    #we only look at training horizon in the first figure\n",
    "    ax=plt.figure(figsize=(10,5)).add_subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    for i in range(int(len(d))):\n",
    "        if i!=pick:\n",
    "            ax.plot(df.index[:len(df)-forecast_horizon], \\\n",
    "                    d[i][:len(df)-forecast_horizon], \\\n",
    "                    alpha=0.05)\n",
    "    ax.plot(df.index[:len(df)-forecast_horizon], \\\n",
    "            d[pick][:len(df)-forecast_horizon], \\\n",
    "            c='#5398d9',linewidth=5,label='Best Fitted')\n",
    "    df['Close'].iloc[:len(df)-forecast_horizon].plot(c='#d75b66',linewidth=5,label='Actual')\n",
    "    plt.title(f'Monte Carlo Simulation\\nTicker: {ticker}')\n",
    "    plt.legend(loc=0)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Date')\n",
    "    plt.show()\n",
    "    \n",
    "    #the second figure plots both training and testing horizons\n",
    "    #we compare the best fitted plus forecast with the actual history\n",
    "    #the figure reveals why monte carlo simulation in trading is house of cards\n",
    "    #it is merely illusion that monte carlo simulation can forecast any asset price or direction\n",
    "    ax=plt.figure(figsize=(10,5)).add_subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.plot(d[pick],label='Best Fitted',c='#edd170')\n",
    "    plt.plot(df['Close'].tolist(),label='Actual',c='#02231c')\n",
    "    plt.axvline(len(df)-forecast_horizon,linestyle=':',c='k')\n",
    "    plt.text(len(df)-forecast_horizon-50, \\\n",
    "             max(max(df['Close']),max(d[pick])),'Training', \\\n",
    "             horizontalalignment='center', \\\n",
    "             verticalalignment='center')\n",
    "    plt.text(len(df)-forecast_horizon+50, \\\n",
    "             max(max(df['Close']),max(d[pick])),'Testing', \\\n",
    "             horizontalalignment='center', \\\n",
    "             verticalalignment='center')\n",
    "    plt.title(f'Training versus Testing\\nTicker: {ticker}\\n')\n",
    "    plt.legend(loc=0)\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('T+Days')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "#we also gotta test if the surge in simulations increases the prediction accuracy\n",
    "#simu_start denotes the minimum simulation number\n",
    "#simu_end denotes the maximum simulation number\n",
    "#sim_delta denotes how many steps it takes to reach the max from the min\n",
    "#its kinda like range(simu_start,simu_end,simu_delta)\n",
    "def test(df,ticker,simu_start=100,simu_end=1000,simu_delta=100,**kwargs):\n",
    "    \n",
    "    table=pd.DataFrame()\n",
    "    table['Simulations']=np.arange(simu_start,simu_end+simu_delta,simu_delta)\n",
    "    table.set_index('Simulations',inplace=True)\n",
    "    table['Prediction']=0\n",
    "\n",
    "    #for each simulation\n",
    "    #we test if the prediction is accurate\n",
    "    #for instance\n",
    "    #if the end of testing horizon is larger than the end of training horizon\n",
    "    #we denote the return direction as +1\n",
    "    #if both actual and predicted return direction align\n",
    "    #we conclude the prediction is accurate\n",
    "    #vice versa\n",
    "    for i in np.arange(simu_start,simu_end+1,simu_delta):\n",
    "        print(i)\n",
    "        \n",
    "        forecast_horizon,d,pick=monte_carlo(df,simulation=i,**kwargs)\n",
    "        \n",
    "        actual_return=np.sign( \\\n",
    "                              df['Close'].iloc[len(df)-forecast_horizon]-df['Close'].iloc[-1])\n",
    "        \n",
    "        best_fitted_return=np.sign(d[pick][len(df)-forecast_horizon]-d[pick][-1])\n",
    "        table.at[i,'Prediction']=np.where(actual_return==best_fitted_return,1,-1)\n",
    "        \n",
    "    #we plot the horizontal bar chart \n",
    "    #to show the accuracy does not increase over the number of simulations\n",
    "    ax=plt.figure(figsize=(10,5)).add_subplot(111)\n",
    "    ax.spines['right'].set_position('center')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    plt.barh(np.arange(1,len(table)*2+1,2),table['Prediction'], \\\n",
    "             color=colorlist[0::int(len(colorlist)/len(table))])\n",
    "\n",
    "    plt.xticks([-1,1],['Failure','Success'])\n",
    "    plt.yticks(np.arange(1,len(table)*2+1,2),table.index)\n",
    "    plt.xlabel('Prediction Accuracy')\n",
    "    plt.ylabel('Times of Simulation')\n",
    "    plt.title(f\"Prediction accuracy doesn't depend on the numbers of simulation.\\nTicker: {ticker}\\n\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#lets try something extreme, pick ge, the worst performing stock in 2018\n",
    "#see how monte carlo works for both direction prediction and fat tail simulation\n",
    "#why the extreme? well if we are risk quants, we care about value at risk, dont we\n",
    "#if quants only look at one sigma event, the portfolio performance would be devastating\n",
    "def main():\n",
    "    \n",
    "    stdate='2016-01-15'\n",
    "    eddate='2019-01-15'\n",
    "    ticker='GE'\n",
    "\n",
    "    df=yf.download(ticker,start=stdate,end=eddate)\n",
    "    df.index=pd.to_datetime(df.index)\n",
    "    \n",
    "    forecast_horizon,d,pick=monte_carlo(df)\n",
    "    plot(df,forecast_horizon,d,pick,ticker)\n",
    "    test(df,ticker)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542654dc-b05d-47d3-820a-4ffdfe4929bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import TMRW\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import linregress\n",
    "from scipy import integrate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sdata(x, y, z):\n",
    "    # x is a stock symbol\n",
    "    # y is a start date\n",
    "    # z is an end date\n",
    "    # kan bruges på commodities, currencies og aktier\n",
    "    \n",
    "    #MSFT er en aktier der kan bruges\n",
    "    #USDEUR=X er en currency der kan bruges\n",
    "    #GC=F er guld priser\n",
    "    \n",
    "    st = pd.DataFrame()\n",
    "    t = yf.Ticker(x)\n",
    "    st = t.history(start=y, end=z)\n",
    "    st.index = pd.to_datetime(st.index).tz_localize(None)\n",
    "    return(st)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                        Slopes, returns and accelerations\n",
    "#                        For calculating import \n",
    "#                        intra and inter day differences\n",
    "###############################################################################\n",
    "\n",
    "def slope(array):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    y = np.array(array)\n",
    "    x = np.arange(len(y))\n",
    "    slopes, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "    return(slopes)\n",
    "\n",
    "def returns(x, logs = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(x) == list: # if you've entered a list\n",
    "        _return = np.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            _return[i] = ((x[i]/x[i-1])-1)\n",
    "    elif type(x) == pd.DataFrame: # if you've entered a dataframe\n",
    "        _return = x.pct_change().dropna()   \n",
    "    else: # if you've entered neither a list or a dataframe\n",
    "        _return = x.pct_change().dropna()\n",
    "    \n",
    "    # if you want log returns\n",
    "    if logs == True:\n",
    "        _return = np.log(abs(_return))\n",
    "    \n",
    "    df = pd.DataFrame(data=_return, dtype=np.float64)\n",
    "    df.columns = [\"returns\"]\n",
    "    return(df)\n",
    "\n",
    "def acceleration(x, logs = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    x = returns(x, logs)\n",
    "    if type(x) == list:  # if you've entered a list\n",
    "        acc = np.zeros(len(x))\n",
    "        for i in range(len(x)):\n",
    "            acc[i] = ((x[i]/x[i-1])-1)\n",
    "    elif type(x) == pd.DataFrame: # if you've entered a dataframe\n",
    "        acc = x.pct_change().dropna() \n",
    "    else: # if you've entered neither a list or a dataframe\n",
    "        acc = x.pct_change().dropna()\n",
    "        \n",
    "     # if you want log acceleration  \n",
    "    if logs == True:\n",
    "        acc = np.log(abs(acc))\n",
    "    \n",
    "    df = pd.DataFrame(data=acc, dtype=np.float64)\n",
    "    df.columns = [\"acceleration\"]\n",
    "    return(df)\n",
    "\n",
    "###############################################################################\n",
    "#                        Moving averages and stds\n",
    "#                        volume weighted averages and stds\n",
    "#                        bollinger bands?\n",
    "###############################################################################\n",
    "\n",
    "# Time-weighted Moving Average\n",
    "def twa(data, window = 20, typ = \"sma\", x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving average\n",
    "    \"\"\"\n",
    "    if typ == \"sma\": # simple moving average\n",
    "        if len(data) < window+1:\n",
    "            raise ValueError(\"not enough data\")\n",
    "            \n",
    "        if x == True:\n",
    "            \n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "                if i < window:\n",
    "                    df.append(data[i])\n",
    "                else:\n",
    "                    df.append(np.mean(data[i-window:i]))\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 0]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "        elif x == False:\n",
    "\n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "\n",
    "                ma = np.zeros(window)\n",
    "\n",
    "                for j in range(1,window):\n",
    "\n",
    "                    if i < window:\n",
    "                        ma[j] = data[j]\n",
    "\n",
    "\n",
    "                    if i >= window:\n",
    "\n",
    "                        ma[j] = np.mean(data[i-j:i])\n",
    "\n",
    "                df.append(ma)\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "    elif typ == \"ema\": # exponential moving average\n",
    "\n",
    "        df = pd.DataFrame(index=data.index, dtype=np.float64)\n",
    "        df.loc[:,0] = data.ewm(span=window, min_periods=0, adjust=True, ignore_na=False).mean().values.flatten() # ????\n",
    "    \n",
    "        return(df)\n",
    "      \n",
    "#Time-Price Moving STD\n",
    "def twstd(data, window = 20, x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving standard deviation of prices\n",
    "    also called volatility\n",
    "    \"\"\"\n",
    "    if len(data) < window+1:\n",
    "        raise ValueError(\"not enough data\")\n",
    "\n",
    "    if x == True:\n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "            if i < window:\n",
    "                df.append(data[i])\n",
    "            else:\n",
    "                df.append(np.std(data[i-window:i]))\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 0]\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "    elif x == False:  \n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            mstd = np.zeros(window)\n",
    "\n",
    "            for j in range(1,window):\n",
    "\n",
    "                if i >= j:\n",
    "\n",
    "                    mstd[j] = np.std(data[i-j:i])\n",
    "\n",
    "            df.append(mstd)\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "    \n",
    "        return(df)\n",
    "   \n",
    "    \n",
    "# Volume-weighted Moving Average\n",
    "def vwa(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving average\n",
    "    \"\"\"\n",
    "    \n",
    "    vwa = np.zeros(window)\n",
    "    vwa = list(vwa)\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i < window:\n",
    "            \n",
    "            vwa[i] = data[i]\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            v_sum = sum(weights[i-window:i])\n",
    "            v_w_avg = 0\n",
    "            for j in range(window):\n",
    "                v_w_avg = v_w_avg + data[i-j] * (weights[i-j] / v_sum)\n",
    "\n",
    "            vwa.append(v_w_avg)\n",
    "    \n",
    "    df = pd.DataFrame(vwa)\n",
    "    return(df)\n",
    "\n",
    "#Volume-weighted Moving STD\n",
    "def vwstd(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving standard deviations\n",
    "    \"\"\"\n",
    "    \n",
    "    v_w_avg = 0\n",
    "    vws = np.zeros(window)\n",
    "    vws = list(vws)\n",
    "    for i in range(window, len(data)):\n",
    "    \n",
    "        v_sum = sum(weights[i-window:i])\n",
    "        v_w_std = 0\n",
    "        for j in range(window):\n",
    "            v_w_std = v_w_std + ((data[i-j] - np.mean(data[i-window:i]))**2 )* (weights[i-j] / v_sum)\n",
    "\n",
    "        vws.append(np.sqrt(v_w_std))\n",
    "    \n",
    "    df = pd.DataFrame(vws)\n",
    "    return(df)\n",
    "###############################################################################\n",
    "#                        Technical indicators\n",
    "#                        RSI, Stochastic Oscillator\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def RSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    delta = data.diff().dropna() # Close_now - Close_yesterday\n",
    "    delta = delta.reset_index(drop = True)\n",
    "\n",
    "    u = pd.DataFrame(np.zeros(len(delta))) # make an array of 0s for the up returns\n",
    "    u = u[0]\n",
    "    d = u.copy() # make an array of 0s for the down returns   \n",
    "\n",
    "    u[delta > 0] = delta[delta > 0] # for all the days where delta is up, transfer them to U\n",
    "    d[delta < 0] = -delta[delta < 0] # for all the days where delta is down, transfer them to D\n",
    "\n",
    "    u[u.index[window-1]] = np.mean( u[:window] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    d[d.index[window-1]] = np.mean( d[:window] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    RS = pd.DataFrame.ewm(u, com=window-1, adjust=False).mean() / pd.DataFrame.ewm(d, com=window-1, adjust=False).mean() # EMA(up) / EMA(down)\n",
    "\n",
    "    RSI_ = 100 - (100 / (1 + RS))\n",
    "    return(RSI_)\n",
    "\n",
    "def FRSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    inverse fisher transform on RSI = 0.1*(rsi-50)\n",
    "    fisher rsi = (np.exp(2*rsi)-1) / (np.exp(2*rsi)+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    RSI_ = 0.1 * (RSI(data, window) - 50)\n",
    "    F_RSI = (np.exp(2*RSI_)-1) / (np.exp(2*RSI_)+1)\n",
    "    return(F_RSI)\n",
    "\n",
    "def BB(data, window = 20, std = 2.5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, window):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(window,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-window:i])\n",
    "        std_lst[i] = np.std(data[i-window:i])\n",
    "        \n",
    "    up = mean_lst + std * std_lst\n",
    "    down = mean_lst - std * std_lst\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Upper'] = up\n",
    "    df['Lower'] = down\n",
    "    return(df)\n",
    "\n",
    "def STO(data, N=14, M=3):\n",
    "    assert 'Low' in data.columns\n",
    "    assert 'High' in data.columns\n",
    "    assert 'Close' in data.columns\n",
    "    \n",
    "    data_ = pd.DataFrame()\n",
    "    data_['low_N'] = data['Low'].rolling(N).min()\n",
    "    data_['high_N'] = data['High'].rolling(N).max()\n",
    "    data_['K'] = 100 * (data['Close'] - data_['low_N']) / \\\n",
    "        (data_['high_N'] - data_['low_N']) # The stochastic oscillator\n",
    "    data_['D'] = data_['K'].rolling(M).mean() # the slow and smoothed K\n",
    "    return data_\n",
    "\n",
    "def ADX(high, low, close, lookback):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.rolling(lookback).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha = 1/lookback).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha = 1/lookback).mean() / atr))\n",
    "    dx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = ((dx.shift(1) * (lookback - 1)) + dx) / lookback\n",
    "    adx_smooth = adx.ewm(alpha = 1/lookback).mean()\n",
    "    df['+DI'] = plus_di\n",
    "    df['-DI'] = minus_di\n",
    "    df['ADX'] = adx_smooth\n",
    "    \n",
    "    return df\n",
    "\n",
    "def MACD(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'Close':'macd'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macd':'signal'})\n",
    "    hist = pd.DataFrame(macd['macd'] - signal['signal']).rename(columns = {0:'hist'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df\n",
    "\n",
    "def SuperTrend(high, low, close, lookback, multiplier):\n",
    "    # ATR\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.ewm(lookback).mean()\n",
    "    \n",
    "    # H/L AVG AND BASIC UPPER & LOWER BAND\n",
    "    \n",
    "    hl_avg = (high + low) / 2\n",
    "    upper_band = (hl_avg + multiplier * atr).dropna()\n",
    "    lower_band = (hl_avg - multiplier * atr).dropna()\n",
    "    \n",
    "    # FINAL UPPER BAND\n",
    "    \n",
    "    final_bands = pd.DataFrame(columns = ['upper', 'lower'])\n",
    "    final_bands.iloc[:,0] = [x for x in upper_band - upper_band]\n",
    "    final_bands.iloc[:,1] = final_bands.iloc[:,0]\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i,0] = 0\n",
    "        else:\n",
    "            if (upper_band[i] < final_bands.iloc[i-1,0]) | (close[i-1] > final_bands.iloc[i-1,0]):\n",
    "                final_bands.iloc[i,0] = upper_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,0] = final_bands.iloc[i-1,0]\n",
    "    \n",
    "    # FINAL LOWER BAND\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i, 1] = 0\n",
    "        else:\n",
    "            if (lower_band[i] > final_bands.iloc[i-1,1]) | (close[i-1] < final_bands.iloc[i-1,1]):\n",
    "                final_bands.iloc[i,1] = lower_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,1] = final_bands.iloc[i-1,1]\n",
    "    \n",
    "    # SUPERTREND\n",
    "    \n",
    "    supertrend = pd.DataFrame(columns = [f'supertrend_{lookback}'])\n",
    "    supertrend.iloc[:,0] = [x for x in final_bands['upper'] - final_bands['upper']]\n",
    "    \n",
    "    for i in range(len(supertrend)):\n",
    "        if i == 0:\n",
    "            supertrend.iloc[i, 0] = 0\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] < final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] > final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] > final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] < final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "    \n",
    "    supertrend = supertrend.set_index(upper_band.index)\n",
    "    supertrend = supertrend.dropna()[1:]\n",
    "    \n",
    "    # ST UPTREND/DOWNTREND\n",
    "    \n",
    "    upt = []\n",
    "    dt = []\n",
    "    close = close.iloc[len(close) - len(supertrend):]\n",
    "\n",
    "    for i in range(len(supertrend)):\n",
    "        if close[i] > supertrend.iloc[i, 0]:\n",
    "            upt.append(supertrend.iloc[i, 0])\n",
    "            dt.append(np.nan)\n",
    "        elif close[i] < supertrend.iloc[i, 0]:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(supertrend.iloc[i, 0])\n",
    "        else:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(np.nan)\n",
    "            \n",
    "    st, upt, dt = pd.Series(supertrend.iloc[:, 0]), pd.Series(upt), pd.Series(dt)\n",
    "    upt.index, dt.index = supertrend.index, supertrend.index\n",
    "    \n",
    "    df['ST'] = st\n",
    "    df['UPT'] = upt\n",
    "    df['DT'] = dt\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "Markets = [\"^OMXC25\", \"^OMXSPI\", \"^OSEAX\", \"^OMXH25\", \"^IXIC\", \"^NYA\"]\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) + timedelta(days = 1) \n",
    "todays = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "MDATA5 = pd.DataFrame()\n",
    "MDATA = pd.DataFrame()\n",
    "for i in Markets:\n",
    "    DATA5 = sdata(i,\"2019-01-01\",todays)\n",
    "    DATA = sdata(i,\"2024-01-01\",todays)\n",
    "    MDATA5[str(i)] = DATA5['Close']/DATA5['Close'][0]\n",
    "    MDATA[str(i)] = DATA['Close']/DATA['Close'][0]\n",
    "    \n",
    "    \n",
    "MDATA5['MEAN'] = 1\n",
    "for i in range(len(MDATA5)):\n",
    "    MDATA5['MEAN'][i] = np.mean(MDATA5.iloc[i])\n",
    "MDATA['MEAN'] = 1\n",
    "for i in range(len(MDATA)):\n",
    "    MDATA['MEAN'][i] = np.mean(MDATA.iloc[i])\n",
    "\n",
    "mkts = []   \n",
    "for i in Markets:\n",
    "    if MDATA[str(i)][-1] > MDATA['MEAN'][-1] and MDATA5[str(i)][-1] > MDATA5['MEAN'][-1]:\n",
    "        mkts.append(str(i))\n",
    "        \n",
    "for i in range(len(mkts)):\n",
    "    if mkts[i] == \"^OMXC25\":\n",
    "        mkts[i] = \"CO\"\n",
    "    if mkts[i] == \"^OMXSPI\":\n",
    "        mkts[i] = \"ST\"\n",
    "    if mkts[i] == \"^OSEAX\":\n",
    "        mkts[i] = \"OL\"\n",
    "    if mkts[i] == \"^OMXH25\":\n",
    "        mkts[i] = \"HE\"\n",
    "    if mkts[i] == \"^IXIC\":\n",
    "        mkts[i] = \"NA\"\n",
    "    if mkts[i] == \"^NYA\":\n",
    "        mkts[i] = \"NY\"\n",
    "        \n",
    "for i in range(len(mkts)):\n",
    "    if i == 0:\n",
    "        DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=mkts[i])\n",
    "        for j in range(len(DATA)):\n",
    "            DATA['SYMBOL'][j] = str(DATA['SYMBOL'][j]) + \".\" + str(mkts[i])\n",
    "        DATA = DATA['SYMBOL']\n",
    "    else:\n",
    "        DATA_ = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=mkts[i])\n",
    "        for j in range(len(DATA_)):\n",
    "            DATA_['SYMBOL'][j] = str(DATA_['SYMBOL'][j]) + \".\" + str(mkts[i])\n",
    "        DATA_ = DATA_['SYMBOL']\n",
    "        DATA = pd.concat([DATA, DATA_])\n",
    "        \n",
    "DATA = DATA.reset_index()\n",
    "        \n",
    "with pd.ExcelWriter(\"C:/Users/Markb/OneDrive/Skrivebord/mkts.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\",) as writer:\n",
    "    DATA.to_excel(writer, sheet_name=\"Sheet1\")\n",
    "    \n",
    "mkts\n",
    "\n",
    "\n",
    "UD3_U = pd.DataFrame(0, index = ['DUU','UUD','UDD','DDU','DUD','DDD','UUU','UDU'], columns = ['UP', 'DOWN'])\n",
    "UD5_U = pd.DataFrame(0, index = ['DUDUU','UDUUD','DUUDD','UUDDU','UDDUD','DDUDD','DUDDD','UDDDD','DDDDD','DDDDU','DDDUU','DDUUU','DUUUU','UUUUU','UUUUD','UUUDD','UUDDD','UDDDU','DDDUD','DDUDU','DUDUD','UDUDD','DUUUD','UUUDU','UUDUD','UDUDU','UDUUU','DUUDU','UUDUU','DUDDU','UDDUU','DDUUD'], columns = ['UP', 'DOWN'])\n",
    "\n",
    "UD3_D = pd.DataFrame(0, index = ['DUU','UUD','UDD','DDU','DUD','DDD','UUU','UDU'], columns = ['UP', 'DOWN'])\n",
    "UD5_D = pd.DataFrame(0, index = ['DUDUU','UDUUD','DUUDD','UUDDU','UDDUD','DDUDD','DUDDD','UDDDD','DDDDD','DDDDU','DDDUU','DDUUU','DUUUU','UUUUU','UUUUD','UUUDD','UUDDD','UDDDU','DDDUD','DDUDU','DUDUD','UDUDD','DUUUD','UUUDU','UUDUD','UDUDU','UDUUU','DUUDU','UUDUU','DUDDU','UDDUU','DDUUD'], columns = ['UP', 'DOWN'])\n",
    "\n",
    "\n",
    "makt = \"CO\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) + \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "makt = \"OL\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) + \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "makt = \"HE\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) + \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "makt = \"ST\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) + \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "\n",
    "\n",
    "makt = \"NA\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) #+ \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "makt = \"NY\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) #+ \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "\n",
    "makt = \"US\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) #+ \".\" + str(makt)   \n",
    "    \n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2000-01-01\",todays)\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "        \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from binance.enums import *\n",
    "import numpy as np\n",
    "\n",
    "quantile = 0.05\n",
    "inter = \"1day\"\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) + timedelta(days = 1) \n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -1820) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "tick = \"DOTUSDT\"\n",
    "    \n",
    "klines = client.get_historical_klines(tick, Client.KLINE_INTERVAL_1HOUR, day7, todays)\n",
    "DATA = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "DATA .Open = DATA.Open.astype(float)\n",
    "DATA .High = DATA.High.astype(float)\n",
    "DATA .Low = DATA.Low.astype(float)\n",
    "DATA .Close = DATA.Close.astype(float)\n",
    "DATA .Volume = DATA.Volume.astype(float)\n",
    "DATA_  = DATA [['Open','High','Low','Close','Volume']]\n",
    "\n",
    "DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "DATA_['returns'] = returns(DATA_['Close'])\n",
    "X = False\n",
    "pos_neg = []\n",
    "for j in range(len(DATA_)):\n",
    "    if DATA_['returns'][j] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif DATA_['returns'][j] < 0:\n",
    "        pos_neg.append(-1)\n",
    "    else:\n",
    "        pos_neg.append(0)\n",
    "DATA_['PN_counter'] = pos_neg\n",
    "DATA_['MPN'] = 0\n",
    "for j in range(19, len(DATA_)):\n",
    "    DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "UD3 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "for j in range(5,len(DATA_)):\n",
    "    string = \"\"\n",
    "    for k in range(5):\n",
    "        if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "            string = string + \"U\"\n",
    "        elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "            string = string + \"D\"\n",
    "\n",
    "    if j > 2:\n",
    "        UD3.append(string[2:5])\n",
    "\n",
    "    if j > 4:\n",
    "        UD5.append(string)\n",
    "\n",
    "DATA_['UD3'] = UD3\n",
    "DATA_['UD5'] = UD5\n",
    "\n",
    "for j in range(10, len(DATA_)):\n",
    "\n",
    "    if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "        UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "        UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "        UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "        UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "\n",
    "\n",
    "\n",
    "    if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "        UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "        UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "        UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "\n",
    "    if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "        UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx\", mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\",) as writer:\n",
    "    UD3_U.to_excel(writer, sheet_name=\"UD3_U\")\n",
    "    UD5_U.to_excel(writer, sheet_name=\"UD5_U\")\n",
    "\n",
    "\n",
    "UD3_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD3_U\")\n",
    "UD5_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD5_U\")\n",
    "UD3_U.index = UD3_U['Unnamed: 0']\n",
    "UD3_U = UD3_U.drop(columns=['Unnamed: 0'])\n",
    "UD5_U.index = UD5_U['Unnamed: 0']\n",
    "UD5_U = UD5_U.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "for i in range(len(UD3_U)):\n",
    "    s = UD3_U.iloc[i, 0] + UD3_U.iloc[i, 1]\n",
    "    UD3_U.iloc[i, 0] = UD3_U.iloc[i, 0] / s\n",
    "    UD3_U.iloc[i, 1] = UD3_U.iloc[i, 1] / s \n",
    "    \n",
    "for i in range(len(UD5_U)):\n",
    "    s = UD5_U.iloc[i, 0] + UD5_U.iloc[i, 1]\n",
    "    UD5_U.iloc[i, 0] = UD5_U.iloc[i, 0] / s\n",
    "    UD5_U.iloc[i, 1] = UD5_U.iloc[i, 1] / s  \n",
    "\n",
    "\n",
    "makt = \"CO\"\n",
    "\n",
    "DATA = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/liste.xlsx', 'rb'),sheet_name=str(makt))\n",
    "for i in range(len(DATA)):\n",
    "    DATA['SYMBOL'][i] = str(DATA['SYMBOL'][i]) + \".\" + str(makt)\n",
    "\n",
    "    \n",
    "    \n",
    "lst = []\n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = sdata(DATA['SYMBOL'][i],\"2024-01-01\",'2024-10-13')\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['XMA'] = 0\n",
    "        DATA_['PXMA'] = 0\n",
    "        for j in range(40, len(DATA_)):\n",
    "            if DATA_['Close'][j] > DATA_['MA5'][j] and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j]:\n",
    "                DATA_['XMA'][j] = 1\n",
    "                \n",
    "            elif DATA_['Close'][j] < DATA_['MA5'][j] and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j]:\n",
    "                DATA_['XMA'][j] = -1\n",
    "\n",
    "        if DATA_['XMA'][-1] == 1 or DATA_['XMA'][-2] == 1 or DATA_['XMA'][-3] == 1:\n",
    "            X = True\n",
    "        \n",
    "        lst.append([DATA['SYMBOL'][i], np.mean(DATA_['MPN']), np.mean(DATA_['returns']), DATA_['XMA'][-1], DATA_['UD3'][-1], DATA_['UD5'][-1]])\n",
    "\n",
    "\n",
    "lst = pd.DataFrame(lst, columns = [\"Symbol\", \"mean PN\", \"mean returns\", \"MA cross?\", \"UD3\", \"UD5\"])\n",
    "#lst1 = lst[lst['mean PN'] > np.mean(lst['mean PN'])]\n",
    "#lst2 = lst1[lst1['mean returns'] > np.mean(lst['mean returns'])]\n",
    "UPLST = lst[lst['MA cross?'] == 1]\n",
    "#DOLST = lst[lst['MA cross?'] == -1]\n",
    "UPLST = UPLST[UPLST['UD5'].isin(UD5_U['UP'][UD5_U['UP'] > 0.85].index)]\n",
    "UPLST = UPLST[UPLST['UD3'].isin(UD3_U['UP'][UD3_U['UP'] > 0.85].index)]\n",
    "UPLST\n",
    "#DOLST[DOLST['UD5'].isin(UD5_D['UP'][UD5_D['UP'] > np.mean(UD5_D['UP'])].index)]\n",
    "\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, mstats\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import edge\n",
    "import edge.edge_mean_reversion as emr\n",
    "import edge.edge_risk_kit as erk\n",
    "import TMRW\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from binance.enums import *\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "todays = datetime(today.year,today.month,today.day) + timedelta(days =-1450) \n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -1555) \n",
    "todays = todays.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "fee = 0.000715\n",
    "#for tick in ['SOLUSDT', 'TRXUSDT', 'XRPUSDT', 'ETHUSDT', 'BTCUSDT', 'BNBUSDT', 'ADAUSDT', 'ATOMUSDT', 'DOGEUSDT', 'LTCUSDT', 'XLMUSDT', 'NEARUSDT', 'AVAXUSDT']:\n",
    "tick = \"LTCUSDT\"\n",
    "\n",
    "def bollinger_bands(data, length):\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, length):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(length,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-length:i])\n",
    "        std_lst[i] = np.std(data[i-length:i])\n",
    "        \n",
    "    up = mean_lst + 2.5 * std_lst\n",
    "    down = mean_lst - 2.5 * std_lst\n",
    "    \n",
    "    boll = pd.DataFrame()\n",
    "    boll['Upper'] = up\n",
    "    boll['Lower'] = down\n",
    "    return(boll)\n",
    "\n",
    "klines = client.get_historical_klines(tick, Client.KLINE_INTERVAL_1HOUR, '2019-01-02', '2024-07-21')\n",
    "data = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "\n",
    "\n",
    "data.Open = data.Open.astype(float)\n",
    "data.High = data.High.astype(float)\n",
    "data.Low = data.Low.astype(float)\n",
    "data.Close = data.Close.astype(float)\n",
    "data.Volume = data.Volume.astype(float)\n",
    "data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "data['value'] = (data['Open'] + data['High'] + data['Low']) / 3\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['Open'])['Open'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "data['Lower'] = bollinger_bands(data['returns'],480)['Lower']\n",
    "data['Upper'] = bollinger_bands(data['returns'],480)['Upper']\n",
    "\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],6)\n",
    "data['mean_return'] = data['returns'].rolling(window=30).mean().values.flatten()\n",
    "\n",
    "pos_neg = []\n",
    "for i in range(len(data)):\n",
    "    if data['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif data['returns'][i] < 0:\n",
    "        pos_neg.append(-1)\n",
    "        \n",
    "data['pos_neg'] = pos_neg\n",
    "\n",
    "\n",
    "\n",
    "# 1 day aggregation\n",
    "\n",
    "quantile = 0.05\n",
    "interval = 24\n",
    "\n",
    "data_day = pd.DataFrame()\n",
    "_Open = []\n",
    "_Close = []\n",
    "_High = []\n",
    "_Low = []\n",
    "_timeframe = round(len(data)/24)\n",
    "for i in range(_timeframe-1):\n",
    "    _Open.append(data['Open'][i*24])\n",
    "    _Close.append(data['Close'][(i+1)*24])\n",
    "    _High.append(max(data['High'][i*24:(i+1)*24]))\n",
    "    _Low.append(min(data['Low'][i*24:(i+1)*24]))\n",
    "\n",
    "data_day['Open'] = _Open\n",
    "data_day['Close'] = _Close\n",
    "data_day['High'] = _High\n",
    "data_day['Low'] = _Low\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data_day['Open'])['Open'])\n",
    "a.insert(0,0)\n",
    "data_day['returns'] = a\n",
    "\n",
    "Q1 = np.zeros(len(data_day))\n",
    "Q2 = np.zeros(len(data_day))\n",
    "for i in range(interval,len(data_day)):\n",
    "    Q1[i] = np.quantile(data_day['returns'][i-interval:i], 1-quantile)\n",
    "    Q2[i] = np.quantile(data_day['returns'][i-interval:i], quantile)\n",
    "\n",
    "data_day['Q1'] = Q1\n",
    "data_day['Q2'] = Q2\n",
    "\n",
    "\n",
    "# 3 day aggregation\n",
    "\n",
    "quantile = 0.1\n",
    "interval = 72\n",
    "\n",
    "data_3day = pd.DataFrame()\n",
    "_Open = []\n",
    "_Close = []\n",
    "_High = []\n",
    "_Low = []\n",
    "_timeframe = round(len(data)/72)\n",
    "for i in range(_timeframe-1):\n",
    "    _Open.append(data['Open'][i*72])\n",
    "    _Close.append(data['Close'][(i+1)*72])\n",
    "    _High.append(max(data['High'][i*72:(i+1)*72]))\n",
    "    _Low.append(min(data['Low'][i*72:(i+1)*72]))\n",
    "    \n",
    "data_3day['Open'] = _Open\n",
    "data_3day['Close'] = _Close\n",
    "data_3day['High'] = _High\n",
    "data_3day['Low'] = _Low\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data_3day['Open'])['Open'])\n",
    "a.insert(0,0)\n",
    "data_3day['returns'] = a\n",
    "\n",
    "Q1 = np.zeros(len(data_3day))\n",
    "Q2 = np.zeros(len(data_3day))\n",
    "for i in range(interval,len(data_3day)):\n",
    "    Q1[i] = np.quantile(data_3day['returns'][i-interval:i], 1-quantile)\n",
    "    Q2[i] = np.quantile(data_3day['returns'][i-interval:i], quantile)\n",
    "\n",
    "data_3day['Q1'] = Q1\n",
    "data_3day['Q2'] = Q2\n",
    "\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "data['MA5'] = TMRW.FINANCE.twa(data['value'], 20)[19]\n",
    "data['MA15'] = TMRW.FINANCE.twa(data['value'], 60)[59] # 20 day\n",
    "data['MA30'] = TMRW.FINANCE.twa(data['value'], 120)[119] # 30 day\n",
    "\n",
    "data['PN20'] = 0\n",
    "data['PN50'] = 0\n",
    "data['PN100'] = 0\n",
    "data['PN200'] = 0\n",
    "\n",
    "for i in range(24, len(data)):\n",
    "    data['PN20'][i] = np.mean(data['pos_neg'][i-24:i]) # 1 day\n",
    "for i in range(120, len(data)):\n",
    "    data['PN50'][i] = np.mean(data['pos_neg'][i-120:i]) # 5 day\n",
    "for i in range(240, len(data)):\n",
    "    data['PN100'][i] = np.mean(data['pos_neg'][i-240:i]) # 10 day\n",
    "for i in range(480, len(data)):\n",
    "    data['PN200'][i] = np.mean(data['pos_neg'][i-480:i]) # 20 day\n",
    "    \n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "\n",
    "for i in range(480, len(data)):\n",
    "\n",
    "    j = round(len(data_day)/len(data) * i)\n",
    "\n",
    "    if j >= len(data_day):\n",
    "        j = len(data_day) - 1\n",
    "\n",
    "    # generate long/buy signal        \n",
    "    if sum(data['returns'][i-(interval):i]) > data_day['Q1'][j] and sum(data['returns'][i-(interval):i]) > data['Upper'][i]:\n",
    "        if sum(data['returns'][i-2:i]) < sum(data['returns'][i-20:i-2]) and sum(data['returns'][i-20:i-2]) > sum(data['returns'][i-30:i-20]):\n",
    "            if data['Open'][i] > 1.15 * np.mean(data['Open'][i-480:i]):\n",
    "                data['sell_signal'][i] = True\n",
    "\n",
    "    #if data['Open'][i] < data['Lower'][i]:\n",
    "        #data['buy_signal'][i] = True\n",
    "\n",
    "    if sum(data['returns'][i-(interval):i]) < data_day['Q2'][j] and sum(data['returns'][i-(interval):i]) < data['Lower'][i]:\n",
    "        if sum(data['returns'][i-2:i]) > sum(data['returns'][i-20:i-2]) and sum(data['returns'][i-20:i-2]) < sum(data['returns'][i-30:i-20]):\n",
    "            if data['Open'][i] < np.mean(data['Open'][i-720:i]):\n",
    "                if data['Open'][i] > data['Open'][i-4] and data['Open'][i] > data['Open'][i-3] and data['Open'][i] > data['Open'][i-2] and data['Open'][i] > data['Open'][i-1]:\n",
    "                    if np.mean(data['Open'][i-10:i]) > 1.01 * data['Open'][i]:\n",
    "                        data['buy_signal'][i] = True\n",
    "                        \n",
    "    #if data['Open'][i] > data['Upper'][i]:\n",
    "        #data['sell_signal'][i] = True \n",
    "        #buy = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(24, len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "\n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "val - 100\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "\n",
    "for i in range(100, len(data)):\n",
    "      \n",
    "    if data['PN200'][i] < 0.02: # down trends ?\n",
    "        data['buy_signal'][i] = True\n",
    "        \n",
    "    if data['PN50'][i] < -0.05 and data['PN100'][i] < -0.02:\n",
    "        data['buy_signal'][i] = True\n",
    "    \n",
    "    if data['PN200'][i] > 0.1:\n",
    "        data['sell_signal'][i] = True\n",
    "        \n",
    "    if data['PN50'][i] > 0.1 and data['PN100'][i] > 0.1: # up trends?\n",
    "        data['sell_signal'][i] = True\n",
    "    \n",
    "    #else:\n",
    "        #if data['PN20'][i] > 0: # and data['PN100'][i] < 0:\n",
    "            #data['buy_signal'][i] = True\n",
    "        \n",
    "        #if data['PN20'][i] < 0 and data['PN100'][i] > 0 and data['PN200'][i] > 0:\n",
    "            #data['sell_signal'][i] = True\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(24, len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "\n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "val - 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sigma Strategy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd021269-b12f-4776-9190-9c05bcc459e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#import needed modules\n",
    "#import needed modules\n",
    "from datetime import datetime\n",
    "from pandas_datareader import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log, polyfit, sqrt, std, subtract\n",
    "import statsmodels.tsa.stattools as ts\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "import sqlite3 as db\n",
    "\n",
    "#set the database file path we wish to connect to\n",
    "#this will obviously be unique to wherever you created\n",
    "#the SQLite database on your local system\n",
    "database = 'C:\\Users\\S666\\sqlite_databases\\etfs.db'\n",
    " \n",
    "#this is the SQL statement containing the information\n",
    "#regarding which tickers we want to pull from the database\n",
    "#As an example I have chosen to pull down all tickers which\n",
    "#have their \"Focus\" listed as being \"Silver\"\n",
    "sql = 'SELECT Ticker FROM etftable WHERE Niche = \"Value\";'\n",
    " \n",
    "#create a connection to the database specified above\n",
    "cnx = db.connect(database)\n",
    "cur = cnx.cursor()\n",
    " \n",
    "#execute the SQL statement and place the results into a \n",
    "#variable called \"tickers\"\n",
    "tickers = pd.read_sql(sql, con=cnx)\n",
    " \n",
    "#create an empty list\n",
    "symbList = []\n",
    " \n",
    "#iterate over the DataFrame and append each item into the empty list\n",
    "for i in xrange(len(tickers)):\n",
    "    symbList.append(tickers.ix[i][0])\n",
    "    \n",
    "    \n",
    "def get_symb_pairs(symbList):\n",
    "    '''symbList is a list of ETF symbols\n",
    "       This function takes in a list of symbols and \n",
    "       returns a list of unique pairs of symbols'''\n",
    " \n",
    "    symbPairs = []\n",
    " \n",
    "    i = 0\n",
    " \n",
    "    #iterate through the list and create all possible combinations of\n",
    "    #ticker pairs - append the pairs to the \"symbPairs\" list\n",
    "    while i < len(symbList)-1:\n",
    "        j = i + 1\n",
    "        while j < len(symbList): \n",
    "            symbPairs.append([symbList[i],symbList[j]]) \n",
    "            j += 1 \n",
    "            i += 1 \n",
    "            \n",
    "    #iterate through the newly created list of pairs and remove any pairs #made up of two identical tickers \n",
    "    for i in symbPairs: \n",
    "        if i[0] == i[1]: \n",
    "            symbPairs.remove(i) \n",
    "    #create a new empty list to store only unique pairs \n",
    "    symbPairs2 = [] \n",
    "    #iterate through the original list and append only unique pairs to the \n",
    "    #new list \n",
    "    for i in symbPairs: \n",
    "        if i not in symbPairs2: \n",
    "            symbPairs2.append(i) \n",
    "    return symbPairs2 \n",
    "\n",
    "symbPairs = get_symb_pairs(symbList) \n",
    "\n",
    "def backtest(symbList): \n",
    "    start_date = '2012/01/01' \n",
    "    end_date = datetime.now() \n",
    "    #download data from Yahoo Finance \n",
    "    y=data.DataReader(symbList[0], \"yahoo\", start=start_date, end=end_date) \n",
    "    x=data.DataReader(symbList[1], \"yahoo\", start=start_date, end=end_date) \n",
    "    #rename column to make it easier to work with later \n",
    "    y.rename(columns={'Adj Close':'price'}, inplace=True) \n",
    "    x.rename(columns={'Adj Close':'price'}, inplace=True) \n",
    "    #make sure DataFrames are the same length \n",
    "    min_date = max(df.dropna().index[0] for df in [y, x]) \n",
    "    max_date = min(df.dropna().index[-1] for df in [y, x]) \n",
    "    y = y[(y.index >= min_date) & (y.index <= max_date)] \n",
    "    x = x[(x.index >= min_date) & (x.index <= max_date)]\n",
    "    \n",
    "    ############################################################\n",
    "    \n",
    "    plt.plot(y.price,label=symbList[0])\n",
    "    plt.plot(x.price,label=symbList[1])\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    sns.jointplot(y.price, x.price ,color='b')\n",
    "    plt.show()\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    #run Odinary Least Squares regression to find hedge ratio\n",
    "    #and then create spread series\n",
    "    df1 = pd.DataFrame({'y':y['price'],'x':x['price']})\n",
    "    est = sm.OLS(df1.y,df1.x)\n",
    "    est = est.fit()\n",
    "    df1['hr'] = -est.params[0]\n",
    "    df1['spread'] = df1.y + (df1.x * df1.hr)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    plt.plot(df1.spread)\n",
    "    plt.show()\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    cadf = ts.adfuller(df1.spread)\n",
    "    print 'Augmented Dickey Fuller test statistic =',cadf[0]\n",
    "    print 'Augmented Dickey Fuller p-value =',cadf[1]\n",
    "    print 'Augmented Dickey Fuller 1%, 5% and 10% test statistics =',cadf[4]\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    def hurst(ts):\n",
    "    \t\"\"\"Returns the Hurst Exponent of the time series vector ts\"\"\"\n",
    "    \t# Create the range of lag values\n",
    "    \tlags = range(2, 100)\n",
    "     \n",
    "    \t# Calculate the array of the variances of the lagged differences\n",
    "    \ttau = [sqrt(std(subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "     \n",
    "    \t# Use a linear fit to estimate the Hurst Exponent\n",
    "    \tpoly = polyfit(log(lags), log(tau), 1)\n",
    "     \n",
    "    \t# Return the Hurst exponent from the polyfit output\n",
    "    \treturn poly[0]*2.0\n",
    "     \n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    print \"Hurst Exponent =\",round(hurst(df1.spread),2)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    #Run OLS regression on spread series and lagged version of itself\n",
    "    spread_lag = df1.spread.shift(1)\n",
    "    spread_lag.ix[0] = spread_lag.ix[1]\n",
    "    spread_ret = df1.spread - spread_lag\n",
    "    spread_ret.ix[0] = spread_ret.ix[1]\n",
    "    spread_lag2 = sm.add_constant(spread_lag)\n",
    "     \n",
    "    model = sm.OLS(spread_ret,spread_lag2)\n",
    "    res = model.fit()\n",
    "     \n",
    "     \n",
    "    halflife = round(-np.log(2) / res.params[1],0)\n",
    "    \n",
    "    if halflife <= 0:\n",
    "        halflife = 1\n",
    "     \n",
    "    print  'Halflife = ',halflife\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    \n",
    "    meanSpread = df1.spread.rolling(window=halflife).mean()\n",
    "    stdSpread = df1.spread.rolling(window=halflife).std()\n",
    "        \n",
    "    df1['zScore'] = (df1.spread-meanSpread)/stdSpread\n",
    "        \n",
    "    df1['zScore'].plot()\n",
    "    plt.show()\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    entryZscore = 2\n",
    "    exitZscore = 0\n",
    "    \n",
    "    #set up num units long             \n",
    "    df1['long entry'] = ((df1.zScore < - entryZscore) & ( df1.zScore.shift(1) > - entryZscore))\n",
    "    df1['long exit'] = ((df1.zScore > - exitZscore) & (df1.zScore.shift(1) < - exitZscore)) df1['num units long'] = np.nan df1.loc[df1['long entry'],'num units long'] = 1 df1.loc[df1['long exit'],'num units long'] = 0 df1['num units long'][0] = 0 df1['num units long'] = df1['num units long'].fillna(method='pad') #set up num units short df1['short entry'] = ((df1.zScore >  entryZscore) & ( df1.zScore.shift(1) < entryZscore))\n",
    "    df1['short exit'] = ((df1.zScore < exitZscore) & (df1.zScore.shift(1) > exitZscore))\n",
    "    df1.loc[df1['short entry'],'num units short'] = -1\n",
    "    df1.loc[df1['short exit'],'num units short'] = 0\n",
    "    df1['num units short'][0] = 0\n",
    "    df1['num units short'] = df1['num units short'].fillna(method='pad')\n",
    "    \n",
    "    df1['numUnits'] = df1['num units long'] + df1['num units short']\n",
    "    df1['spread pct ch'] = (df1['spread'] - df1['spread'].shift(1)) / ((df1['x'] * abs(df1['hr'])) + df1['y'])\n",
    "    df1['port rets'] = df1['spread pct ch'] * df1['numUnits'].shift(1)\n",
    "    \n",
    "    df1['cum rets'] = df1['port rets'].cumsum()\n",
    "    df1['cum rets'] = df1['cum rets'] + 1\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    try:\n",
    "        sharpe = ((df1['port rets'].mean() / df1['port rets'].std()) * sqrt(252)) \n",
    "    except ZeroDivisionError:\n",
    "        sharpe = 0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(df1['cum rets'])\n",
    "    plt.xlabel(i[1])\n",
    "    plt.ylabel(i[0])\n",
    "    plt.show()\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    \n",
    "        \n",
    "    start_val = 1\n",
    "    end_val = df1['cum rets'].iat[-1]\n",
    "        \n",
    "    start_date = df1.iloc[0].name\n",
    "    end_date = df1.iloc[-1].name\n",
    "    days = (end_date - start_date).days\n",
    "        \n",
    "    CAGR = round(((float(end_val) / float(start_val)) ** (252.0/days)) - 1,4)\n",
    "    \n",
    "    \n",
    "    print \"CAGR = {}%\".format(CAGR*100)\n",
    "    print \"Sharpe Ratio = {}\".format(round(sharpe,2))\n",
    "    print 100*\"----\"\n",
    "    \n",
    "for i in symbPairs:\n",
    "    backtest(i)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data\n",
    "\n",
    "sp500 = data.DataReader('^GSPC', 'yahoo',start='1/1/2000')\n",
    "\n",
    "s500.head()\n",
    "\n",
    "sp500['Close'].plot(grid=True,figsize=(8,5))\n",
    "\n",
    "sp500['42d'] = np.round(sp500['Close'].rolling(window=42).mean(),2)\n",
    "sp500['252d'] = np.round(sp500['Close'].rolling(window=252).mean(),2)\n",
    "\n",
    "sp500.tail\n",
    "\n",
    "sp500[['Close','42d','252d']].plot(grid=True,figsize=(8,5))\n",
    "\n",
    "sp500['42-252'] = sp500['42d'] - sp500['252d']\n",
    "\n",
    "X = 50\n",
    "sp500['Stance'] = np.where(sp500['42-252'] > X, 1, 0)\n",
    "sp500['Stance'] = np.where(sp500['42-252'] < -X, -1, sp500['Stance'])\n",
    "sp500['Stance'].value_counts()\n",
    "\n",
    "sp500['Stance'].plot(lw=1.5,ylim=[-1.1,1.1])\n",
    "\n",
    "sp500['Market Returns'] = np.log(sp500['Close'] / sp500['Close'].shift(1))\n",
    "sp500['Strategy'] = sp500['Market Returns'] * sp500['Stance'].shift(1)\n",
    "\n",
    "sp500[['Market Returns','Strategy']].cumsum().plot(grid=True,figsize=(8,5))\n",
    "\n",
    "\n",
    "# zipline's magic command for enabling support to run in jupyter notebook\n",
    "%load_ext zipline\n",
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from alphatools.research import get_pricing\n",
    "\n",
    "START_DATE = pd.to_datetime('2016-6-1')#pd.datetime(2007,1,3)\n",
    "END_DATE = pd.to_datetime('2016-12-30')\n",
    "stock_x, stock_y = 'CVX', 'XOM'\n",
    "df = get_pricing([stock_x, stock_y], START_DATE, END_DATE)\n",
    "df = df.rename(lambda col: col.symbol.encode('UTF-8'), axis='columns')\n",
    "\n",
    "\n",
    "_, p_value, _ = coint(df[stock_x], df[stock_y])\n",
    "if p_value < 0.05:\n",
    "    print('{0} and {1} are cointegrated!'.format(stock_x, stock_y))\n",
    "else:\n",
    "    print('{0} and {1} are NOT cointegrated!'.format(stock_x, stock_y))\n",
    "    \n",
    "X, Y = df[stock_x], df[stock_y]\n",
    "\n",
    "def get_beta(X, Y):\n",
    "    stock_x = X.name\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    X = X[stock_x]\n",
    "    return model.params[stock_x]\n",
    "\n",
    "beta = get_beta(X,Y)\n",
    "spread = Y - beta * X\n",
    "plt.figure()\n",
    "spread.plot()\n",
    "plt.axhline(spread.mean(), color='black')\n",
    "plt.legend(['Spread']);\n",
    "\n",
    "def zscore(series):\n",
    "    return (series - series.mean()) / np.std(series)\n",
    "\n",
    "plt.figure()\n",
    "zscore(spread).plot()\n",
    "plt.axhline(zscore(spread).mean(), color='black')\n",
    "plt.axhline(1.0, color='red', linestyle='--')\n",
    "plt.axhline(-1.0, color='green', linestyle='--')\n",
    "plt.legend(['Spread z-score', 'Mean', '+1', '-1']);\n",
    "plt.show()\n",
    "\n",
    "from zipline import run_algorithm\n",
    "from zipline.api import symbol, date_rules, time_rules, order_target_percent\n",
    "\n",
    "def initialize(context):\n",
    "    context.pair = (symbol(stock_x), symbol(stock_y))\n",
    "    \n",
    "    context.z_signal = 2\n",
    "    context.z_exit = 0\n",
    "    context.lookback = 200 # used for regression\n",
    "    #context.z_window = 20 # used for zscore calculation, must be <= lookback\n",
    "    context.status = 'flat'\n",
    "    \n",
    "    context.target_weights = { k: 0 for k in context.pair}\n",
    "    \n",
    "    # schedule to execute strategy 30 minutes before close\n",
    "    schedule_function(func=check_pair_status, date_rule=date_rules.every_day(), time_rule=time_rules.market_close(minutes=30))\n",
    "    \n",
    "def check_pair_status(context, data):\n",
    "    \n",
    "    prices = data.history(context.pair, 'price', 35, '1d').iloc[-context.lookback::]\n",
    "    \n",
    "    new_spreads = np.ndarray((len(context.pair), 1))\n",
    "    \n",
    "\n",
    "    stock_x, stock_y = context.pair\n",
    "    \n",
    "    Y = prices[stock_y]\n",
    "    X = prices[stock_x]\n",
    "    \n",
    "    beta = get_beta(X, Y) \n",
    "\n",
    "    spread = Y - beta * X\n",
    "\n",
    "    z = zscore(spread)[-1]\n",
    "    \n",
    "    # short spread\n",
    "    if z > context.z_signal and not context.status == 'short':\n",
    "        context.target_weights[stock_x] = -0.5 # short top\n",
    "        context.target_weights[stock_y] = 0.5 # long bottom\n",
    "        context.status = 'short'\n",
    "            \n",
    "    # long spread\n",
    "    elif z < -context.z_signal and not context.status == 'long':\n",
    "        context.target_weights[stock_x] = 0.5 # long top\n",
    "        context.target_weights[stock_y] = -0.5 # short bottom\n",
    "        context.status = 'long'\n",
    "            \n",
    "    # exit case\n",
    "    elif abs(z) < context.z_exit:\n",
    "        context.target_weights[stock_x] = 0 # close out\n",
    "        context.target_weights[stock_y] = 0 # close out\n",
    "        context.status = 'flat'\n",
    "    record('zscore', z)\n",
    "        \n",
    "    for ticker, weight in context.target_weights.items():\n",
    "        order_target_percent(ticker, weight)\n",
    "        \n",
    "START_DATE = pd.to_datetime('2007-6-3')\n",
    "END_DATE = pd.to_datetime('2016-12-30')\n",
    "perf = run_algorithm(\n",
    "    initialize=initialize,\n",
    "    data_frequency='daily',\n",
    "    capital_base=10000,\n",
    "    bundle='quantopian-quandl',\n",
    "    start=START_DATE.tz_localize('US/Eastern'),\n",
    "    end=END_DATE.tz_localize('US/Eastern'))\n",
    "\n",
    "plt.figure()\n",
    "perf.portfolio_value.plot()\n",
    "plt.legend(['Equity']);\n",
    "plt.ylabel('return $')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "perf.zscore.plot()\n",
    "plt.axhline(2, color='green', linestyle='--')\n",
    "plt.axhline(-2, color='red', linestyle='--')\n",
    "plt.ylabel('zscore')\n",
    "plt.show()\n",
    "\n",
    "%load_ext zipline\n",
    "%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from zipline.api import symbol, order_percent, schedule_function, date_rules, time_rules, symbol, record\n",
    "\n",
    "from utils.plotting import plot_acf\n",
    "\n",
    "ts = None\n",
    "ticker = None\n",
    "\n",
    "def before_trading_start(context, data):\n",
    "    # extract historical data from zipline backtester to use outside of backtester\n",
    "    global ts\n",
    "    global ticker\n",
    "    ticker = 'MCD'\n",
    "    ts = pd.DataFrame(data.history(symbol(ticker), 'price', 500, '1d'))\n",
    "    ts = ts.rename(index=str, columns={symbol(ticker): ticker})\n",
    "    ts.index = pd.to_datetime(ts.index)\n",
    "    \n",
    "    print('ts head:')\n",
    "    print(ts.head())\n",
    "    print('ts tail:')\n",
    "    print(ts.tail())\n",
    "    \n",
    "%zipline --bundle quantopian-quandl --start 2015-6-3 --end 2015-6-4 -o backtest.pickle\n",
    "\n",
    "# log scale\n",
    "ts['log'] = np.log(ts)\n",
    "\n",
    "# log return\n",
    "ts['logret'] = ts['log'] - ts['log'].shift()\n",
    "ts = ts[1:]\n",
    "\n",
    "print('ts head:')\n",
    "print(ts.head())\n",
    "print('ts tail:')\n",
    "print(ts.tail())\n",
    "\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "ax1=fig.add_subplot(211)\n",
    "ax2=fig.add_subplot(212)\n",
    "\n",
    "# plot log return\n",
    "ts['logret'].plot(ax=ax1)\n",
    "\n",
    "plot_acf(ts['logret'])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "def fit_arima(X, p_max=20, d_max=1, q_max=20):\n",
    "    \n",
    "    ps, ds, qs = range(p_max + 1), range(d_max + 1), range(q_max + 1)\n",
    "    best_params_aic, best_model_aic = None, None\n",
    "    best_params_bic, best_model_bic = None, None\n",
    "    for p, d, q in itertools.product(ps, ds, qs):\n",
    "        if p == 0 and q == 0:\n",
    "            continue\n",
    "        try:\n",
    "            model_arima = sm.tsa.ARIMA(X, (p,d,q)).fit()\n",
    "\n",
    "            # save best model according to AIC\n",
    "            if not best_model_aic or model_arima.aic < best_model_aic.aic:\n",
    "                best_params_aic = (p,d,q)\n",
    "                best_model_aic = model_arima\n",
    "\n",
    "            # save best model according to BIC\n",
    "            if not best_model_bic or model_arima.bic < best_model_bic.bic:\n",
    "                best_params_bic = (p,d,q)\n",
    "                best_model_bic = model_arima\n",
    "        except:\n",
    "            pass\n",
    "            #print('could not fit ({},{},{})'.format(p,d,q))\n",
    "\n",
    "    # chose model with fewest params\n",
    "    if sum(best_params_aic) < sum(best_params_bic):\n",
    "        best_params, best_model = best_params_aic, best_model_aic\n",
    "    else:\n",
    "        best_params, best_model = best_params_bic, best_model_bic\n",
    "\n",
    "    print('----Summary----')\n",
    "    print('Best model AIC: {}'.format(best_params_aic))\n",
    "    print('Best model BIC: {}'.format(best_params_bic))\n",
    "    print('Best overall model: {}'.format(best_params))\n",
    "    \n",
    "    # TODO: returns only from aic atm. Too many models fail to converge\n",
    "    return best_model_aic, best_params_aic\n",
    "\n",
    "best_model, best_params = fit_arima(ts['logret'], 5, 1, 5)\n",
    "\n",
    "print('best model params: ',best_params)\n",
    "_, p_values = sm.stats.diagnostic.acorr_ljungbox(best_model.resid, lags=20, boxpierce=False)\n",
    "print('p values: ', list(map(lambda x: \"{:.4}\".format(x),p_values)))\n",
    "print('number of p values below 0.05: ', len([p for p in p_values if p < 0.05]))\n",
    "print(best_model.summary())\n",
    "\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "ax=fig.add_subplot(111)\n",
    "plot_acf(best_model.resid,nlags=35)\n",
    "\n",
    "fig=plt.figure(figsize=(10,20))\n",
    "\n",
    "\"\"\" Sample forecasts for different dates \"\"\"\n",
    "ax=fig.add_subplot(311)\n",
    "step = 20\n",
    "for start in range(step, len(ts['log']) - step, step):\n",
    "    preds = best_model.predict(start=start, end=start + 10)\n",
    "    ret = [ts['log'][start]]\n",
    "    for pred in preds:\n",
    "        ret.append(ret[-1] + pred)\n",
    "    ret = pd.Series(np.exp(ret[1:]), index=preds.index)\n",
    "    ret.plot(ax=ax)\n",
    "ts[ticker].plot(ax=ax, label=ticker)\n",
    "\n",
    "\"\"\" Simple backtest \"\"\"\n",
    "ax=fig.add_subplot(312)\n",
    "signal = []\n",
    "step = 1\n",
    "# simple signal generation and backtest\n",
    "for start in range(0,len(ts['log']),step):\n",
    "    preds = best_model.predict(start=start, end=start)\n",
    "    signal.append(np.sign(preds[0]))\n",
    "ret = (ts[ticker] - ts[ticker].shift()) * np.array(signal)\n",
    "ret[0] = ts[ticker][0]\n",
    "ret.cumsum().plot(ax=ax, label='backtest')\n",
    "ts[ticker].plot(ax=ax, label=ticker)\n",
    "ax.legend()\n",
    "print('NOTE! THIS IS IN-SAMPLE BACKTEST RESULT WITHOUT TRADING COSTS')\n",
    "\n",
    "\"\"\" Forecasting the future \"\"\"\n",
    "ax=fig.add_subplot(313)\n",
    "preds = best_model.forecast(steps=20)[0]\n",
    "ret = [ts['log'][-1]]\n",
    "ret_dates = [ts.index[-1]]\n",
    "for pred in preds:\n",
    "    ret.append(ret[-1] + pred)\n",
    "    ret_dates.append(ret_dates[-1] + pd.Timedelta(days=1))\n",
    "ret = pd.Series(np.exp(ret[1:]), index=ret_dates[1:])\n",
    "ts[ticker].plot(ax=ax, label=ticker)\n",
    "ret.plot(ax=ax, label=ticker + ' prediction')\n",
    "ax.legend()\n",
    "\n",
    "def initialize(context):\n",
    "    context.ticker = symbol('MCD')\n",
    "    \n",
    "    schedule_function(\n",
    "        rebalance_daily,\n",
    "        date_rules.every_day(),\n",
    "        time_rules.market_open()\n",
    "    )\n",
    "\n",
    "def before_trading_start(context, data):\n",
    "    \"\"\" Called once per day before trading starts \"\"\"\n",
    "    \n",
    "    ts = data.history(context.ticker, 'price', 500, '1d')\n",
    "    ts = ts.rename('price')\n",
    "    \n",
    "    # log scale\n",
    "    ts = np.log(ts)\n",
    "\n",
    "    # log return\n",
    "    ts = ts - ts.shift()\n",
    "    ts = ts[1:]\n",
    "    \n",
    "    # fit ARIMA model to data\n",
    "    best_model, best_params = fit_arima(ts, 5, 1, 5)\n",
    "    pred = best_model.forecast()[0][0]\n",
    "   \n",
    "    context.weight = 1 if pred > 0 else -1\n",
    "    \n",
    "def handle_data(context,data):\n",
    "    \"\"\" Called every tick \"\"\"\n",
    "    # record data for visualization\n",
    "    record(leverage=context.account.leverage)\n",
    "    record(price=data.current(context.ticker, 'price'))\n",
    "    record(signal=context.weight)\n",
    "    \n",
    "    \n",
    "def rebalance_daily(context, data):\n",
    "    \"\"\" Daily rebalance portfolio function \"\"\"\n",
    "    if context.ticker in context.portfolio.positions:\n",
    "        if (context.portfolio.positions[context.ticker].amount > 0 and context.weight < 0) or \\\n",
    "        (context.portfolio.positions[context.ticker].amount < 0 and context.weight > 0):\n",
    "            # change direction\n",
    "            order_percent(context.ticker, context.weight)\n",
    "        \n",
    "    else:\n",
    "        # new position\n",
    "        order_percent(context.ticker, context.weight)\n",
    "    \n",
    "    \n",
    "def analyze(context, perf):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10,14))\n",
    "    ax1.plot(perf.price, label='price', c='black')\n",
    "    ax1.set_ylabel('Price chart')\n",
    "    \n",
    "    ax2.plot(perf.portfolio_value)\n",
    "    ax2.set_ylabel('portfolio $ value')\n",
    "    ax2.legend(loc=0)\n",
    "    \n",
    "    ax3.plot(perf.signal)\n",
    "    \n",
    "    plt.show()  \n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%zipline --bundle quantopian-quandl --start 2015-1-2 --end 2017-12-31 -o backtest.pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import datetime\n",
    "import back_testing as bt\n",
    "\n",
    "#API Keys stored in alpha_vantage.txt file\n",
    "API_KEY = open('alphavantage_api_keys.txt').read()\n",
    "\n",
    "#Choose symbol\n",
    "symbol = 'SQ'\n",
    "\n",
    "#Get request\n",
    "r = requests.get(f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&outputsize=full&apikey={API_KEY}').json()\n",
    "\n",
    "#Data Cleaning\n",
    "metadata = r['Meta Data']\n",
    "df = pd.DataFrame(r['Time Series (Daily)'], dtype=float).transpose()\n",
    "\n",
    "#Reverse index\n",
    "df = df.reindex(index=df.index[::-1])\n",
    "df.reset_index(level=0, inplace=True)\n",
    "\n",
    "#Rename cols\n",
    "df = df.rename({\n",
    "    'index': 'date',\n",
    "    '1. open': 'open',\n",
    "    '2. high': 'high',\n",
    "    '3. low': 'low',\n",
    "    '4. close': 'close',\n",
    "    '5. adjusted close': 'adjusted_close',\n",
    "    '6. volume': 'volume',\n",
    "    '7. dividend amount': 'dividend_amount',\n",
    "    '8. split coefficient': 'split_coefficient'\n",
    "},axis=1)\n",
    "\n",
    "#Convert date col to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "df = df[((df['date'] >= pd.to_datetime('2019-01-01')) & (df['date'] <= pd.to_datetime('2021-06-01')))]\n",
    "\n",
    "#Short moving averages (5-20 periods) are best suited for short-term trends and trading\n",
    "df['ma_200'] = df['close'].rolling(200).mean()\n",
    "\n",
    "#plot for validation\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.plot(df['date'], df['close'], label = 'Close')\n",
    "plt.plot(df['date'], df['ma_200'], label = '200 ma')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "period = 20\n",
    "\n",
    "#Calculate 20-ma and std dev\n",
    "df['ma_20'] = df['close'].rolling(period).mean()\n",
    "df['std'] = df['close'].rolling(period).std()\n",
    "\n",
    "#Calculate bollinger bands using std dev\n",
    "df['upper_bollinger'] = df['ma_20'] + (2 * df['std'])\n",
    "df['lower_bollinger'] = df['ma_20'] - (2 * df['std'])\n",
    "\n",
    "#plot for validation\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "x_axis = df['date']\n",
    "\n",
    "plt.plot(x_axis, df['close'], label = 'Close')\n",
    "plt.plot(df['date'], df['ma_20'], label = '20 ma')\n",
    "plt.fill_between(x_axis, df['upper_bollinger'], df['lower_bollinger'], label = 'Bollinger Bands', color='lightgrey')\n",
    "\n",
    "plt.title('SQ Price Chart', fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def gain(value):\n",
    "    if value < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "def loss(value):\n",
    "    if value > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return abs(value)\n",
    "    \n",
    "period = 6\n",
    "\n",
    "#Calculate price delta\n",
    "df['delta'] = df['close'].diff()\n",
    "\n",
    "#Classify delta into gain & loss\n",
    "df['gain'] = df['delta'].apply(lambda x:gain(x))\n",
    "df['loss'] = df['delta'].apply(lambda x:loss(x))\n",
    "\n",
    "#Calculate ema \n",
    "df['ema_gain'] = df['gain'].ewm(period).mean()\n",
    "df['ema_loss'] = df['loss'].ewm(period).mean()\n",
    "\n",
    "#Calculate RSI\n",
    "df['rs'] = df['ema_gain']/df['ema_loss']\n",
    "df['rsi'] = df['rs'].apply(lambda x: 100 - (100/(x+1)))\n",
    "\n",
    "\n",
    "#plot for validation\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "x_axis = df['date']\n",
    "\n",
    "plt.plot(x_axis, df['rsi'])\n",
    "plt.axhline(30, c= (.5,.5,.5), ls='--')\n",
    "plt.axhline(70, c= (.5,.5,.5), ls='--')\n",
    "\n",
    "plt.title('RSI (6)', fontweight=\"bold\")\n",
    "plt.ylim([0, 100])\n",
    "\n",
    "#buy signal\n",
    "df['signal'] = np.where(\n",
    "    (df['rsi'] < 30) &\n",
    "    (df['close'] < df['lower_bollinger']), 1, np.nan)\n",
    "\n",
    "#sell signal\n",
    "df['signal'] = np.where(\n",
    "    (df['rsi'] > 70) & \n",
    "    (df['close'] > df['upper_bollinger']), -1, df['signal'])\n",
    "\n",
    "#buy/sell next trading day\n",
    "df['signal'] = df['signal'].shift()\n",
    "df['signal'] = df['signal'].fillna(0)\n",
    "\n",
    "bt.backtest_dataframe(df)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.plot(df['date'], df['close'])\n",
    "plt.scatter(df[(df['signal'] == 1)]['buy_date'], df[(df['signal'] == 1)]['close'], label = 'Buy', marker='^', c='g')\n",
    "plt.scatter(df[(df['signal'] == -1)]['sell_date'], df[(df['signal'] == -1)]['close'], label = 'Sell', marker='v', c='r')\n",
    "\n",
    "plt.title('Price Chart & Historical Trades', fontweight=\"bold\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# future coding part\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "dfr = pd.DataFrame()\n",
    "\n",
    "df = yf.Ticker('MSFT').history(start=one, end=today)\n",
    "df = df.replace(np.nan, np.mean(df))\n",
    "dfr = (df/df.shift(1)-1)*100\n",
    "\n",
    "dfrc = pd.DataFrame()\n",
    "dfrc = dfr.dropna()\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "PC2 = pca.fit_transform(data_RC)\n",
    "f_1 = PC2[:,0]\n",
    "f_2 = PC2[:,1]\n",
    "plt.scatter(f_1,f_2,alpha=.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "proj1 = pca.components_.T[:,0]\n",
    "proj2 = pca.components_.T[:,1]\n",
    "indexproj1_max = np.argmax(proj1)\n",
    "indexproj2_max = np.argmax(proj2)\n",
    "nameproj1_max\n",
    "nameproj2_ax\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(proj1,proj2,alpha=.5)\n",
    "ax.scatter(proj1[indexproj1_max], proj2[indexproj1_max])\n",
    "ax.annotate(nameproj1_x,(proj1[indexproj1_max],proj2[indexproj1_max]))\n",
    "\n",
    "ax.scatter(proj1[indexproj2_max], proj2[indexproj2_max])\n",
    "ax.annotate(nameproj2_x,(proj1[indexproj2_max],proj2[indexproj2_max]))\n",
    "\n",
    "plt.xlabel('beta to factor 1')\n",
    "plt.ylabel('beta to factor 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataF = yf.download(\"EURUSD=X\", start=\"2022-12-30\", end=\"2023-01-30\", interval='15m')\n",
    "\n",
    "def signal_generator(df):\n",
    "    open = df.Open.iloc[-1]\n",
    "    close = df.Close.iloc[-1]\n",
    "    previous_open = df.Open.iloc[-2]\n",
    "    previous_close = df.Close.iloc[-2]\n",
    "    \n",
    "    # Bearish Pattern\n",
    "    if (open>close and \n",
    "    previous_open<previous_close and \n",
    "    close<previous_open and\n",
    "    open>=previous_close):\n",
    "        return 1\n",
    "\n",
    "    # Bullish Pattern\n",
    "    elif (open<close and \n",
    "        previous_open>previous_close and \n",
    "        close>previous_open and\n",
    "        open<=previous_close):\n",
    "        return 2\n",
    "    \n",
    "    # No clear pattern\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "signal = []\n",
    "signal.append(0)\n",
    "for i in range(1,len(dataF)):\n",
    "    df = dataF[i-1:i+1]\n",
    "    signal.append(signal_generator(df))\n",
    "#signal_generator(data)\n",
    "dataF[\"signal\"] = signal\n",
    "\n",
    "dataF.signal.value_counts()\n",
    "\n",
    "df = yf.Ticker('MSFT').history(start=one, end=today)\n",
    "#print(df)\n",
    "#Check if any zero volumes are available\n",
    "indexZeros = df[ df['Volume'] == 0 ].index\n",
    "\n",
    "df.drop(indexZeros , inplace=True)\n",
    "df.loc[(df[\"Volume\"] == 0 )]\n",
    "df.isna().sum()\n",
    "\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "#df.ta.indicators()\n",
    "#help(ta.atr)\n",
    "df['ATR'] = df.ta.atr(length=20)\n",
    "df['RSI'] = df.ta.rsi()\n",
    "df['Average'] = df.ta.midprice(length=1) #midprice\n",
    "df['MA40'] = df.ta.sma(length=40)\n",
    "df['MA80'] = df.ta.sma(length=80)\n",
    "df['MA160'] = df.ta.sma(length=160)\n",
    "\n",
    "from scipy.stats import linregress\n",
    "def get_slope(array):\n",
    "    y = np.array(array)\n",
    "    x = np.arange(len(y))\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "    return slope\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "backrollingN = 6\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df['slopeMA40'] = df['MA40'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['slopeMA80'] = df['MA80'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['slopeMA160'] = df['MA160'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['AverageSlope'] = df['Average'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['RSISlope'] = df['RSI'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "\n",
    "#Target flexible way\n",
    "pipdiff = 500*1e-5 #for TP\n",
    "SLTPRatio = 2 #pipdiff/Ratio gives SL\n",
    "\n",
    "def mytarget(barsupfront, df1):\n",
    "    length = len(df1)\n",
    "    high = list(df1['High'])\n",
    "    low = list(df1['Low'])\n",
    "    close = list(df1['Close'])\n",
    "    open = list(df1['Open'])\n",
    "    trendcat = [None] * length\n",
    "    \n",
    "    for line in range (0,length-barsupfront-2):\n",
    "        valueOpenLow = 0\n",
    "        valueOpenHigh = 0\n",
    "        for i in range(1,barsupfront+2):\n",
    "            value1 = open[line+1]-low[line+i]\n",
    "            value2 = open[line+1]-high[line+i]\n",
    "            valueOpenLow = max(value1, valueOpenLow)\n",
    "            valueOpenHigh = min(value2, valueOpenHigh)\n",
    "\n",
    "            if ( (valueOpenLow >= pipdiff) and (-valueOpenHigh <= (pipdiff/SLTPRatio)) ):\n",
    "                trendcat[line] = 1 #-1 downtrend\n",
    "                break\n",
    "            elif ( (valueOpenLow <= (pipdiff/SLTPRatio)) and (-valueOpenHigh >= pipdiff) ):\n",
    "                trendcat[line] = 2 # uptrend\n",
    "                break\n",
    "            else:\n",
    "                trendcat[line] = 0 # no clear trend\n",
    "            \n",
    "    return trendcat\n",
    "\n",
    "df['mytarget'] = mytarget(16, df)\n",
    "df.head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (15,20))\n",
    "ax = fig.gca()\n",
    "df_model= df[['Volume', 'ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', 'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope', 'mytarget']] \n",
    "df_model.hist(ax = ax)\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "df_up=df.RSI[ df['mytarget'] == 2 ]\n",
    "df_down=df.RSI[ df['mytarget'] == 1 ]\n",
    "df_unclear=df.RSI[ df['mytarget'] == 0 ]\n",
    "pyplot.hist(df_unclear, bins=100, alpha=0.5, label='unclear')\n",
    "pyplot.hist(df_down, bins=100, alpha=0.5, label='down')\n",
    "pyplot.hist(df_up, bins=100, alpha=0.5, label='up')\n",
    "\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n",
    "\n",
    "df_model=df_model.dropna()\n",
    "\n",
    "attributes=['ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', 'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "X = df_model[attributes]\n",
    "y = df_model[\"mytarget\"]\n",
    "\n",
    "print(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=55, weights='uniform', algorithm='kd_tree', leaf_size=30, p=1, metric='minkowski', metric_params=None, n_jobs=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy train: %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy test: %.2f%%\" % (accuracy_test * 100.0))\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# save your ML model to disk\n",
    "filename = 'mymodel1.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "#load the model from disk\n",
    "loaded_model = joblib.load('mymodel1.sav')\n",
    "\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "import json\n",
    "from oandapyV20 import API\n",
    "import oandapyV20.endpoints.orders as orders\n",
    "from oandapyV20.contrib.requests import MarketOrderRequest\n",
    "from oanda_candles import Pair, Gran, CandleCollector\n",
    "from oandapyV20.contrib.requests import TakeProfitDetails, StopLossDetails\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ModelPrediction = 0\n",
    "def some_job():\n",
    "    access_token=\"INSERT TOKEN HERE, YOU GET IT FROM YOUR OANDA ACCOUNT\"\n",
    "    collector = CandleCollector(access_token, Pair.USD_CHF, Gran.H4)\n",
    "    candles = collector.grab(2*161)\n",
    "\n",
    "    dfstream = pd.DataFrame(columns=['Open','Close','High','Low'])\n",
    "    i=0\n",
    "    for candle in candles:\n",
    "        dfstream.loc[i, ['Open']] = float(str(candle.bid.o))\n",
    "        dfstream.loc[i, ['Close']] = float(str(candle.bid.c))\n",
    "        dfstream.loc[i, ['High']] = float(str(candle.bid.h))\n",
    "        dfstream.loc[i, ['Low']] = float(str(candle.bid.l))\n",
    "        i=i+1\n",
    "\n",
    "    dfstream['Open'] = dfstream['Open'].astype(float)\n",
    "    dfstream['Close'] = dfstream['Close'].astype(float)\n",
    "    dfstream['High'] = dfstream['High'].astype(float)\n",
    "    dfstream['Low'] = dfstream['Low'].astype(float)\n",
    "\n",
    "    #dfstream['Average'] = (dfstream['High']+dfstream['Low'])/2\n",
    "    #dfstream['MA40'] = dfstream['Open'].rolling(window=40).mean()\n",
    "    #dfstream['MA80'] = dfstream['Open'].rolling(window=80).mean()\n",
    "    #dfstream['MA160'] = dfstream['Open'].rolling(window=160).mean()\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas_ta as ta\n",
    "    #attributes=['ATR', 'RSI', 'Average', \n",
    "    #'MA40', 'MA80', 'MA160', 'slopeMA40', \n",
    "    #'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "    dfstream['ATR'] = dfstream.ta.atr(length=20)\n",
    "    dfstream['RSI'] = dfstream.ta.rsi()\n",
    "    dfstream['Average'] = dfstream.ta.midprice(length=1) #midprice\n",
    "    dfstream['MA40'] = dfstream.ta.sma(length=40)\n",
    "    dfstream['MA80'] = dfstream.ta.sma(length=80)\n",
    "    dfstream['MA160'] = dfstream.ta.sma(length=160)\n",
    "\n",
    "#from scipy.stats import linregress\n",
    "#def get_slope(array):\n",
    "#    y = np.array(array)\n",
    "#    x = np.arange(len(y))\n",
    "#    slope, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "#    return slope\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    backrollingN = 6\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    dfstream['slopeMA40'] = dfstream['MA40'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['slopeMA80'] = dfstream['MA80'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['slopeMA160'] = dfstream['MA160'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['AverageSlope'] = dfstream['Average'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['RSISlope'] = dfstream['RSI'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "\n",
    "    #________________________________________________________________________________________________\n",
    "    X_stream = dfstream.iloc[[320]]# !!! Index takes last CLOSED candle\n",
    "    #attributes=['ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', \n",
    "    #'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "    X_model = X_stream[attributes]\n",
    "    \n",
    "    # Apply the model for predictions\n",
    "    ModelPrediction = loaded_model.predict(X_model)\n",
    "  \n",
    "    msg = str(ModelPrediction) # 0 no clear trend, 1 downtrend, 2 uptrend\n",
    "    #------------------------------------\n",
    "    # send email with \n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.ehlo()\n",
    "    server.login(gmail_user, gmail_password)\n",
    "    server.sendmail(sent_from, to, msg)\n",
    "    server.close()\n",
    "    #________________________________________________________________________________________________\n",
    "    \n",
    "    \n",
    "    # EXECUTING ORDERS\n",
    "    accountID = \"1432-432-0000\" #use your account ID\n",
    "    client = API(access_token)\n",
    "\n",
    "    candles = collector.grab(1)\n",
    "#    for candle in candles:\n",
    "#        print(candle.bid.o)\n",
    "#        print(candle.bid.c)\n",
    "    \n",
    "    pipdiff = 500*1e-5 #for TP\n",
    "    SLTPRatio = 2 #pipdiff/Ratio gives SL\n",
    "    \n",
    "    TPBuy = float(str(candle.bid.o))+pipdiff\n",
    "    SLBuy = float(str(candle.bid.o))-(pipdiff/SLTPRatio)\n",
    "    TPSell = float(str(candle.bid.o))-pipdiff\n",
    "    SLSell = float(str(candle.bid.o))+(pipdiff/SLTPRatio)\n",
    "    \n",
    "    #Sell\n",
    "    if ModelPrediction == 1:\n",
    "        mo = MarketOrderRequest(instrument=\"USD_CHF\", units=-1000, takeProfitOnFill=TakeProfitDetails(price=TPSell).data, stopLossOnFill=StopLossDetails(price=SLSell).data)\n",
    "        r = orders.OrderCreate(accountID, data=mo.data)\n",
    "        rv = client.request(r)\n",
    "        print(rv)\n",
    "    #Buy\n",
    "    elif ModelPrediction == 2:\n",
    "        mo = MarketOrderRequest(instrument=\"USD_CHF\", units=1000, takeProfitOnFill=TakeProfitDetails(price=TPBuy).data, stopLossOnFill=StopLossDetails(price=SLBuy).data)\n",
    "        r = orders.OrderCreate(accountID, data=mo.data)\n",
    "        rv = client.request(r)\n",
    "        print(rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28d3eb-ef2f-4a1f-a1cf-46b698d201ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.DataFrame()\n",
    "prices['BTC-USD'] = TMRW.DATA.data('BTC-USD','2014-01-01','2024-05-17')['Close']\n",
    "prices['ETH-USD'] = TMRW.DATA.data('ETH-USD','2014-01-01','2024-05-17')['Close']\n",
    "returns = prices.pct_change()#.dropna()\n",
    "plt.plot(prices['BTC-USD'].index, prices['BTC-USD'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(prices['ETH-USD'].index, prices['ETH-USD'])\n",
    "plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t = returns[returns.index >= '2024-01-01'].head(100)\n",
    "\n",
    "t1 = t['BTC-USD']\n",
    "t2 = t['ETH-USD']\n",
    "\n",
    "plt.plot(t1.index, t1)\n",
    "print(\"mean of interval 1:\", np.mean(t1),\"mean of interval 2:\", np.mean(t2))\n",
    "plt.plot(t2.index, t2)\n",
    "plt.show()\n",
    "\n",
    "print(ttest_ind(t1,t2)[1] > 0.50, ttest_ind(t1,t2)[1])\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "t = returns[returns.index >= '2018-07-01'].head(100)\n",
    "\n",
    "t1 = t['BTC-USD']\n",
    "t2 = t['ETH-USD']\n",
    "\n",
    "plt.plot(t1.index, t1)\n",
    "print(\"mean of interval 1:\", np.mean(t1),\"mean of interval 2:\", np.mean(t2))\n",
    "plt.plot(t2.index, t2)\n",
    "plt.show()\n",
    "\n",
    "print(ttest_ind(t1,t2)[1] > 0.50, ttest_ind(t1,t2)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfad62-618c-4c04-ad50-638d21d7f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, mstats\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import edge\n",
    "import edge.edge_mean_reversion as emr\n",
    "import edge.edge_risk_kit as erk\n",
    "import TMRW\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import yfinance as yf\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) #today\n",
    "m1 = datetime(today.year,today.month-1,today.day)\n",
    "m3 = datetime(today.year,today.month-3,today.day)\n",
    "m6 = datetime(today.year,today.month-5,today.day)\n",
    "one = datetime(today.year-1,today.month,today.day) #one year ago\n",
    "three = datetime(today.year-3,today.month,today.day) #one year ago\n",
    "five = datetime(today.year-5,today.month,today.day) #one year ago\n",
    "\n",
    "CUR = pd.read_excel(open('E:/Investering/Currencies.xlsx', 'rb'),sheet_name='Currencies')\n",
    "CUR = list(CUR[CUR['BSYMBOL'].notnull()]['SYMBOL'])\n",
    "lste = CUR\n",
    "fee = 0.000715\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy as sa\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import yfinance as yf\n",
    "from scipy import integrate\n",
    "import random\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import TMRW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "# from tqdm import tnrange, notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Import the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.dates import MonthLocator\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import edge\n",
    "\n",
    "import edge.edge_mean_reversion as a\n",
    "import edge.edge_risk_kit as erk\n",
    "\n",
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from matplotlib.dates import MonthLocator\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1aa548-156c-4f83-a755-87d1a8635bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_table(data, typ = \"above close\", length = \"3\", probsa = True):\n",
    "    \n",
    "    if typ == \"under close\":\n",
    "        data = data[data['MA21_ind'] == 1] # ma21 is below close\n",
    "    elif typ == \"above close\":\n",
    "        data = data[data['MA21_ind'] == -1] # ma21 is above close\n",
    "\n",
    "    if length == \"3\":\n",
    "        indx = pd.DataFrame(data['UD3indicator'].unique()).sort_values(by = 0, ascending=False)\n",
    "        indx = indx.reset_index(drop = True)\n",
    "        \n",
    "        lst = []\n",
    "        for i in list(indx[0]):\n",
    "\n",
    "            a = data[data['UD3indicator'] == i]\n",
    "            b = a[a['velocity'] >= 0]\n",
    "            c = a[a['velocity'] < 0]\n",
    "            lst.append([len(b),len(c), len(a)])\n",
    "        \n",
    "        probs = pd.DataFrame(lst, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "    elif length == \"5\":\n",
    "        indx =pd.DataFrame(data['UD5indicator'].unique()).sort_values(by = 0, ascending=False)\n",
    "        indx = indx.reset_index(drop = True)\n",
    "        probs = pd.DataFrame(0.0, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "        lst = []\n",
    "        for i in list(indx[0]):\n",
    "\n",
    "            a = data[data['UD5indicator'] == i]\n",
    "            b = a[a['velocity'] >= 0]\n",
    "            c = a[a['velocity'] < 0]\n",
    "            lst.append([len(b),len(c), len(a)])\n",
    "        \n",
    "        probs = pd.DataFrame(lst, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "        \n",
    "    if probsa == True:\n",
    "        probs['UP'] = probs['UP'] / probs['SUM']\n",
    "        probs['DOWN'] = probs['DOWN'] / probs['SUM']\n",
    "    return(probs)\n",
    "\n",
    "\n",
    "def volume_weighted_velocity(data, window = 21):\n",
    "\n",
    "    v_w_vel = 0\n",
    "    lst = np.zeros(window)\n",
    "    lst = list(lst)\n",
    "    for i in range(window, len(data)):\n",
    "        v_sum = sum(data2['Volume'][i-window:i])\n",
    "        v_w_vel = 0\n",
    "        for j in range(window):\n",
    "            v_w_vel = v_w_vel + data2['velocity'][i-j] * (data2['Volume'][i-j] / v_sum)\n",
    "\n",
    "        lst.append(v_w_vel)\n",
    "    return(lst)\n",
    "\n",
    "def volume_weighted_mean(data, window = 21):\n",
    "\n",
    "    v_w_vel = 0\n",
    "    lst = np.zeros(window)\n",
    "    lst = list(lst)\n",
    "    for i in range(window, len(data)):\n",
    "        v_sum = sum(data2['Volume'][i-window:i])\n",
    "        v_w_vel = 0\n",
    "        for j in range(window):\n",
    "            v_w_vel = v_w_vel + data2['Close'][i-j] * (data2['Volume'][i-j] / v_sum)\n",
    "\n",
    "        lst.append(v_w_vel)\n",
    "    return(lst)\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.stats import norm, mstats\n",
    "\n",
    "def markov_table(data, typ = \"above close\", length = \"3\", probsa = True):\n",
    "    \n",
    "    if typ == \"under close\":\n",
    "        data = data[data['MA21_ind'] == 1] # ma21 is below close\n",
    "    elif typ == \"above close\":\n",
    "        data = data[data['MA21_ind'] == -1] # ma21 is above close\n",
    "\n",
    "    if length == \"3\":\n",
    "        indx = pd.DataFrame(data['UD3indicator'].unique()).sort_values(by = 0, ascending=False)\n",
    "        indx = indx.reset_index(drop = True)\n",
    "        \n",
    "        lst = []\n",
    "        for i in list(indx[0]):\n",
    "\n",
    "            a = data[data['UD3indicator'] == i]\n",
    "            b = a[a['velocity'] >= 0]\n",
    "            c = a[a['velocity'] < 0]\n",
    "            lst.append([len(b),len(c), len(a)])\n",
    "        \n",
    "        probs = pd.DataFrame(lst, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "    elif length == \"5\":\n",
    "        indx =pd.DataFrame(data['UD5indicator'].unique()).sort_values(by = 0, ascending=False)\n",
    "        indx = indx.reset_index(drop = True)\n",
    "        probs = pd.DataFrame(0.0, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "        lst = []\n",
    "        for i in list(indx[0]):\n",
    "\n",
    "            a = data[data['UD5indicator'] == i]\n",
    "            b = a[a['velocity'] >= 0]\n",
    "            c = a[a['velocity'] < 0]\n",
    "            lst.append([len(b),len(c), len(a)])\n",
    "        \n",
    "        probs = pd.DataFrame(lst, index = indx[0], columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "        \n",
    "    if probsa == True:\n",
    "        probs['UP'] = probs['UP'] / probs['SUM']\n",
    "        probs['DOWN'] = probs['DOWN'] / probs['SUM']\n",
    "    return(probs)\n",
    "\n",
    "def bollinger_bands(data, length):\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, length):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(length,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-length:i])\n",
    "        std_lst[i] = np.std(data[i-length:i])\n",
    "        \n",
    "    up = mean_lst + 2.5 * std_lst\n",
    "    down = mean_lst - 2.5 * std_lst\n",
    "    \n",
    "    boll = pd.DataFrame()\n",
    "    boll['Upper'] = up\n",
    "    boll['Lower'] = down\n",
    "    return(boll)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_sizes(data, t):\n",
    "    \n",
    "    sizes = moving_averages(data, t)\n",
    "    for i in range(len(sizes)):\n",
    "        \n",
    "        for j in range(len(sizes.columns)):\n",
    "            \n",
    "            sizes.iloc[i,j] = data.iloc[i] - sizes.iloc[i,j] \n",
    "            \n",
    "    return(sizes)\n",
    "\n",
    "def mean_size_dist(data, t):\n",
    "    \n",
    "    ms = mean_sizes(data,t)\n",
    "    msd = pd.DataFrame(0, index = ['upper+2s', 'upper-m', 'upper-2s','lower+2s','lower-m','lower-2s'], columns = ms.columns)\n",
    "    for ti in range(1, len(ms.columns)):\n",
    "        \n",
    "        msp = ms[ti+1][ms[ti+1] > 0]\n",
    "        msn = ms[ti+1][ms[ti+1] < 0]\n",
    "        \n",
    "        msd[ti+1]['upper+2s'] = np.mean(msp) + 2 * np.std(msp)\n",
    "        msd[ti+1]['upper-m'] = np.mean(msp)\n",
    "        msd[ti+1]['upper-2s'] = min(msp)\n",
    "        \n",
    "        msd[ti+1]['lower+2s'] = max(msn)\n",
    "        msd[ti+1]['lower-m'] = np.mean(msn)\n",
    "        msd[ti+1]['lower-2s'] = np.mean(msn) - 2 * np.std(msn)\n",
    "        \n",
    "    return(msd)\n",
    "        \n",
    "def mean_times(data, t):\n",
    "    ms = mean_sizes(data, t)\n",
    "    mrt = pd.DataFrame(index = [\"mrt\"], columns = ms.columns)\n",
    "    for i in range(len(ms.columns)):\n",
    "        n = 0\n",
    "        for j in range(len(ms)):\n",
    "            if ms.iloc[j,i] > 0 and ms.iloc[j-1,i] < 0:\n",
    "                n = n + 1\n",
    "            elif ms.iloc[j,i] < 0 and ms.iloc[j-1,i] > 0:\n",
    "                n = n + 1\n",
    "            else:\n",
    "                n = n\n",
    "        mrt[i+1]['mrt'] = len(ms) / (n+1)\n",
    "    return(mrt)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8d30c-2626-4791-b30b-839abb7839b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "n = 100\n",
    "mat = []\n",
    "h = TMRW.DATA.data(lste[0],m6,today)['Close']\n",
    "for i in range(len(lste[0:n])):\n",
    "    df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)['Close'].shift(-1)\n",
    "\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df = df.fillna(0)\n",
    "lst = []\n",
    "for i in range(len(df.columns)):\n",
    "    if np.mean(df.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "\n",
    "df = df.drop(df.columns[lst], axis=1)\n",
    "\n",
    "df['BTC-USD'] = h\n",
    "\n",
    "for i in range(len(lste[0:n])):\n",
    "    a = np.cov(df.iloc[:,0],df.iloc[:,i])[1,0] # covariance mellem XYZ og BTC\n",
    "    b = np.corrcoef(df.iloc[:,0],df.iloc[:,i])[1,0] # Correlation mellem XYZ og BTC\n",
    "    c = variance_inflation_factor(df, i) # Collinearity mellem XYZ og BTC\n",
    "    mat.append([lste[i],a,b,c])\n",
    "    \n",
    "matrice1d = pd.DataFrame(mat, columns = [\"Symbol\", \"Covariance\", \"Correlation\", \"Collinearity\"])    \n",
    "m1d = matrice1d[matrice1d['Correlation'] > 0.70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c9047-9908-4593-ab34-63fe370a984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "n = 50\n",
    "for i in range(len(lste[0:n])):\n",
    "    df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)['Close']\n",
    "\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df = df.fillna(0)\n",
    "lst = []\n",
    "for i in range(len(df.columns)):\n",
    "    if np.mean(df.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "\n",
    "df = df.drop(df.columns[lst], axis=1)\n",
    "df = df.pct_change()#.dropna()\n",
    "df = df.iloc[1:len(df),:]\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df = df.fillna(0)\n",
    "\n",
    "lst = []\n",
    "slope_m = 0\n",
    "intercept_m = 0\n",
    "r_m = 0\n",
    "std_err_m = 0\n",
    "\n",
    "for i in range(1,len(df.columns)):\n",
    "    \n",
    "    plt.scatter(df.iloc[:,0], df.iloc[:,i], color = \"lightblue\", alpha = 0.1)\n",
    "    slope, intercept, r, p, std_err = stats.linregress(df.iloc[:,0], df.iloc[:,i])\n",
    "    def myfunc(x):\n",
    "        return slope * x + intercept\n",
    "    mymodel = list(map(myfunc, df.iloc[:,0]))\n",
    "    #print(r)\n",
    "    \n",
    "    if r > 0.5:\n",
    "        lst.append([df.columns[0], df.columns[i]])\n",
    "        slope_m = slope_m + slope\n",
    "        intercept_m = intercept_m + intercept\n",
    "        r_m = r_m + r**2\n",
    "        std_err_m = std_err_m + std_err\n",
    "        plt.plot(df.iloc[:,0], mymodel, color = \"red\", alpha = 0.1)\n",
    "        \n",
    "plt.ylim(-0.5,0.5)\n",
    "plt.show()\n",
    "\n",
    "slope_m = slope_m / len(lst)\n",
    "intercept_m = intercept_m / len(lst)\n",
    "r_m = r_m / len(lst)\n",
    "std_err_m = std_err_m / len(lst)\n",
    "\n",
    "print(slope_m)\n",
    "print(intercept_m)\n",
    "print(r_m)\n",
    "print(std_err_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e6541-1a2a-402e-beb4-8480a3ae18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstn = []\n",
    "for i in range(1, len(df.columns)):\n",
    "    if emr.perform_coint_test(df.iloc[:,0], df.iloc[:,i], False)[1] == False:\n",
    "        next\n",
    "    elif emr.perform_coint_test(df.iloc[:,0], df.iloc[:,i], False)[1] == True:\n",
    "        lstn.append([df.columns[0], df.columns[i]])\n",
    "        \n",
    "komplet = []\n",
    "lstm = list(m1d['Symbol'])\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    for j in range(len(lstn)):\n",
    "        if lst[i][1] == lstn[j][1]:\n",
    "            komplet.append(lst[i][1])\n",
    "            \n",
    "for i in range(len(lstm)):\n",
    "    for j in range(len(komplet)):\n",
    "        if lstm[i] == komplet[j]:\n",
    "            print(lstm[i])\n",
    "            \n",
    "            \n",
    "threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe38a81-3604-4f6a-9fcd-4c9246407c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(timeseries, window = 50, alpha = .00001, debug = False):\n",
    "    \"\"\"\n",
    "    Detects exponential trend\n",
    "\n",
    "    \"\"\"\n",
    "    n = window\n",
    "    idx = np.arange(1,n+1)\n",
    "    series = pandas.Series(list(timeseries[-n:]), index=idx)\n",
    "\n",
    "    X = zip(series[:-1],series[1:])\n",
    "    sqr_sum = pow(sum(series),2)\n",
    "    n_f = float(n)\n",
    "    r = (n_f * sum([x[0]*x[1] for x in X]) - sqr_sum + n_f*series[1]*series[n]) / \\\n",
    "        (n_f * sum([x**2 for x in series]) - sqr_sum)\n",
    "    \n",
    "    r_ = abs(r + 1./(n_f-1.)) / math.sqrt(n_f*(n_f-3)/(n_f+1)/pow((n_f-1),2))\n",
    "    u = norm.ppf(1-alpha/2)\n",
    "    \n",
    "    return abs(r_) > u, abs(r_)\n",
    "\n",
    "if autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == False:\n",
    "            h = \"Non-Trending\"\n",
    "elif autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == True:\n",
    "            h = \"Trending\"\n",
    "        \n",
    " & (data['autocorr_30'] >= threshold) & (data['value'].diff() > 0) \n",
    "    (data['autocorr_30'] >= threshold) & (data['value'].diff() < 0)\n",
    "\n",
    "returns = prices.pct_change()#.dropna()\n",
    "returns = returns.iloc[1:len(returns),:]\n",
    "returns.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "returns = returns.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e3d3d-a48f-452f-a91e-1848aeb2b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test['indicator1']\n",
    "X = test.loc[:, test.columns != 'indicator1']\n",
    "X = X.loc[:, X.columns != 'indicator3']\n",
    "X = X.loc[:, X.columns != 'indicator7']\n",
    "X = X.loc[:, X.columns != 'indicator21']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=1) # random sampling\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train, xgb_model=model.load_model('XGB.json'))\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print('****Train Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_train))\n",
    "print('****Test Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_test))\n",
    "\n",
    "#plot feature importance\n",
    "plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "#for at lave predictions skal modellen have alle de satte features\n",
    "\n",
    "#model.save_model('XGB.json')\n",
    "\n",
    "X2 = data2.loc[:, data.columns != 'indicator1']\n",
    "X2 = X2.loc[:, X2.columns != 'indicator3']\n",
    "X2 = X2.loc[:, X2.columns != 'indicator7']\n",
    "X2 = X2.loc[:, X2.columns != 'indicator21']\n",
    "Y2 = model.predict(X2)\n",
    "\n",
    "data = data.dropna()\n",
    "data.replace([np.inf, -np.inf, np.nan], 1, inplace=True)\n",
    "\n",
    "l = list(data['UD3indicator'].unique())\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(l)):\n",
    "        if data.iloc[i,37] == l[j]:\n",
    "            data.iloc[i,37] = j\n",
    "\n",
    "\n",
    "l2 = list(data['UD5indicator'].unique())\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(l2)):\n",
    "        if data.iloc[i,38] == l2[j]:\n",
    "            data.iloc[i,38] = j\n",
    "\n",
    "data['UD3indicator'] = data['UD3indicator'].astype(int)\n",
    "data['UD5indicator'] = data['UD5indicator'].astype(int)\n",
    "\n",
    "l = list(data2['UD3indicator'].unique())\n",
    "for i in range(len(data2)):\n",
    "    for j in range(len(l)):\n",
    "        if data2.iloc[i,37] == l[j]:\n",
    "            data2.iloc[i,37] = j\n",
    "\n",
    "\n",
    "l2 = list(data2['UD5indicator'].unique())\n",
    "for i in range(len(data2)):\n",
    "    for j in range(len(l2)):\n",
    "        if data2.iloc[i,38] == l2[j]:\n",
    "            data2.iloc[i,38] = j\n",
    "\n",
    "data2['UD3indicator'] = data2['UD3indicator'].astype(int)\n",
    "data2['UD5indicator'] = data2['UD5indicator'].astype(int)\n",
    "\n",
    "quotes = data\n",
    "# Unpack quotes\n",
    "close = np.array(quotes.Close)[1:]\n",
    "vel = np.array(quotes.velocity)[1:]\n",
    "vol_vel = np.array(quotes['volume velocity'])[1:]\n",
    "vol_acc = np.array(quotes['volume acceleration'])[1:]\n",
    "vol = np.array(quotes.Volume)[1:]\n",
    "acc = np.array(quotes.acceleration)[1:]\n",
    "RSI = np.array(quotes.RSI)[1:]\n",
    "MA3 = np.array(quotes.MA3_ind)[1:]\n",
    "STD3 = np.array(quotes.MSTD3)[1:]\n",
    "MA5 = np.array(quotes.MA5_ind)[1:]\n",
    "STD5 = np.array(quotes.MSTD5)[1:]\n",
    "MA10 = np.array(quotes.MA10_ind)[1:]\n",
    "STD10 = np.array(quotes.MSTD10)[1:]\n",
    "MA60 = np.array(quotes.MA60_ind)[1:]\n",
    "STD60 = np.array(quotes.MSTD60)[1:]\n",
    "ind = np.array(quotes['indicator1'])[1:]\n",
    "ind3 = np.array(quotes['indicator3'])[1:]\n",
    "ind7 = np.array(quotes['indicator7'])[1:]\n",
    "ind21 = np.array(quotes['indicator21'])[1:]\n",
    "UD3 = np.array(quotes['UD3indicator'])[1:]\n",
    "UD5 = np.array(quotes['UD5indicator'])[1:]\n",
    "\n",
    "# Pack diff and volume for training.\n",
    "X = np.column_stack([close, vel, acc, vol, vol_vel, vol_acc, RSI, MA3, STD3, MA10, STD10, MA60, STD60])\n",
    "\n",
    "# Make an HMM instance and execute fit\n",
    "model = GaussianHMM(n_components=6, covariance_type=\"diag\", n_iter=10000, random_state = 123).fit(X)\n",
    "\n",
    "# Predict the optimal sequence of internal hidden state\n",
    "hidden_states = model.predict(X)\n",
    "\n",
    "print(\"Transition matrix\")\n",
    "print(model.transmat_)\n",
    "print()\n",
    "\n",
    "print(\"Means and vars of each hidden state\")\n",
    "for i in range(model.n_components):\n",
    "    print(\"{0}th hidden state\".format(i))\n",
    "    print(\"mean = \", model.means_[i])\n",
    "    print(\"var = \", np.diag(model.covars_[i]))\n",
    "    print()\n",
    "    \n",
    "data = data.iloc[1:,:]\n",
    "\n",
    "Z = model.predict(X)\n",
    "states = pd.unique(Z)\n",
    "\n",
    "hidden_states = model.predict(X)\n",
    "data['States'] = hidden_states\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 10))\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"Close\"].iloc[want]\n",
    "    axes[0,0].plot(x, y, '.')\n",
    "axes[0,0].grid(True)\n",
    "axes[0,0].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,0].set_ylabel(\"Close\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"velocity\"].iloc[want]\n",
    "    axes[1,0].plot(x, y, '.')\n",
    "axes[1,0].grid(True)\n",
    "axes[1,0].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,0].set_ylabel(\"Velocity\", fontsize=16)\n",
    "axes[1,0].set_ylim(-0.5,0.5)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"Volume\"].iloc[want]\n",
    "    axes[2,0].plot(x, y, '.')\n",
    "axes[2,0].grid(True)\n",
    "axes[2,0].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[2,0].set_ylabel(\"Volume\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = (data['acceleration']/data['volume acceleration']).iloc[want]\n",
    "    axes[0,1].plot(x, y, '.')\n",
    "axes[0,1].grid(True)\n",
    "axes[0,1].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,1].set_ylabel(\"acceleration(price/vol)\", fontsize=16)\n",
    "axes[0,1].set_ylim(-15,15)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"MSTD5\"].iloc[want]\n",
    "    axes[1,1].plot(x, y, '.')\n",
    "axes[1,1].grid(True)\n",
    "axes[1,1].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,1].set_ylabel(\"MSTD5\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = (data['Close']-data[\"MA5\"]).iloc[want]\n",
    "    axes[2,1].plot(x, y, '.')\n",
    "axes[2,1].grid(True)\n",
    "axes[2,1].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[2,1].set_ylabel(\"close - MA5\", fontsize=16)\n",
    "\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = (data[\"velocity\"] / data[\"Volume\"]).iloc[want]\n",
    "    axes[0,2].plot(x, y, '.')\n",
    "axes[0,2].grid(True)\n",
    "axes[0,2].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,2].set_ylabel(\"Returns / volume\", fontsize=16)\n",
    "axes[0,2].set_ylim(-0.000000002,0.000000002)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"mean velocity 5\"].iloc[want]\n",
    "    axes[1,2].plot(x, y, '.')\n",
    "axes[1,2].grid(True)\n",
    "axes[1,2].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,2].set_ylabel(\"mean return 5\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"mean std 5\"].iloc[want]\n",
    "    axes[2,2].plot(x, y, '.')\n",
    "axes[2,2].grid(True)\n",
    "axes[2,2].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[2,2].set_ylabel(\"std of return 5\", fontsize=16)\n",
    "\n",
    "y = data['indicator1']\n",
    "X = data.loc[:, data.columns == \"mean velocity\"]\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "clf3 = clf3.fit(X, y)\n",
    "scores = cross_val_score(clf3, X, y, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "plt.plot(X,y, '.')\n",
    "plt.show()\n",
    "\n",
    "lst = [0,0,0,0,0]\n",
    "for i in range(5,len(data)):\n",
    "    \n",
    "    lst.append(np.mean(data.iloc[i-5:i,7]))\n",
    "data['mean velocity'] = lst\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"velocity\"].iloc[want]\n",
    "    axes[0,0].plot(x, y, '.')\n",
    "axes[0,0].grid(True)\n",
    "axes[0,0].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,0].set_ylabel(\"Velocity\", fontsize=16)\n",
    "axes[0,0].set_ylim(-0.1,0.1)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = (data['Close']-data[\"MA5\"]).iloc[want]\n",
    "    axes[0,1].plot(x, y, '.')\n",
    "axes[0,1].grid(True)\n",
    "axes[0,1].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,1].set_ylabel(\"Close-MA5\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = (data[\"velocity\"] / data[\"Volume\"]).iloc[want]\n",
    "    axes[0,2].plot(x, y, '.')\n",
    "axes[0,2].grid(True)\n",
    "axes[0,2].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[0,2].set_ylabel(\"Returns / volume\", fontsize=16)\n",
    "axes[0,2].set_ylim(-0.0000001,0.0000001)\n",
    "\n",
    "\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"Volume\"].iloc[want]\n",
    "    axes[1,0].plot(x, y, '.')\n",
    "axes[1,0].grid(True)\n",
    "axes[1,0].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,0].set_ylabel(\"Volume\", fontsize=16)\n",
    "axes[1,0].set_ylim(0,20000000)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"MSTD5\"].iloc[want]\n",
    "    axes[1,1].plot(x, y, '.')\n",
    "axes[1,1].grid(True)\n",
    "axes[1,1].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,1].set_ylabel(\"stds\", fontsize=16)\n",
    "\n",
    "for i in states:\n",
    "    want = (Z == i)\n",
    "    x = data.index[want]\n",
    "    y = data[\"mean velocity\"].iloc[want]\n",
    "    axes[1,2].plot(x, y, '.')\n",
    "axes[1,2].grid(True)\n",
    "axes[1,2].set_xlabel(\"datetime\", fontsize=16)\n",
    "axes[1,2].set_ylabel(\"mean velocity\", fontsize=16)\n",
    "\n",
    "want = (Z == 2)\n",
    "test = data.iloc[want]\n",
    "X2\n",
    "\n",
    "\n",
    "X2 = data2[['velocity']].iloc[-7]\n",
    "X2_ = np.array(list(X2)).reshape(1,-1)\n",
    "Y2_ = model.predict(X2_)\n",
    "Y2_\n",
    "\n",
    "X2\n",
    "\n",
    "X2 = data2[['velocity']].iloc[-8]\n",
    "X2_ = np.array(list(X2)).reshape(1,-1)\n",
    "Y2_ = model.predict(X2_)\n",
    "Y2_\n",
    "\n",
    "X2\n",
    "\n",
    "X2 = data2[['velocity']].iloc[-9]\n",
    "X2_ = np.array(list(X2)).reshape(1,-1)\n",
    "Y2_ = model.predict(X2_)\n",
    "Y2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ddb0f-5585-4efc-a8c6-00575ce46869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ea78b-ea60-4607-8ff9-64ab16695078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d78243-35b7-4cfd-b942-31e4bcec6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day+1)\n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -5) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "klines = client.get_historical_klines(\"ETHUSDT\", Client.KLINE_INTERVAL_15MINUTE, day7, todays)\n",
    "data = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "data.Open = data.Open.astype(float)\n",
    "data.High = data.High.astype(float)\n",
    "data.Low = data.Low.astype(float)\n",
    "data.Close = data.Close.astype(float)\n",
    "data.Volume = data.Volume.astype(float)\n",
    "data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "## finding min/max?\n",
    "\n",
    "data['diff'][i] = data['value'][i] - data['MA10'][i]\n",
    "data['z'][i] = data['diff'][i] / data['STD10'][i]\n",
    "\n",
    "if data['z'][i] < -1 and data['MA10'][i] > data['MA100'][i]:\n",
    "data['buy_signal'][i] = True\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from binance.enums import *\n",
    "#import time\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day+1)\n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -365) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "klines = client.get_historical_klines(\"TRXUSDT\", Client.KLINE_INTERVAL_1HOUR, day7, todays)\n",
    "data = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "data.Open = data.Open.astype(float)\n",
    "data.High = data.High.astype(float)\n",
    "data.Low = data.Low.astype(float)\n",
    "data.Close = data.Close.astype(float)\n",
    "data.Volume = data.Volume.astype(float)\n",
    "data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "data['MA5'] = TMRW.FINANCE.twa(data['value'], 5)[4] # 10 day\n",
    "data['MA5'][0] = 0\n",
    "data['MA10'] = TMRW.FINANCE.twa(data['value'], 20)[19] # 30 day\n",
    "data['MA10'][0] = 0\n",
    "data['MA30'] = TMRW.FINANCE.twa(data['value'], 40)[39] # 60 day\n",
    "data['MA30'][0] = 0\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'], 20)\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "data['short_signal'] = False\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "pos_neg = []\n",
    "for i in range(len(data)):\n",
    "    if data['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif data['returns'][i] < 0:\n",
    "        pos_neg.append(-1)\n",
    "\n",
    "data['pos_neg'] = pos_neg\n",
    "data['mean_return'] = data['returns'].rolling(window=10).mean().values.flatten()\n",
    "\n",
    "k = 60\n",
    "\n",
    "for i in range(60, len(data)):\n",
    "    \n",
    "    event = False\n",
    "    \n",
    "    if data['value'][i] < 0.95 * np.mean(data['value'][i-120:i]):\n",
    "        event = True\n",
    "    \n",
    "   # make sure there's no significant trend\n",
    "    elif np.mean(data['pos_neg'][i-240:i]) > 0.2:\n",
    "        next\n",
    "\n",
    "    elif np.mean(data['pos_neg'][i-1440:i]) < -0.1:\n",
    "        next\n",
    "\n",
    "    else:\n",
    "\n",
    "         # generate long/buy signal\n",
    "        #if i > 60 and (data['MA5'][i] < data['MA30'][i]) and (data['MA30'][i-1] > data['value'][i-1]):# and data['RSI'][i] < 1.05*min(data['RSI'][i-20:i]): #  (data['MA5'][i] > data['MA10'][i]) and \n",
    "            #data['buy_signal'][i] = True\n",
    "            #k = i\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if np.mean(data['returns'][i-72:i]) < 0 and np.mean(data['returns'][i-48:i]) > -0.02 and np.mean(data['value'][i-72:i]) < 1.05 * np.mean(data['value'][i-240:i]):\n",
    "            if np.mean(data['returns'][i-48:i]) > np.mean(data['returns'][i-480:i]):\n",
    "                if buy == False and event == False:\n",
    "                    #if np.mean(data['returns'][i-2:i]) > -0.0005:\n",
    "                        #if data['returns'][i] > data['returns'][i-1]:\n",
    "                    data['buy_signal'][i] = True\n",
    "                    k = i\n",
    "                    buy = True\n",
    "                    \n",
    "        #elif data['returns'][i] < 0 and np.mean(data['value'][i-2:i]) > 1.1 * np.mean(data['value'][i-5:i]):\n",
    "            #if buy == False:\n",
    "                #if np.mean(data['returns'][i-2:i]) > -0.0005:\n",
    "                    #if data['returns'][i] > data['returns'][i-1]:\n",
    "                #data['buy_signal'][i] = True\n",
    "                #k = i\n",
    "                #buy = True\n",
    "        \n",
    "        # generate short/sell signal\n",
    "\n",
    "        #if  i > 60 and (data['MA10'][i] > data['MA30'][i]) and (data['MA30'][i-1] < data['value'][i-1]):# and data['RSI'][i] > 0.95*max(data['RSI'][i-20:i]): # (data['MA5'][i] < data['MA10'][i]) and\n",
    "            #data['sell_signal'][i] = True\n",
    "        \n",
    "        #if data['value'][i] <= max(data['value'][k-1:i]) and np.mean(data['returns'][i-2:i]) < np.mean(data['returns'][i-6:i]) and np.mean(data['returns'][i-2:i]) < np.mean(data['returns'][i-10:i]):\n",
    "        if np.mean(data['returns'][i-24:i]) < 0 and i > k and buy == True: #and data['value'][i] > 1.01 * data['value'][k]:\n",
    "            if np.mean(data['returns'][k:i-48] > 0):\n",
    "                data['sell_signal'][i] = True\n",
    "                buy = False\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "long_signal_dates = data[data['buy_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "short_signal_dates = data[data['sell_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Max-Min Strategy')\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i]\n",
    "        val = val/data['Close'][i] * (1 - 0.000712)\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (val * data['value'][i])\n",
    "\n",
    "        \n",
    "val - 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef53f0-a970-4449-8974-d776d5a80be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Buy**\n",
    "if loc-min(RSI6)[-1] and RSI[i] > RSI[i-1] and price[i] < 5-SMA[i] and price[i] < 20-SMA[i]\n",
    "RSI[i-1] < 30\n",
    "\n",
    "extra indicator - distance to lower bollinger band is less than xxx? doesn't catch local minimum\n",
    "volume?\n",
    "\n",
    "and mean return isn't negative?\n",
    "pos_neg counter > 0?\n",
    "volume above average? - doesn't work\n",
    "\n",
    "\n",
    "**Sell**\n",
    "if loc-max(RSI6)[i] and RSI[i] > 80 and price[i] > 5-SMA[i] and price[i] > 20-SMA[i]\n",
    "\n",
    "extra indicator -  distance to upper bollinger band is less than xxx? - doesn't catch local maximum\n",
    "RSI > 60\n",
    "\n",
    "\n",
    "\n",
    "if price[i] > 1.05*price[k] and price[i] < 1.5-boll(20)-upper\n",
    "\n",
    "\n",
    "**Short-sell**\n",
    "if price > 2.5-boll(20)-upper and price < price[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c6714-764c-4b9e-9127-61330688730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "\n",
    "dataF = yf.download(\"EURUSD=X\", start=\"2022-11-19\", end=\"2023-01-16\", interval='5m')\n",
    "#dataF.iloc[:,:]\n",
    "\n",
    "grid_distance = 0.005\n",
    "midprice = 1.065\n",
    "\n",
    "def generate_grid(midprice, grid_distance, grid_range):\n",
    "    return (np.arange(midprice-grid_range, midprice+grid_range, grid_distance))\n",
    "\n",
    "grid = generate_grid(midprice=midprice, grid_distance=grid_distance, grid_range=0.1)\n",
    "grid\n",
    "\n",
    "\n",
    "signal = [0]*len(dataF)\n",
    "i=0\n",
    "for index, row in dataF.iterrows():\n",
    "    for p in grid:\n",
    "        if min(row.Low, row.High)<p and max(row.Low, row.High)>p:\n",
    "            signal[i]=1\n",
    "    i+=1\n",
    "\n",
    "dataF[\"signal\"]=signal\n",
    "dataF[dataF[\"signal\"]==1]\n",
    "\n",
    "dfpl = dataF[:].copy()\n",
    "def SIGNAL():\n",
    "    return dfpl.signal\n",
    "dfpl['ATR'] = ta.atr(high = dfpl.High, low = dfpl.Low, close = dfpl.Close, length = 16)\n",
    "dfpl.dropna(inplace=True)\n",
    "\n",
    "from backtesting import Strategy\n",
    "from backtesting import Backtest\n",
    "import backtesting\n",
    "\n",
    "class MyStrat(Strategy):\n",
    "    mysize = 50\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.signal1 = self.I(SIGNAL)\n",
    "\n",
    "    def next(self):\n",
    "        super().next()\n",
    "        slatr = 1.5*grid_distance #5*self.data.ATR[-1]\n",
    "        TPSLRatio = 0.5\n",
    "\n",
    "        if self.signal1==1 and len(self.trades)<=10000:             \n",
    "            sl1 = self.data.Close[-1] + slatr\n",
    "            tp1 = self.data.Close[-1] - slatr*TPSLRatio\n",
    "            self.sell(sl=sl1, tp=tp1, size=self.mysize)\n",
    "\n",
    "            sl1 = self.data.Close[-1] - slatr\n",
    "            tp1 = self.data.Close[-1] + slatr*TPSLRatio\n",
    "            self.buy(sl=sl1, tp=tp1, size=self.mysize)\n",
    "\n",
    "bt = Backtest(dfpl, MyStrat, cash=50, margin=1/100, hedging=True, exclusive_orders=False)\n",
    "stat = bt.run()\n",
    "stat\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv(\"EURUSD_Candlestick_1_D_BID_04.05.2003-21.01.2023.csv\")\n",
    "\n",
    "backcandles = 45\n",
    "\n",
    "def isPivot(candle, window):\n",
    "    \"\"\"\n",
    "    function that detects if a candle is a pivot/fractal point\n",
    "    args: candle index, window before and after candle to test if pivot\n",
    "    returns: 1 if pivot high, 2 if pivot low, 3 if both and 0 default\n",
    "    \"\"\"\n",
    "    if candle-window < 0 or candle+window >= len(df):\n",
    "        return 0\n",
    "    \n",
    "    pivotHigh = 1\n",
    "    pivotLow = 2\n",
    "    for i in range(candle-window, candle+window+1):\n",
    "        if df.iloc[candle].Low > df.iloc[i].Low:\n",
    "            pivotLow=0\n",
    "        if df.iloc[candle].High < df.iloc[i].High:\n",
    "            pivotHigh=0\n",
    "    if (pivotHigh and pivotLow):\n",
    "        return 3\n",
    "    elif pivotHigh:\n",
    "        return pivotHigh\n",
    "    elif pivotLow:\n",
    "        return pivotLow\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "window=3\n",
    "df['isPivot'] = df.apply(lambda x: isPivot(x.name,window), axis=1)\n",
    "\n",
    "def collect_channel(candle, backcandles, window):\n",
    "    localdf = df[candle-backcandles-window:candle-window]\n",
    "    #localdf['isPivot'] = localdf.apply(lambda x: isPivot(x.name,window), axis=1)\n",
    "    highs = localdf[localdf['isPivot']==1].High.values\n",
    "    idxhighs = localdf[localdf['isPivot']==1].High.index\n",
    "    lows = localdf[localdf['isPivot']==2].Low.values\n",
    "    idxlows = localdf[localdf['isPivot']==2].Low.index\n",
    "    \n",
    "    if len(lows)>=3 and len(highs)>=3:\n",
    "        sl_lows, interc_lows, r_value_l, _, _ = stats.linregress(idxlows,lows)\n",
    "        sl_highs, interc_highs, r_value_h, _, _ = stats.linregress(idxhighs,highs)\n",
    "    \n",
    "        return(sl_lows, interc_lows, sl_highs, interc_highs, r_value_l**2, r_value_h**2)\n",
    "    else:\n",
    "        return(0,0,0,0,0,0)\n",
    "\n",
    "df['Channel'] = [collect_channel(candle, backcandles, window) for candle in df.index]\n",
    "\n",
    "def isBreakOut(candle, backcandles, window):\n",
    "    if (candle-backcandles-window)<0:\n",
    "        return 0\n",
    "    \n",
    "    sl_lows, interc_lows, sl_highs, interc_highs, r_sq_l, r_sq_h = df.iloc[candle].Channel\n",
    "    \n",
    "    prev_idx = candle-1\n",
    "    prev_high = df.iloc[candle-1].High\n",
    "    prev_low = df.iloc[candle-1].Low\n",
    "    prev_close = df.iloc[candle-1].Close\n",
    "    \n",
    "    curr_idx = candle\n",
    "    curr_high = df.iloc[candle].High\n",
    "    curr_low = df.iloc[candle].Low\n",
    "    curr_close = df.iloc[candle].Close\n",
    "    curr_open = df.iloc[candle].Open\n",
    "\n",
    "    if ( prev_high > (sl_lows*prev_idx + interc_lows) and\n",
    "        prev_close < (sl_lows*prev_idx + interc_lows) and\n",
    "        curr_open < (sl_lows*curr_idx + interc_lows) and\n",
    "        curr_close < (sl_lows*prev_idx + interc_lows)): #and r_sq_l > 0.9\n",
    "        return 1\n",
    "    \n",
    "    elif ( prev_low < (sl_highs*prev_idx + interc_highs) and\n",
    "        prev_close > (sl_highs*prev_idx + interc_highs) and\n",
    "        curr_open > (sl_highs*curr_idx + interc_highs) and\n",
    "        curr_close > (sl_highs*prev_idx + interc_highs)): #and r_sq_h > 0.9\n",
    "        return 2\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df[\"isBreakOut\"] = [isBreakOut(candle, backcandles, window) for candle in df.index]\n",
    "\n",
    "def SIGNAL():\n",
    "    return df.isBreakOut\n",
    "\n",
    "from backtesting import Strategy\n",
    "from backtesting import Backtest\n",
    "import backtesting\n",
    "\n",
    "class BreakOut(Strategy):\n",
    "    initsize = 0.1\n",
    "    mysize = initsize\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.signal1 = self.I(SIGNAL)\n",
    "\n",
    "    def next(self):\n",
    "        super().next()\n",
    "        TPSLRatio = 1.2\n",
    "\n",
    "        if self.signal1==2 and len(self.trades)==0:   \n",
    "            sl1 = self.data.Low[-2]\n",
    "            tp1 = self.data.Close[-1] + abs(self.data.Close[-1]-sl1)*TPSLRatio\n",
    "            #tp2 = self.data.Close[-1] + abs(self.data.Close[-1]-sl1)*TPSLRatio/3\n",
    "            self.buy(sl=sl1, tp=tp1, size=self.mysize)\n",
    "            #self.buy(sl=sl1, tp=tp2, size=self.mysize)\n",
    "        \n",
    "        elif self.signal1==1 and len(self.trades)==0:         \n",
    "            sl1 = self.data.High[-2]\n",
    "            tp1 = self.data.Close[-1] - abs(sl1-self.data.Close[-1])*TPSLRatio\n",
    "            #tp2 = self.data.Close[-1] - abs(sl1-self.data.Close[-1])*TPSLRatio/3\n",
    "            self.sell(sl=sl1, tp=tp1, size=self.mysize)\n",
    "            #self.sell(sl=sl1, tp=tp2, size=self.mysize)\n",
    "\n",
    "bt = Backtest(df, BreakOut, cash=1000, margin=1/50, commission=.000)\n",
    "stat = bt.run()\n",
    "stat\n",
    "\n",
    "bt.plot()\n",
    "\n",
    "df =df.reset_index()\n",
    "\n",
    "df = df.drop(\"Gmt time\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a45c2e-e1d6-4902-9f8a-fbbd629cf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['velocity'] = TMRW.FINANCE.returns(data['Close'])\n",
    "data['acceleration'] = TMRW.FINANCE.acceleration(data['Close'])\n",
    "data['volume velocity'] = TMRW.FINANCE.returns(data['Volume'])\n",
    "data['volume acceleration'] = TMRW.FINANCE.acceleration(data['Volume'])\n",
    "\n",
    "\n",
    "for i in [3*24, 5*24, 10*24, 21*24, 35*24, 60*24, 120*24]:\n",
    "    txt = 'MA' + str(i)\n",
    "    txt_i = 'MA' + str(i) + '_ind'\n",
    "    txt2 = 'MSTD' + str(i)\n",
    "    txt_mv = 'MV' + str(i)\n",
    "    txt_std = 'MVSTD' + str(i)\n",
    "\n",
    "    _ma = [0]\n",
    "    _ma.extend(list(TMRW.FINANCE.twa(data['Close'], i).iloc[:, i-2]))\n",
    "\n",
    "    ma_ind = []\n",
    "    for j in range(len(_ma)):\n",
    "        if _ma[j] < data.iloc[j,3]:\n",
    "            ma_ind.append(1)  # moving average is above\n",
    "        elif _ma[j] >= data.iloc[j,3]:\n",
    "            ma_ind.append(-1) # moving average is below\n",
    "        else:\n",
    "            ma_ind.append(0) # unknown\n",
    "\n",
    "    std = [0]\n",
    "    std.extend(list(TMRW.FINANCE.twstd(data['Close'], i).iloc[:, i-2]))\n",
    "\n",
    "    data[txt] = list(_ma)\n",
    "    data[txt_i] = ma_ind\n",
    "    data[txt2] = list(std)\n",
    "    data[txt_mv] = TMRW.FINANCE.twa_r(data['velocity'], i)\n",
    "    data[txt_std] = TMRW.FINANCE.twstd_r(data['velocity'], i)\n",
    "\n",
    "# up down indicators\n",
    "data = tf.UD_ind(self.data)\n",
    "\n",
    "# trend detection\n",
    "data['PN20'] = 0\n",
    "data['PN50'] = 0\n",
    "data['PN100'] = 0\n",
    "data['PN200'] = 0\n",
    "\n",
    "for i in range(24, len(data)):\n",
    "    data['PN20'][i] = np.mean(data['pos_neg'][i-24:i]) # 1 day\n",
    "for i in range(120, len(data)):\n",
    "    data['PN50'][i] = np.mean(data['pos_neg'][i-120:i]) # 5 day\n",
    "for i in range(240, len(data)):\n",
    "    data['PN100'][i] = np.mean(data['pos_neg'][i-240:i]) # 10 day\n",
    "for i in range(480, len(data)):\n",
    "    data['PN200'][i] = np.mean(data['pos_neg'][i-480:i]) # 20 day\n",
    "\n",
    "# hidden states\n",
    "data['States'] = tf.hidden_states(data)\n",
    "\n",
    "\n",
    "lste = ['BTC-USD', 'ETC-USD']\n",
    "\n",
    "for k in range(len(lste)):\n",
    "    t = yf.Ticker(lste[k])\n",
    "    info = pd.DataFrame.from_dict(t.info.items())\n",
    "    info.index = info.iloc[:,0]\n",
    "    info = info.iloc[:,1]\n",
    "    TD = TMRW.DATA.data_object(t)\n",
    "    try:\n",
    "        TD.insert_data(t, \"1d\", '2014-01-01', '2024-05-30')\n",
    "        data = TD.data.copy()\n",
    "\n",
    "        lst = [\"\",\"\",\"\",\"\"]\n",
    "\n",
    "        for i in range(4,len(data)):\n",
    "\n",
    "            st = \"\"\n",
    "\n",
    "            if data.iloc[i-3, 7] < 0:\n",
    "                st = \"D\"\n",
    "            elif data.iloc[i-3, 7] >= 0:\n",
    "                st = \"U\"\n",
    "\n",
    "            if data.iloc[i-2, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-2, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-1, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-1, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            lst.append(st)\n",
    "\n",
    "        data['UD3indicator'] = lst\n",
    "\n",
    "        lst = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "        for i in range(5,len(data)):\n",
    "\n",
    "            st = \"\"\n",
    "\n",
    "            if data.iloc[i-5, 7] < 0:\n",
    "                st = \"D\"\n",
    "            elif data.iloc[i-5, 7] >= 0:\n",
    "                st = \"U\"\n",
    "\n",
    "            if data.iloc[i-4, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-4, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-3, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-3, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-2, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-2, 7] >= 0:\n",
    "                st = st +\"U\"\n",
    "\n",
    "            if data.iloc[i-1, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-1, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            lst.append(st)\n",
    "\n",
    "        data['UD5indicator'] = lst\n",
    "        \n",
    "        data['Trend'] = \"\"\n",
    "        for i in range(21, len(data)):\n",
    "            data['Trend'][i] = mk_test(data['Close'][i-21:i], 'full', window = 15, alpha = 0.1)[0]\n",
    "            \n",
    "        data['Trend2'] = \"\"\n",
    "        for i in range(15, len(data)):\n",
    "            t_ind = np.mean(data['velocity'][i-15:i]) * 100\n",
    "            if t_ind < -0.5:\n",
    "                data['Trend2'][i] = \"decreasing\"\n",
    "            elif t_ind > 0.5:\n",
    "                data['Trend2'][i] = \"increasing\"\n",
    "            else:\n",
    "                data['Trend2'][i] = \"no trend\"\n",
    "        \n",
    "        data2 = data.copy()\n",
    "        data = data.iloc[2:-21,:]\n",
    "        \n",
    "        data_up_trend = data[(data['Trend'] == \"increasing\") | (data['Trend2'] == \"increasing\")]\n",
    "        data_down_trend = data[(data['Trend'] == \"decreasing\") | (data['Trend2'] == \"decreasing\")]\n",
    "        data_no_trend = data[data['Trend2'] == \"no trend\"]    \n",
    "        \n",
    "        \n",
    "        data2['VWV21'] = volume_weighted_mean(data2, window = 21)\n",
    "        lst = []\n",
    "        for i in range(len(data2)):\n",
    "            if data2['VWV21'][i] <= data2['Close'][i]:\n",
    "                lst.append(-1)\n",
    "            elif data2['VWV21'][i] > data2['Close'][i]:\n",
    "                lst.append(1)\n",
    "\n",
    "        data2['VWV21_ind'] = lst \n",
    "\n",
    "        v = np.ones(len(data2))\n",
    "        a = volume_weighted_velocity(data2, window = 5)\n",
    "        for i in range(len(v)):\n",
    "            v[i] = v[i] + a[i]\n",
    "            v[i] = v[i] * data2['MA21'][i]\n",
    "\n",
    "        if k == 0:\n",
    "            ut_uc_3 = markov_table(data_up_trend, \"under close\", \"3\")\n",
    "            dt_uc_3 = markov_table(data_down_trend, \"under close\", \"3\")\n",
    "            nt_uc_3 = markov_table(data_no_trend, \"under close\", \"3\")\n",
    "\n",
    "            ut_ac_3 = markov_table(data_up_trend, \"above close\", \"3\")\n",
    "            dt_ac_3 = markov_table(data_down_trend, \"above close\", \"3\")\n",
    "            nt_ac_3 = markov_table(data_no_trend, \"above close\", \"3\")\n",
    "\n",
    "\n",
    "            ut_uc_5 = markov_table(data_up_trend, \"under close\", \"5\")\n",
    "            dt_uc_5 = markov_table(data_down_trend, \"under close\", \"5\")\n",
    "            nt_uc_5 = markov_table(data_no_trend, \"under close\", \"5\")\n",
    "\n",
    "            ut_ac_5 = markov_table(data_up_trend, \"above close\", \"5\")\n",
    "            dt_ac_5 = markov_table(data_down_trend, \"above close\", \"5\")\n",
    "            nt_ac_5 = markov_table(data_no_trend, \"above close\", \"5\")\n",
    "        else:\n",
    "            ut_uc_3 = (ut_uc_3 + markov_table(data_up_trend, \"under close\", \"3\"))/2\n",
    "            dt_uc_3 = (dt_uc_3 + markov_table(data_down_trend, \"under close\", \"3\"))/2\n",
    "            nt_uc_3 = (nt_uc_3 + markov_table(data_no_trend, \"under close\", \"3\"))/2\n",
    "            \n",
    "            ut_ac_3 = (ut_ac_3 + markov_table(data_up_trend, \"above close\", \"3\"))/2\n",
    "            dt_ac_3 = (dt_ac_3 + markov_table(data_down_trend, \"above close\", \"3\"))/2\n",
    "            nt_ac_3 = (nt_ac_3 + markov_table(data_no_trend, \"above close\", \"3\"))/2\n",
    "            \n",
    "            ut_uc_5 = (ut_uc_5 + markov_table(data_up_trend, \"under close\", \"5\"))/2\n",
    "            dt_uc_5 = (dt_uc_5 + markov_table(data_down_trend, \"under close\", \"5\"))/2\n",
    "            nt_uc_5 = (nt_uc_5 + markov_table(data_no_trend, \"under close\", \"5\"))/2\n",
    "            \n",
    "            ut_ac_5 = (ut_ac_5 + markov_table(data_up_trend, \"above close\", \"5\"))/2\n",
    "            dt_ac_5 = (dt_ac_5 + markov_table(data_down_trend, \"above close\", \"5\"))/2\n",
    "            nt_ac_5 = (nt_ac_5 + markov_table(data_no_trend, \"above close\", \"5\"))/2\n",
    "            \n",
    "\n",
    "    except:\n",
    "        next\n",
    "        \n",
    "if k == 0:\n",
    "\n",
    "    probs3dsum = pd.DataFrame(0.0, index = list(indx1[0]), columns = ['UP', 'DOWN', 'SUM'])        \n",
    "    probs5dsum = pd.DataFrame(0.0, index = list(indx2[0]), columns = ['UP', 'DOWN', 'SUM'])\n",
    "\n",
    "    # above moving average\n",
    "    probs3dsum_up = pd.DataFrame(0.0, index = list(indx1[0]), columns = ['UP', 'DOWN', 'SUM'])        \n",
    "    probs5dsum_up = pd.DataFrame(0.0, index = list(indx2[0]), columns = ['UP', 'DOWN', 'SUM'])\n",
    "\n",
    "    # below moving average\n",
    "    probs3dsum_down = pd.DataFrame(0.0, index = list(indx1[0]), columns = ['UP', 'DOWN', 'SUM'])        \n",
    "    probs5dsum_down = pd.DataFrame(0.0, index = list(indx2[0]), columns = ['UP', 'DOWN', 'SUM'])\n",
    "        \n",
    "if len(probs3d) <= len(probs3dsum):\n",
    "    length = len(probs3d)\n",
    "\n",
    "elif len(probs3d) > len(probs3dsum):\n",
    "    length = len(probs3dsum)\n",
    "\n",
    "for i in range(length):\n",
    "    probs3dsum.iloc[i,0] = probs3dsum.iloc[i,0] + probs3d.iloc[i,0]\n",
    "    probs3dsum.iloc[i,1] = probs3dsum.iloc[i,1] + probs3d.iloc[i,1]\n",
    "    probs3dsum.iloc[i,2] = probs3dsum.iloc[i,2] + probs3d.iloc[i,2]\n",
    "\n",
    "\n",
    "    probs3dsum_up.iloc[i,0] = probs3dsum_up.iloc[i,0] + probs3d_u.iloc[i,0]\n",
    "    probs3dsum_up.iloc[i,1] = probs3dsum_up.iloc[i,1] + probs3d_u.iloc[i,1]\n",
    "    probs3dsum_up.iloc[i,2] = probs3dsum_up.iloc[i,2] + probs3d_u.iloc[i,2]\n",
    "\n",
    "    probs3dsum_down.iloc[i,0] = probs3dsum_down.iloc[i,0] + probs3d_d.iloc[i,0]\n",
    "    probs3dsum_down.iloc[i,1] = probs3dsum_down.iloc[i,1] + probs3d_d.iloc[i,1]\n",
    "    probs3dsum_down.iloc[i,2] = probs3dsum_down.iloc[i,2] + probs3d_d.iloc[i,2]\n",
    "\n",
    "if len(probs5d) <= len(probs5dsum):\n",
    "    length = len(probs5d)\n",
    "\n",
    "elif len(probs5d) != len(probs5dsum):\n",
    "    length = len(probs5dsum)\n",
    "\n",
    "for i in range(length):\n",
    "    probs5dsum.iloc[i,0] = probs5dsum.iloc[i,0] + probs5d.iloc[i,0]\n",
    "    probs5dsum.iloc[i,1] = probs5dsum.iloc[i,1] + probs5d.iloc[i,1]\n",
    "    probs5dsum.iloc[i,2] = probs5dsum.iloc[i,2] + probs5d.iloc[i,2]\n",
    "\n",
    "    probs5dsum_up.iloc[i,0] = probs5dsum_up.iloc[i,0] + probs5d_u.iloc[i,0]\n",
    "    probs5dsum_up.iloc[i,1] = probs5dsum_up.iloc[i,1] + probs5d_u.iloc[i,1]\n",
    "    probs5dsum_up.iloc[i,2] = probs5dsum_up.iloc[i,2] + probs5d_u.iloc[i,2]\n",
    "\n",
    "    probs5dsum_down.iloc[i,0] = probs5dsum_down.iloc[i,0] + probs5d_d.iloc[i,0]\n",
    "    probs5dsum_down.iloc[i,1] = probs5dsum_down.iloc[i,1] + probs5d_d.iloc[i,1]\n",
    "    probs5dsum_down.iloc[i,2] = probs5dsum_down.iloc[i,2] + probs5d_d.iloc[i,2]\n",
    "    \n",
    "data2[data2['Trend2'] == \"decreasing\"]\n",
    "\n",
    "probs3dsum['UP'] = probs3dsum['UP'] / probs3dsum['SUM'] * 100\n",
    "probs3dsum['DOWN'] = probs3dsum['DOWN'] / probs3dsum['SUM'] * 100\n",
    "probs5dsum['UP'] = probs5dsum['UP'] / probs5dsum['SUM'] * 100\n",
    "probs5dsum['DOWN'] = probs5dsum['DOWN'] / probs5dsum['SUM'] * 100\n",
    "\n",
    "probs3d_u['UP'] = probs3d_u['UP'] / probs3d_u['SUM'] * 100\n",
    "probs3d_u['DOWN'] = probs3d_u['DOWN'] / probs3d_u['SUM'] * 100\n",
    "\n",
    "probs3d_d['UP'] = probs3d_d['UP'] / probs3d_d['SUM'] * 100\n",
    "probs3d_d['DOWN'] = probs3d_d['DOWN'] / probs3d_d['SUM'] * 100\n",
    "\n",
    "probs5d_u['UP'] = probs5d_u['UP'] / probs5d_u['SUM'] * 100\n",
    "probs5d_u['DOWN'] = probs5d_u['DOWN'] / probs5d_u['SUM'] * 100\n",
    "probs5d_d['UP'] = probs5d_d['UP'] / probs5d_d['SUM'] * 100\n",
    "probs5d_d['DOWN'] = probs5d_d['DOWN'] / probs5d_d['SUM'] * 100\n",
    "\n",
    "display(probs3d_u)\n",
    "display(probs3d_d)\n",
    "\n",
    "display(probs5d_u)\n",
    "display(probs5d_d)\n",
    "\n",
    "probs3dsum_up['UP'] = probs3dsum_up['UP'] / probs3dsum_up['SUM'] * 100\n",
    "probs3dsum_up['DOWN'] = probs3dsum_up['DOWN'] / probs3dsum_up['SUM'] * 100\n",
    "probs3dsum_down['UP'] = probs3dsum_down['UP'] / probs3dsum_down['SUM'] * 100\n",
    "probs3dsum_down['DOWN'] = probs3dsum_down['DOWN'] / probs3dsum_down['SUM'] * 100\n",
    "\n",
    "probs5dsum_up['UP'] = probs5dsum_up['UP'] / probs5dsum_up['SUM'] * 100\n",
    "probs5dsum_up['DOWN'] = probs5dsum_up['DOWN'] / probs5dsum_up['SUM'] * 100\n",
    "probs5dsum_down['UP'] = probs5dsum_down['UP'] / probs5dsum_down['SUM'] * 100\n",
    "probs5dsum_down['DOWN'] = probs5dsum_down['DOWN'] / probs5dsum_down['SUM'] * 100\n",
    "\n",
    "display(probs3dsum_up)\n",
    "display(probs3dsum_down)\n",
    "\n",
    "display(probs5dsum_up)\n",
    "display(probs5dsum_down)\n",
    "\n",
    "lste = ['SHEL']\n",
    "\n",
    "for k in range(len(lste)):\n",
    "    t = yf.Ticker(lste[k])\n",
    "    info = pd.DataFrame.from_dict(t.info.items())\n",
    "    info.index = info.iloc[:,0]\n",
    "    info = info.iloc[:,1]\n",
    "    TD = TMRW.DATA.data_object(t)\n",
    "    try:\n",
    "        TD.insert_data(t, \"1d\", '2000-01-01', '2024-05-17')\n",
    "        data = TD.data.copy()\n",
    "\n",
    "        lst = [\"\",\"\",\"\",\"\"]\n",
    "\n",
    "        for i in range(4,len(data)):\n",
    "\n",
    "            st = \"\"\n",
    "\n",
    "            if data.iloc[i-3, 7] < 0:\n",
    "                st = \"D\"\n",
    "            elif data.iloc[i-3, 7] >= 0:\n",
    "                st = \"U\"\n",
    "\n",
    "            if data.iloc[i-2, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-2, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-1, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-1, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            lst.append(st)\n",
    "\n",
    "        data['UD3indicator'] = lst\n",
    "\n",
    "        lst = [\"\", \"\", \"\", \"\", \"\"]\n",
    "\n",
    "        for i in range(5,len(data)):\n",
    "\n",
    "            st = \"\"\n",
    "\n",
    "            if data.iloc[i-5, 7] < 0:\n",
    "                st = \"D\"\n",
    "            elif data.iloc[i-5, 7] >= 0:\n",
    "                st = \"U\"\n",
    "\n",
    "            if data.iloc[i-4, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-4, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-3, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-3, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            if data.iloc[i-2, 7] < 0:\n",
    "                st = st +\"D\"\n",
    "            elif data.iloc[i-2, 7] >= 0:\n",
    "                st = st +\"U\"\n",
    "\n",
    "            if data.iloc[i-1, 7] < 0:\n",
    "                st = st + \"D\"\n",
    "            elif data.iloc[i-1, 7] >= 0:\n",
    "                st = st + \"U\"\n",
    "\n",
    "            lst.append(st)\n",
    "\n",
    "        data['UD5indicator'] = lst\n",
    "        data2 = data.copy()\n",
    "        data = data.iloc[2:-21,:]\n",
    "        \n",
    "    except:\n",
    "        next\n",
    "        \n",
    "    data = data.dropna()\n",
    "    data.replace([np.inf, -np.inf, np.nan], 1, inplace=True)\n",
    "\n",
    "    l = list(data['UD3indicator'].unique())\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(l)):\n",
    "            if data.iloc[i,37] == l[j]:\n",
    "                data.iloc[i,37] = j\n",
    "\n",
    "\n",
    "    l2 = list(data['UD5indicator'].unique())\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(l2)):\n",
    "            if data.iloc[i,38] == l2[j]:\n",
    "                data.iloc[i,38] = j\n",
    "\n",
    "    data['UD3indicator'] = data['UD3indicator'].astype(int)\n",
    "    data['UD5indicator'] = data['UD5indicator'].astype(int)\n",
    "\n",
    "    l = list(data2['UD3indicator'].unique())\n",
    "    for i in range(len(data2)):\n",
    "        for j in range(len(l)):\n",
    "            if data2.iloc[i,37] == l[j]:\n",
    "                data2.iloc[i,37] = j\n",
    "\n",
    "\n",
    "    l2 = list(data2['UD5indicator'].unique())\n",
    "    for i in range(len(data2)):\n",
    "        for j in range(len(l2)):\n",
    "            if data2.iloc[i,38] == l2[j]:\n",
    "                data2.iloc[i,38] = j\n",
    "\n",
    "    data2['UD3indicator'] = data2['UD3indicator'].astype(int)\n",
    "    data2['UD5indicator'] = data2['UD5indicator'].astype(int)\n",
    "\n",
    "data = data.dropna()\n",
    "data.replace([np.inf, -np.inf, np.nan], 1, inplace=True)\n",
    "\n",
    "l = list(data['UD3indicator'].unique())\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(l)):\n",
    "        if data.iloc[i,37] == l[j]:\n",
    "            data.iloc[i,37] = j\n",
    "\n",
    "\n",
    "l2 = list(data['UD5indicator'].unique())\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(l2)):\n",
    "        if data.iloc[i,38] == l2[j]:\n",
    "            data.iloc[i,38] = j\n",
    "            \n",
    "data['UD3indicator'] = data['UD3indicator'].astype(int)\n",
    "data['UD5indicator'] = data['UD5indicator'].astype(int)\n",
    "\n",
    "l = list(data2['UD3indicator'].unique())\n",
    "for i in range(len(data2)):\n",
    "    for j in range(len(l)):\n",
    "        if data2.iloc[i,37] == l[j]:\n",
    "            data2.iloc[i,37] = j\n",
    "\n",
    "\n",
    "l2 = list(data2['UD5indicator'].unique())\n",
    "for i in range(len(data2)):\n",
    "    for j in range(len(l2)):\n",
    "        if data2.iloc[i,38] == l2[j]:\n",
    "            data2.iloc[i,38] = j\n",
    "            \n",
    "data2['UD3indicator'] = data2['UD3indicator'].astype(int)\n",
    "data2['UD5indicator'] = data2['UD5indicator'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b5e1a-65ec-4e19-90c0-658829cf9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = five #m1, m3, m6, one, three, five\n",
    "Symbol = \"TRX-USD\"\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "data['MA5'] = TMRW.FINANCE.twa(data['value'], 5)[4] # 5 day\n",
    "data['MA20'] = TMRW.FINANCE.twa(data['value'], 20)[19] # 20 day\n",
    "data['MA40'] = TMRW.FINANCE.twa(data['value'], 40)[39] # 40 day\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if  data['value'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and data['MA20'][i] < data['MA40'][i]:\n",
    "        data['buy_signal'][i] = True\n",
    "        \n",
    "    if data['value'][i] > data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and data['MA20'][i] > data['MA40'][i]:\n",
    "        data['sell_signal'][i] = True\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "long_signal_dates = data[data['buy_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "short_signal_dates = data[data['sell_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Mean reversion Strategy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9bd56-856c-4c17-b543-6ec3d17cb33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from binance.enums import *\n",
    "import numpy as np\n",
    "\n",
    "quantile = 0.05\n",
    "inter = \"1day\"\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) + timedelta(days = +1) \n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -1825) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "opt_factor = []\n",
    "best_factors = []\n",
    "\n",
    "for tick in ['SOLUSDT', 'TRXUSDT', 'XRPUSDT', 'ETHUSDT', 'BTCUSDT', 'BNBUSDT', 'ADAUSDT', 'ATOMUSDT', 'DOGEUSDT', 'LTCUSDT', 'XLMUSDT', 'NEARUSDT', 'AVAXUSDT']:\n",
    "\n",
    "    klines = client.get_historical_klines(tick, Client.KLINE_INTERVAL_1HOUR, day7, todays)\n",
    "    data = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "    data.Open = data.Open.astype(float)\n",
    "    data.High = data.High.astype(float)\n",
    "    data.Low = data.Low.astype(float)\n",
    "    data.Close = data.Close.astype(float)\n",
    "    data.Volume = data.Volume.astype(float)\n",
    "    data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "    data['value'] = (data['Open'] + data['High'] + data['Low']) / 3\n",
    "    data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    a = list(TMRW.FINANCE.returns(data['Open'])['Open'])\n",
    "    a.insert(0,0)\n",
    "    data['returns'] = a\n",
    "\n",
    "    data_day = pd.DataFrame()\n",
    "    _Open = []\n",
    "    _Close = []\n",
    "    _High = []\n",
    "    _Low = []\n",
    "    _timeframe = round(len(data)/24)\n",
    "    for i in range(_timeframe-1):\n",
    "        _Open.append(data['Open'][i*24])\n",
    "        _Close.append(data['Close'][(i+1)*24])\n",
    "        _High.append(max(data['High'][i*24:(i+1)*24]))\n",
    "        _Low.append(min(data['Low'][i*24:(i+1)*24]))\n",
    "\n",
    "\n",
    "    data_day['Open'] = _Open\n",
    "    data_day['Close'] = _Close\n",
    "    data_day['High'] = _High\n",
    "    data_day['Low'] = _Low\n",
    "\n",
    "    a = list(TMRW.FINANCE.returns(data_day['Open'])['Open'])\n",
    "    a.insert(0,0)\n",
    "    data_day['returns'] = a\n",
    "\n",
    "    data['buy_signal'] = False\n",
    "    data['sell_signal'] = False\n",
    "    \n",
    "    for interval in [20,30,40,480, 10, 240, 500]:\n",
    "        \n",
    "        Q1 = np.zeros(len(data_day))\n",
    "        Q2 = np.zeros(len(data_day))\n",
    "        for i in range(interval,len(data_day)):\n",
    "            Q1[i] = np.quantile(data_day['returns'][i-interval:i], 1-quantile)\n",
    "            Q2[i] = np.quantile(data_day['returns'][i-interval:i], quantile)\n",
    "\n",
    "        data_day['Q1'] = Q1\n",
    "        data_day['Q2'] = Q2\n",
    "\n",
    "        for i in range(1, len(data)):\n",
    "\n",
    "\n",
    "            j = round(len(data_day)/len(data) * i)\n",
    "\n",
    "            if j > 363:\n",
    "                j = 363\n",
    "\n",
    "            # generate long/buy signal        \n",
    "            if sum(data['returns'][i-interval:i]) > data_day['Q1'][j]:\n",
    "                data['buy_signal'][i] = True\n",
    "\n",
    "            #if data['Open'][i] < data['Lower'][i]:\n",
    "                #data['buy_signal'][i] = True\n",
    "\n",
    "            if sum(data['returns'][i-interval:i]) < data_day['Q2'][j]:\n",
    "                data['sell_signal'][i] = True\n",
    "            \n",
    "            #if data['Open'][i] > data['Upper'][i]:\n",
    "                #data['sell_signal'][i] = True \n",
    "                #buy = False\n",
    "\n",
    "\n",
    "        #fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Add arrows for long (buy) signals\n",
    "        #signal_dates = data[data['buy_signal']].index\n",
    "        #for date in signal_dates:\n",
    "            #ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "        # Add arrows for short (sell) signals\n",
    "        #signal_dates = data[data['sell_signal']].index\n",
    "        #for date in signal_dates:\n",
    "            #ax.annotate('o', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "        # Plot the time series\n",
    "        #data['value'][:].plot(figsize=(10, 6))\n",
    "        #plt.xlabel('Date')\n",
    "        #plt.ylabel('Value')\n",
    "        #plt.title('Sigma Strategy')\n",
    "        #plt.show()\n",
    "\n",
    "        val = 100\n",
    "        buy = False\n",
    "\n",
    "        for i in range(24, len(data)):\n",
    "            if data['buy_signal'][i-24] == True and buy == False:\n",
    "                buy = True\n",
    "                vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "\n",
    "            if data['sell_signal'][i-24] == True and buy == True:\n",
    "                buy = False\n",
    "                val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "        opt_factor.append([inter, tick, val, interval, quantile])\n",
    "        #test.append([vol, mu, )\n",
    "\n",
    "opt_factor # 5 year data'\n",
    "# somewhere between 30 and 50 days is optimal\n",
    "\n",
    "#best_factors\n",
    "\n",
    "interval = 10\n",
    "quantile = 0.1\n",
    "\n",
    "data_3day = pd.DataFrame()\n",
    "_Open = []\n",
    "_Close = []\n",
    "_High = []\n",
    "_Low = []\n",
    "_timeframe = round(len(data)/72)\n",
    "for i in range(_timeframe-1):\n",
    "    _Open.append(data['Open'][i*72])\n",
    "    _Close.append(data['Close'][(i+1)*72])\n",
    "    _High.append(max(data['High'][i*72:(i+1)*72]))\n",
    "    _Low.append(min(data['Low'][i*72:(i+1)*72]))\n",
    "    \n",
    "    \n",
    "data_3day['Open'] = _Open\n",
    "data_3day['Close'] = _Close\n",
    "data_3day['High'] = _High\n",
    "data_3day['Low'] = _Low\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data_3day['Open'])['Open'])\n",
    "a.insert(0,0)\n",
    "data_3day['returns'] = a\n",
    "\n",
    "Q1 = np.zeros(len(data_3day))\n",
    "Q2 = np.zeros(len(data_3day))\n",
    "for i in range(interval,len(data_3day)):\n",
    "    Q1[i] = np.quantile(data_3day['returns'][i-interval:i], quantile)\n",
    "    Q2[i] = np.quantile(data_3day['returns'][i-interval:i], 1-quantile)\n",
    "\n",
    "data_3day['Q1'] = Q1\n",
    "data_3day['Q2'] = Q2\n",
    "\n",
    "interval = 100\n",
    "quantile = 0.05\n",
    "\n",
    "\n",
    "data_5day = pd.DataFrame()\n",
    "_Open = []\n",
    "_Close = []\n",
    "_High = []\n",
    "_Low = []\n",
    "_timeframe = round(len(data)/120)\n",
    "for i in range(_timeframe-1):\n",
    "    _Open.append(data['Open'][i*120])\n",
    "    _Close.append(data['Close'][(i+1)*120])\n",
    "    _High.append(max(data['High'][i*120:(i+1)*120]))\n",
    "    _Low.append(min(data['Low'][i*120:(i+1)*120]))\n",
    "    \n",
    "    \n",
    "data_5day['Open'] = _Open\n",
    "data_5day['Close'] = _Close\n",
    "data_5day['High'] = _High\n",
    "data_5day['Low'] = _Low\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data_5day['Open'])['Open'])\n",
    "a.insert(0,0)\n",
    "data_5day['returns'] = a\n",
    "\n",
    "Q1 = np.zeros(len(data_5day))\n",
    "Q2 = np.zeros(len(data_5day))\n",
    "for i in range(interval,len(data_5day)):\n",
    "    Q1[i] = np.quantile(data_5day['returns'][i-interval:i], quantile)\n",
    "    Q2[i] = np.quantile(data_5day['returns'][i-interval:i], 1-quantile)\n",
    "\n",
    "data_5day['Q1'] = Q1\n",
    "data_5day['Q2'] = Q2\n",
    "\n",
    "var = 0\n",
    "for i in range(len(opt_factor)):\n",
    "    if opt_factor[i][1] > var:\n",
    "        best_factors = opt_factor[i]\n",
    "        var = opt_factor[i][1]\n",
    "\n",
    "data['value'] = (data['Open'] + data['Close']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],6)\n",
    "\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "for i in range(4, len(data)):\n",
    "    \n",
    "    # generate long/buy signal  \n",
    "    if np.mean(data['returns'][i-24:i]) > np.mean(data['returns'][i-48:i-24]) and np.mean(data['returns'][i-48:i-24]) < np.mean(data['returns'][i-72:i-48]) and np.mean(data['value'][i-72:i-48]) > np.mean(data['value'][i-24:i]):\n",
    "        if np.mean(data['returns'][i-2:i]) > np.mean(data['returns'][i-12:i-5]) and np.mean(data['returns'][i-12:i-5]) < np.mean(data['returns'][i-24:i-12]) and np.mean(data['value'][i-24:i-12]) > np.mean(data['value'][i-4:i]):\n",
    "            #if np.mean(data['returns'][i-96:i-72]) < np.mean(data['returns'][i-72:i-48]):\n",
    "                data['buy_signal'][i] = True\n",
    "    #elif data['sell_signal'][i-4] == True:\n",
    "        #data['buy_signal'][i] = True\n",
    "\n",
    "        \n",
    "    # generate short/sell signal\n",
    "    if np.mean(data['returns'][i-24:i]) < np.mean(data['returns'][i-48:i-24]) and np.mean(data['returns'][i-48:i-24]) > np.mean(data['returns'][i-72:i-48]):\n",
    "        if np.mean(data['returns'][i-2:i]) < np.mean(data['returns'][i-12:i-5]) and np.mean(data['returns'][i-12:i-5]) > np.mean(data['returns'][i-24:i-12]):\n",
    "            #if np.mean(data['returns'][i-96:i-72]) > np.mean(data['returns'][i-72:i-48]):\n",
    "            data['sell_signal'][i] = True\n",
    "        \n",
    "    #if data['High'][i-1] > data['High'][i-3] and data['High'][i-2] > data['High'][i-4] and data['value'][i] > data['High'][i-2]:\n",
    "        #data['sell_signal'][i] = True\n",
    "\n",
    "#data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],144)\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "k = 0\n",
    "j = 0\n",
    "\n",
    "for i in range(720,len(data)):\n",
    "\n",
    "    # generate long/buy signal\n",
    "\n",
    "    if np.mean(data['returns'][i-720:i]) < 0 and np.mean(data['returns'][i-480:i]) < 0.01 and np.mean(data['returns'][i-240:i]) < 0:\n",
    "        \n",
    "        if buy == True:\n",
    "            data['sell_signal'][i] = True\n",
    "            buy = False\n",
    "            \n",
    "        next\n",
    "        \n",
    "        \n",
    "    elif np.std(data['returns'][i-200:i]) < -0.06 or np.std(data['returns'][i-200:i]) > 0.06:\n",
    "        \n",
    "        next\n",
    "    \n",
    "    \n",
    "    else:\n",
    "\n",
    "        if data['Open'][i] < 1.001 * min(data['value'][i-480:i-24]) and np.mean(data['returns'][i-480:i-24]) > 0:\n",
    "            #if data['Open'][i] \n",
    "            data['buy_signal'][i] = True\n",
    "            k = i\n",
    "            buy = True\n",
    "\n",
    "        elif data['Open'][i] < 1.0001 * min(data['value'][i-480:i-24]) and np.mean(data['returns'][i-240:i-24]) > -0.01:\n",
    "            data['buy_signal'][i] = True\n",
    "            k = i\n",
    "            buy = True\n",
    "\n",
    "        elif data['Open'][i] < 1.001 * min(data['value'][i-36:i-12]) and np.mean(data['returns'][i-24:i-1]) > -0.01 and np.mean(data['returns'][i-24:i-4]) > np.mean(data['returns'][i-480:i-48]) and data['Open'][i] > 0.92 * np.mean(data['Open'][i-48:i-6]):\n",
    "            if np.mean(data['returns'][i-24:i]) < 0.01 and np.mean(data['returns'][i-12:i-4]) < 0 and np.mean(data['returns'][i-4:i]) > -0.01:\n",
    "                data['buy_signal'][i] = True\n",
    "                k = i\n",
    "                buy = True\n",
    "\n",
    "        # generate short/sell signal\n",
    "\n",
    "        if data['Open'][i] > 1.3 * data['Open'][k] and data['Open'][i] > 0.9999 * max(data['value'][i-480:i-24]) and np.mean(data['returns'][i-24:i-4]) < np.mean(data['returns'][i-480:i-48]):\n",
    "            data['sell_signal'][i] = True\n",
    "            buy = False\n",
    "\n",
    "        if data['Open'][i] > 0.9 * max(data['value'][i-240:i-24]):\n",
    "            if np.mean(data['returns'][i-30:i-10]) > 0.001 and np.mean(data['returns'][i-24:i-8]) > -0.0005 and np.mean(data['returns'][i-24:i-8]) < 0.0005 and np.mean(data['returns'][i-12:i]) < -0.0005:\n",
    "                data['sell_signal'][i] = True\n",
    "                buy = False\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "long_signal_dates = data[data['buy_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "short_signal_dates = data[data['sell_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Max-Min Strategy')\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "        \n",
    "val - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fbddc-d087-4cc7-9bcf-5780d3eec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today() \n",
    "time = three #m1, m3, m6, one, three, five\n",
    "Symbol = \"MSFT\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "data['mean_return'] = data['returns'].rolling(window=30).mean().values.flatten()\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "for i in range(3,len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "\n",
    "    if data['mean_return'][i] < np.mean(data['returns']) and data['value'][i] < 0.97*np.mean(data['value'][i-30:i]) and data['value'][i] > data['value'][i-1]:\n",
    "        data['buy_signal'][i] = True \n",
    "        \n",
    "    #if data['mean_return'][i-3] > 0 and data['mean_return'][i-2] > 0 and data['mean_return'][i-1] > 0 and np.mean(data['returns'][i-2:i]) < 0 :\n",
    "    if data['autocorr_30'][i] > 0.96 or data['value'][i] > 1.2*np.mean(data['value'][i-30:i]) :\n",
    "        data['sell_signal'][i] = True    \n",
    "        \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Trend following Strategy:' + Symbol)\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "        \n",
    "val - 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e4bfe-ab28-4950-b15a-a14246bb2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = one #m1, m3, m6, one, three, five\n",
    "Symbol = \"MSFT\"\n",
    "((val-100)/100)/(np.std(data['value'])/np.mean(data['value']))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "pos_neg = []\n",
    "for i in range(len(data)):\n",
    "    if data['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif data['returns'][i] < 0:\n",
    "        pos_neg.append(-1)\n",
    "        \n",
    "data['pos_neg'] = pos_neg\n",
    "\n",
    "data['mean_return'] = data['returns'].rolling(window=15).mean().values.flatten()\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],5)\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "data['short_signal'] = False\n",
    "data['long_signal'] = False\n",
    "s = False\n",
    "\n",
    "for i in range(3,len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "\n",
    "    if data['mean_return'][i] > 0.005 and data['returns'][i-1] < 0.01 and data['returns'][i-2] < 0.02:\n",
    "        data['buy_signal'][i] = True \n",
    "        \n",
    "    if data['buy_signal'][i-3] == True and data['returns'][i-1] > 0 and data['returns'][i-2] > 0:\n",
    "        data['sell_signal'][i] = True \n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate short/sell signal\n",
    "    if data['mean_return'][i] < -0.01 and s == False:\n",
    "        data['short_signal'][i] = True\n",
    "        s = True\n",
    "    \n",
    "    if data['returns'][i-1] > 0 and data['returns'][i-2] > 0 and s == True:\n",
    "        data['long_signal'][i] = True\n",
    "        s = False\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "\n",
    "signal_dates = data[data['short_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "signal_dates = data[data['long_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='green', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Trend following Strategy:' + Symbol)\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "buy2 = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i] - 0.005 * data['Open'][i]\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = val * (1 - 0.005+(data['Close'][i] - buy_price)/buy_price)\n",
    "    \n",
    "    if data['short_signal'][i] == True and buy2 == False:\n",
    "        buy2 = True\n",
    "        buy2_price = data['Open'][i] - 0.005 * data['Open'][i]\n",
    "    \n",
    "    if (data['long_signal'][i] == True or data['buy_signal'][i] == True) and buy2 == True:\n",
    "        buy2 = False\n",
    "        val = val * (1 - 0.005 +(buy2_price-data['Close'][i])/buy2_price)\n",
    "    \n",
    "        \n",
    "\n",
    "val-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8a9c7-80cd-4e80-b497-6eb468067573",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = one #m1, m3, m6, one, three, five\n",
    "Symbol = \"ETH-USD\"\n",
    "((val-100)/100)/(np.std(data['value'])/np.mean(data['value']))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data = b.copy()\n",
    "#data.Open = data.Open.astype(float)\n",
    "#data.High = data.High.astype(float)\n",
    "#data.Low = data.Low.astype(float)\n",
    "#data.Close = data.Close.astype(float)\n",
    "#data.Volume = data.Volume.astype(float)\n",
    "#data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "pos_neg = []\n",
    "for i in range(len(data)):\n",
    "    if data['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif data['returns'][i] < 0:\n",
    "        pos_neg.append(-1)\n",
    "        \n",
    "data['pos_neg'] = pos_neg\n",
    "\n",
    "data['mean_return'] = data['returns'].rolling(window=15).mean().values.flatten()\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],5)\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "data['short_signal'] = False\n",
    "data['long_signal'] = False\n",
    "\n",
    "for i in range(1,len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "\n",
    "    if np.mean(data['pos_neg'][i-2:i]) < 0 and data['value'][i] > data['value'][i-1] and data['value'][i] < np.mean(data['value'][i-5:i-2]):\n",
    "        data['buy_signal'][i] = True\n",
    "        k = i\n",
    "        buy = True\n",
    "        \n",
    "    elif buy == False and np.mean(data['pos_neg'][i-10:i]) > 0:\n",
    "        data['buy_signal'][i] = True\n",
    "        k = i\n",
    "        buy = True\n",
    "        \n",
    "    if np.mean(data['pos_neg'][i-10:i]) > 0 and data['value'][i] < data['value'][i-1] and i-k > 2:\n",
    "        data['sell_signal'][i] = True\n",
    "        buy = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate short/sell signal\n",
    "    #if data['pos_neg'][i] < np.mean(data['pos_neg'][i-30:i]):\n",
    "        #data['short_signal'][i] = True\n",
    "    \n",
    "    #if data['short_signal'][i-1] == True and data['returns'][i] > 0:\n",
    "        #data['long_signal'][i] = True\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "\n",
    "#signal_dates = data[data['short_signal']].index\n",
    "#for date in signal_dates:\n",
    "    #ax.annotate('o', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "#signal_dates = data[data['long_signal']].index\n",
    "#for date in signal_dates:\n",
    "    #ax.annotate('x', xy=(date, data.loc[date, 'value']), color='green', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Trend following Strategy:' + Symbol)\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "#buy2 = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i] - 0.005 * data['Open'][i]\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = val * (1 - 0.005 + (data['Close'][i] - buy_price)/buy_price)\n",
    "    \n",
    "    \n",
    "    #if data['short_signal'][i] == True and buy2 == False:\n",
    "        #buy2 = True\n",
    "        #buy2_price = data['Open'][i] - 0.005 * data['Open'][i]\n",
    "    \n",
    "    #if data['long_signal'][i] == True and buy2 == True:\n",
    "        #buy2 = False\n",
    "        #val = val * (1 - 0.005 + (buy2_price-data['Close'][i])/buy2_price)\n",
    "    \n",
    "        \n",
    "\n",
    "val-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279154b-a2e7-4015-897a-2a46436b7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollinger_bands(data, length):\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, length):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(length,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-length:i])\n",
    "        std_lst[i] = np.std(data[i-length:i])\n",
    "        \n",
    "    up = mean_lst + 2.5 * std_lst\n",
    "    down = mean_lst - 2.5 * std_lst\n",
    "    \n",
    "    boll = pd.DataFrame()\n",
    "    boll['Upper'] = up\n",
    "    boll['Lower'] = down\n",
    "    return(boll)\n",
    "\n",
    "time = three #m1, m3, m6, one, three, five\n",
    "Symbol = \"MSFT\"\n",
    "((val-100)/100)/(np.std(data['value'])/np.mean(data['value']))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "data['Lower'] = bollinger_bands(data['value'],720)['Lower']\n",
    "data['Upper'] = bollinger_bands(data['value'],720)['Upper']\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "buy = False\n",
    "\n",
    "for i in range(720, len(data)):\n",
    "    \n",
    "    # generate long/buy signal        \n",
    "    if data['value'][i] < data['Lower'][i]:\n",
    "        data['buy_signal'][i] = True\n",
    "    \n",
    "    if data['value'][i] > data['Upper'][i]:\n",
    "        data['sell_signal'][i] = True \n",
    "        buy = False\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sigma Strategy')\n",
    "plt.show()\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        vp = val / data['Close'][i] * (1 - 0.000712)\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = (vp * data['Close'][i]) * (1 - 0.000712)\n",
    "\n",
    "val - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6bcfe-684c-487b-a55c-f8198129ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autocorrelation test - too many problems\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "data['autocorr_30'] = data['value'].rolling(window=15).apply(lambda x: x.autocorr())\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],5)\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "data['short_signal'] = False\n",
    "data['long_signal'] = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if i > 100 and data['returns'][i] > 0.7 * max(data['returns'][i-100:i]):\n",
    "        next\n",
    "        \n",
    "    elif i > 100 and data['value'][i] > 0.7 * max(data['value'][i-100:i]):\n",
    "        next\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # generate long/buy signal\n",
    "\n",
    "        if data['autocorr_30'][i] > 0.90 and np.mean(data['returns'][i-15:i]) < 0:\n",
    "            data['buy_signal'][i] = True \n",
    "\n",
    "        if (data['autocorr_30'][i] < 0.76 and np.mean(data['returns'][i-15:i]) < 0):\n",
    "            data['sell_signal'][i] = True\n",
    "            \n",
    "        elif (i> 60 and data['value'][i-1] > 0.9*max(data['value'][i-60:i])):\n",
    "            data['sell_signal'][i] = True\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Autocorrelation Strategy:' + Symbol)\n",
    "\n",
    "val = 100\n",
    "\n",
    "buy_count = 0\n",
    "buy = False\n",
    "buy2 = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if data['buy_signal'][i] == True:\n",
    "        buy_count = buy_count + 1\n",
    "    \n",
    "    if buy_count > 3 and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i]    \n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = val * (1+(data['Close'][i] - buy_price)/buy_price)\n",
    "    \n",
    "        \n",
    "\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f1a47-ec21-4205-bf4f-77278eb34edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['High'] + data['Low']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "data['short_signal'] = False\n",
    "data['long_signal'] = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "\n",
    "    if data['autocorr_30'][i] < 0.78 and data['returns'][i-1] > 0:\n",
    "        data['buy_signal'][i] = True \n",
    "        \n",
    "    if data['autocorr_30'][i] > 0.95 and data['returns'][i] < -0.02:\n",
    "        data['sell_signal'][i] = True \n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate short/sell signal\n",
    "    if data['sell_signal'][i-1] == True:\n",
    "        data['short_signal'][i] = True\n",
    "    \n",
    "    if data['buy_signal'][i-1] == True  :\n",
    "        data['long_signal'][i] = True\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "signal_dates = data[data['buy_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "signal_dates = data[data['sell_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "\n",
    "signal_dates = data[data['short_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "    \n",
    "signal_dates = data[data['long_signal']].index\n",
    "for date in signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='green', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Trend following Strategy:' + Symbol)\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "buy2 = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i]    \n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = val * (1+(data['Close'][i] - buy_price)/buy_price)\n",
    "    \n",
    "    \n",
    "    if data['short_signal'][i] == True and buy2 == False:\n",
    "        buy2 = True\n",
    "        buy2_price = data['Open'][i]\n",
    "    \n",
    "    if data['long_signal'][i] == True and buy2 == True:\n",
    "        buy2 = False\n",
    "        val = val * (1+(buy2_price-data['Close'][i])/buy2_price)\n",
    "    \n",
    "        \n",
    "\n",
    "val\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "threshold = 0.1\n",
    "n = 100\n",
    "for i in range(len(lste[0:n])):\n",
    "    df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)['High']\n",
    "    #alph = np.zeros(152)\n",
    "    #for j in range(len(alpha)):\n",
    "        #alph[j] = np.mean(list(alpha.iloc[j,0:4]))\n",
    "    \n",
    "    #df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)\n",
    "\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df = df.fillna(0)\n",
    "lst = []\n",
    "for i in range(len(df.columns)):\n",
    "    if np.mean(df.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "\n",
    "df = df.drop(df.columns[lst], axis=1)\n",
    "\n",
    "values = pd.DataFrame(df['BTC-USD'].values)\n",
    "data = pd.DataFrame({'value': values[0]})\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "data['mean_return'] = data['returns'].rolling(window=15).mean().values.flatten()\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "data['MA15'] = TMRW.FINANCE.twa(data['value'], 15)[14]\n",
    "data['MA15'][0] = 0\n",
    "data['MA60'] = TMRW.FINANCE.twa(data['value'], 30)[29]\n",
    "data['MA60'][0] = 0\n",
    "data['Lower'] = TMRW.FINANCE.bollinger_bands(data['value'], 15)['Lower']\n",
    "data['Upper'] = TMRW.FINANCE.bollinger_bands(data['value'], 15)['Upper']\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],5)\n",
    "data['long_signal'] = False\n",
    "data['short_signal'] = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "    if (data['MA15'][i] < data['MA60'][i]) and (data['MA15'][i] > data['value'][i]) and (data['value'][i] < data['Lower'][i]):# and (data['RSI'][i] >25):\n",
    "        data['long_signal'][i] = True\n",
    "    \n",
    "    elif data['mean_return'][i] > 0.005 and (data['value'][i] < 1.05*data['MA15'][i]) and (data['RSI'][i] < 60):\n",
    "        data['long_signal'][i] = True \n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate short/sell signal\n",
    "    if data['MA15'][i] > data['MA60'][i] and data['MA15'][i] < data['value'][i] and data['value'][i] > data['Upper'][i]:\n",
    "        data['short_signal'][i] = True\n",
    "    \n",
    "    elif data['mean_return'][i] < -0.005 and (data['value'][i] < data['MA15'][i]) and (data['value'][i] < data['MA60'][i]) and (data['RSI'][i] < 40) and (data['RSI'][i] > 20):\n",
    "        data['short_signal'][i] = True\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "long_signal_dates = data[data['long_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "short_signal_dates = data[data['short_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Data')\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "threshold = 0.1\n",
    "n = 100\n",
    "for i in range(len(lste[0:n])):\n",
    "    df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)['Low']\n",
    "    #alph = np.zeros(152)\n",
    "    #for j in range(len(alpha)):\n",
    "        #alph[j] = np.mean(list(alpha.iloc[j,0:4]))\n",
    "    \n",
    "    #df[lste[i]] = TMRW.DATA.data(lste[i],m6,today)\n",
    "\n",
    "df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "df = df.fillna(0)\n",
    "lst = []\n",
    "for i in range(len(df.columns)):\n",
    "    if np.mean(df.iloc[:,i]) == 0:\n",
    "        lst.append(i)\n",
    "\n",
    "df = df.drop(df.columns[lst], axis=1)\n",
    "\n",
    "values = pd.DataFrame(df['BTC-USD'].values)\n",
    "data = pd.DataFrame({'value': values[0]})\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "data['mean_return'] = data['returns'].rolling(window=15).mean().values.flatten()\n",
    "data['autocorr_30'] = data['value'].rolling(window=30).apply(lambda x: x.autocorr())\n",
    "data['MA15'] = TMRW.FINANCE.twa(data['value'], 15)[14]\n",
    "data['MA15'][0] = 0\n",
    "data['MA60'] = TMRW.FINANCE.twa(data['value'], 30)[29]\n",
    "data['MA60'][0] = 0\n",
    "data['Lower'] = TMRW.FINANCE.bollinger_bands(data['value'], 15)['Lower']\n",
    "data['Upper'] = TMRW.FINANCE.bollinger_bands(data['value'], 15)['Upper']\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'],5)\n",
    "data['long_signal'] = False\n",
    "data['short_signal'] = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    # generate long/buy signal\n",
    "    if (data['MA15'][i] < data['MA60'][i]) and (data['MA15'][i] > data['value'][i]) and (data['value'][i] < data['Lower'][i]):# and (data['RSI'][i] >25):\n",
    "        data['long_signal'][i] = True\n",
    "    \n",
    "    elif data['mean_return'][i] > 0.005 and (data['value'][i] < 1.05*data['MA15'][i]) and (data['RSI'][i] < 60):\n",
    "        data['long_signal'][i] = True \n",
    "    \n",
    "    \n",
    "    \n",
    "    # generate short/sell signal\n",
    "    if data['MA15'][i] > data['MA60'][i] and data['MA15'][i] < data['value'][i] and data['value'][i] > data['Upper'][i]:\n",
    "        data['short_signal'][i] = True\n",
    "    \n",
    "    elif data['mean_return'][i] < -0.005 and (data['value'][i] < data['MA15'][i]) and (data['value'][i] < data['MA60'][i]) and (data['RSI'][i] < 40) and (data['RSI'][i] > 20):\n",
    "        data['short_signal'][i] = True\n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Add arrows for long (buy) signals\n",
    "long_signal_dates = data[data['long_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "# Add arrows for short (sell) signals\n",
    "short_signal_dates = data[data['short_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time Series Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7385015-d04e-4cb1-b663-11cd97aa2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "# Visualise autocorrelation using the plot_acf function\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_acf(data['value'], lags=30, ax=ax)\n",
    "\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.title('Autocorrelation Plot')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot the PACF\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_pacf(data['value'], lags=30, ax=ax)\n",
    "\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Partial Autocorrelation')\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e60ab-3871-4687-996e-3e6a20cdae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation og cointegration liste\n",
    "m1_c = pd.DataFrame(corr_coint_list(lste, m1, today))\n",
    "m3_c = pd.DataFrame(corr_coint_list(lste, m3, today))\n",
    "m6_c = pd.DataFrame(corr_coint_list(lste, m6, today))\n",
    "\n",
    "# stationarity, randomwalk, mean reversion check 1, mean reversion check 2, Linear trend, Linear Trend, Exponential trend?, auto-correlation?\n",
    "m1_s = stat_test(m1, today, lste, \"prices\", 15)\n",
    "m3_s = stat_test(m3, today, lste, \"prices\", 15)\n",
    "m6_s = stat_test(m6, today, lste, \"prices\", 15)\n",
    "\n",
    "\n",
    "\n",
    "# linear pairing mellem a og b currency\n",
    "# Are there resistance levels?\n",
    "# trend detection with returns mean\n",
    "# Trend detection with autocorrelation loss?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"E:/Investering/stats.xlsx\") as writer:\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            m1_c.to_excel(writer, sheet_name=\"1m corr coint\")\n",
    "        elif i == 1:\n",
    "            m1_s.to_excel(writer, sheet_name=\"1m stats\")\n",
    "        elif i == 2:\n",
    "            m3_c.to_excel(writer, sheet_name=\"3m corr coint\")\n",
    "        elif i == 3:\n",
    "            m3_s.to_excel(writer, sheet_name=\"3m stats\")\n",
    "        elif i == 4:\n",
    "            m6_c.to_excel(writer, sheet_name=\"6m corr coint\")\n",
    "        elif i == 5:\n",
    "            m6_s.to_excel(writer, sheet_name=\"6m stats\")\n",
    "            \n",
    "            \n",
    "# round(a.half_life_v2(x)/PERIOD_PER_DAY)\n",
    "\n",
    "# detect overall markov chain\n",
    "\n",
    "# detect overall mean reversion times\n",
    "\n",
    "# detect overall mean reversion sizes\n",
    "\n",
    "def stat_test(fra, til, lste, typ, win):\n",
    "    \n",
    "    prices = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(lste)):\n",
    "        prices[lste[i]] = TMRW.DATA.data(lste[i],fra,til)['Close']\n",
    "\n",
    "    prices = prices.fillna(0)\n",
    "\n",
    "    lst = []\n",
    "    for i in range(len(prices.columns)):\n",
    "        if np.mean(prices.iloc[:,i]) == 0:\n",
    "            lst.append(i)\n",
    "\n",
    "    prices = prices.drop(prices.columns[lst], axis=1)\n",
    "\n",
    "    returns = prices.pct_change()#.dropna()\n",
    "    returns = returns.iloc[1:len(returns),:]\n",
    "    returns.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    returns = returns.fillna(0)\n",
    "\n",
    "    if typ == \"prices\":\n",
    "        data = prices\n",
    "    elif typ == \"returns\":\n",
    "        data = returns\n",
    "    \n",
    "    df = []\n",
    "    for i in range(len(data.columns)):\n",
    "        if a.perform_adf_test(data.iloc[:,i])[2] == True:\n",
    "            b = \"Stationary\"\n",
    "        else:\n",
    "            b = \"Non-stationary\"\n",
    "\n",
    "        if a.perform_variance_ratio_test(data.iloc[:,i])[1] == True:\n",
    "            c = \"Random walk\"\n",
    "        else:\n",
    "            c = \"Not Random walk\"\n",
    "\n",
    "        if a.perform_hurst_exp_test(data.iloc[:,i])[0] < 0.25:\n",
    "            d = \"Mean reverting\"\n",
    "        elif a.perform_hurst_exp_test(data.iloc[:,i])[0] > 0.75:\n",
    "            d = \"Trending\"\n",
    "        else:\n",
    "            d = \"None\"\n",
    "\n",
    "        adf = adfuller(data.iloc[:,i], 1)\n",
    "\n",
    "        if adf[1] < 0.2 and adf[0] > (adf[4]['10%'] - adf[4]['1%'])/adf[4]['1%'] * 20: \n",
    "            d_ = \"Mean reverting\"\n",
    "\n",
    "        elif adf[1] < 0.1 and adf[0] > adf[4]['10%']:\n",
    "            d_ = \"not mean reverting\"\n",
    "\n",
    "        elif (adf[0] > dict(adf[4])['1%']) and (adf[0] > dict(adf[4])['5%']) and (adf[0] > dict(adf[4])['10%']):\n",
    "            d_ = \"not mean reverting\"\n",
    "\n",
    "        else:\n",
    "            d_ = \"Mean reverting\"\n",
    "\n",
    "        if mk_test(data.iloc[:,i], mode = 'simple', window = win, alpha = 0.05)[0] == False:\n",
    "            e = \"Non-Trending\"\n",
    "        elif mk_test(data.iloc[:,i], mode = 'simple', window = win, alpha = 0.05)[0] == True:\n",
    "            e = \"Trending\"\n",
    "\n",
    "        if cox_stuart(data.iloc[:,i], window = win, alpha = 0.00005)[0] == False:\n",
    "            f = \"Non-Trending\"\n",
    "        elif cox_stuart(data.iloc[:,i], window = win, alpha = 0.00005)[0] == True:\n",
    "            f = \"Trending\"\n",
    "\n",
    "        if abbe_criterion(data.iloc[:,i], window = win, alpha = 0.00005)[0] == False:\n",
    "            g = \"Non-Trending\"\n",
    "        elif abbe_criterion(data.iloc[:,i], window = win, alpha = 0.00005)[0] == True:\n",
    "            g = \"Trending\"\n",
    "\n",
    "        if autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == False:\n",
    "            h = \"Non-Trending\"\n",
    "        elif autocorrelation(data.iloc[:,i], window = win, alpha = 0.0001)[0] == True:\n",
    "            h = \"Trending\"\n",
    "\n",
    "\n",
    "\n",
    "        df.append([prices.columns[i],b,c,d,d_,e,f,g,h])\n",
    "    df = pd.DataFrame(df, columns = ['Symbol', 'Stationarity', 'Randomwalk(yes/no)', 'Hurst Exponent', 'ADfuller test', 'MK test', 'Cox Stuart', 'Abbe Criterion', 'Autocorrelation'])\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# correlation og cointegration liste\n",
    "m1_c = pd.DataFrame(corr_coint_list(lste, m1, today))\n",
    "m3_c = pd.DataFrame(corr_coint_list(lste, m3, today))\n",
    "m6_c = pd.DataFrame(corr_coint_list(lste, m6, today))\n",
    "\n",
    "# stationarity, randomwalk, mean reversion check 1, mean reversion check 2, Linear trend, Linear Trend, Exponential trend?, auto-correlation?\n",
    "m1_s = stat_test(m1, today, lste, \"prices\", 15)\n",
    "m3_s = stat_test(m3, today, lste, \"prices\", 15)\n",
    "m6_s = stat_test(m6, today, lste, \"prices\", 15)\n",
    "\n",
    "\n",
    "\n",
    "# linear pairing mellem a og b currency\n",
    "# Are there resistance levels?\n",
    "# trend detection with returns mean\n",
    "# Trend detection with autocorrelation loss?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(\"E:/Investering/stats.xlsx\") as writer:\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            m1_c.to_excel(writer, sheet_name=\"1m corr coint\")\n",
    "        elif i == 1:\n",
    "            m1_s.to_excel(writer, sheet_name=\"1m stats\")\n",
    "        elif i == 2:\n",
    "            m3_c.to_excel(writer, sheet_name=\"3m corr coint\")\n",
    "        elif i == 3:\n",
    "            m3_s.to_excel(writer, sheet_name=\"3m stats\")\n",
    "        elif i == 4:\n",
    "            m6_c.to_excel(writer, sheet_name=\"6m corr coint\")\n",
    "        elif i == 5:\n",
    "            m6_s.to_excel(writer, sheet_name=\"6m stats\")\n",
    "            \n",
    "            \n",
    "# round(a.half_life_v2(x)/PERIOD_PER_DAY)\n",
    "\n",
    "#detect overall markov chain\n",
    "\n",
    "# detect overall mean reversion times\n",
    "\n",
    "# detect overall mean reversion sizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17886eb4-40c8-42de-a76e-4d75cf27e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, mstats\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import edge\n",
    "import edge.edge_mean_reversion as emr\n",
    "import edge.edge_risk_kit as erk\n",
    "import TMRW\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import yfinance as yf\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) #today\n",
    "m1 = datetime(today.year,today.month-1,today.day)\n",
    "m3 = datetime(today.year,today.month-3,today.day)\n",
    "m6 = datetime(today.year,today.month-5,today.day)\n",
    "one = datetime(today.year-1,today.month,today.day) #one year ago\n",
    "three = datetime(today.year-3,today.month,today.day) #one year ago\n",
    "five = datetime(today.year-5,today.month,today.day) #one year ago\n",
    "\n",
    "CUR = pd.read_excel(open('E:/Investering/Currencies.xlsx', 'rb'),sheet_name='Currencies')\n",
    "CUR = list(CUR[CUR['BSYMBOL'].notnull()]['SYMBOL'])\n",
    "lste = CUR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from binance.enums import *\n",
    "import numpy as np\n",
    "#import time\n",
    "\n",
    "\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day+1)\n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -200) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "klines = client.get_historical_klines(\"TRXUSDT\", Client.KLINE_INTERVAL_15MINUTE, day7, todays)\n",
    "data = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "data.Open = data.Open.astype(float)\n",
    "data.High = data.High.astype(float)\n",
    "data.Low = data.Low.astype(float)\n",
    "data.Close = data.Close.astype(float)\n",
    "data.Volume = data.Volume.astype(float)\n",
    "data = data[['Open','High','Low','Close','Volume']]\n",
    "\n",
    "#data = TMRW.DATA.data(Symbol,time,today)\n",
    "data['value'] = (data['Open'] + data['Close']) / 2\n",
    "data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(data['value'])['value'])\n",
    "a.insert(0,0)\n",
    "data['returns'] = a\n",
    "\n",
    "pos_neg = []\n",
    "for i in range(len(data)):\n",
    "    if data['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif data['returns'][i] < 0:\n",
    "        pos_neg.append(-1)\n",
    "\n",
    "data['pos_neg'] = pos_neg\n",
    "data['RSI'] = TMRW.FINANCE.RSI(data['value'], 1440)\n",
    "\n",
    "data['buy_signal'] = False\n",
    "data['sell_signal'] = False\n",
    "\n",
    "k = 1\n",
    "j = 1\n",
    "\n",
    "for i in range(2000, len(data)):\n",
    "    \n",
    "    if np.mean(data['pos_neg'][i-2000:i]) > 0.1:\n",
    "        next\n",
    "\n",
    "    elif np.mean(data['pos_neg'][i-2000:i]) < -0.1:\n",
    "        next\n",
    "    \n",
    "    else:\n",
    "        # generate long/buy signal    \n",
    "        if data['value'][i-20] <= 1.01 * min(data['value'][i-2000:i]) and data['value'][i-12] >= data['value'][i-24] and data['value'][i-6] >= data['value'][i]:\n",
    "            data['buy_signal'][i] = True\n",
    "            \n",
    "        #elif np.mean(data['pos_neg'][i-200:i-60]) < -0.2 and np.mean(data['pos_neg'][i-60:i]) > -0.2:\n",
    "            #data['buy_signal'][i] = True\n",
    "\n",
    "        # generate short/sell signal\n",
    "        if data['value'][i-20] >= 0.99 * max(data['value'][i-2000:i]) and data['value'][i-12] <= data['value'][i-6] and data['value'][i-6] <= data['value'][i-1] and data['RSI'][i] > 0.99 * max(data['RSI'][i-1000:i]):\n",
    "        #if i > 1 and (max(data['value'][k:i])*0.999999 < data['value'][i-1]) and data['value'][i] < data['value'][i-1]:        \n",
    "            data['sell_signal'][i] = True\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "long_signal_dates = data[data['buy_signal']].index\n",
    "for date in long_signal_dates:\n",
    "    ax.annotate('o', xy=(date, data.loc[date, 'value']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "short_signal_dates = data[data['sell_signal']].index\n",
    "for date in short_signal_dates:\n",
    "    ax.annotate('x', xy=(date, data.loc[date, 'value']), color='red', fontsize=12, ha='center')\n",
    "\n",
    "# Plot the time series\n",
    "data['value'][:].plot(figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Max-Min Strategy')\n",
    "\n",
    "val = 100\n",
    "buy = False\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['buy_signal'][i] == True and buy == False:\n",
    "        buy = True\n",
    "        buy_price = data['Open'][i]\n",
    "    \n",
    "    if data['sell_signal'][i] == True and buy == True:\n",
    "        buy = False\n",
    "        val = val * (1+(data['Close'][i] - buy_price)/buy_price) - 0.002 * data['Close'][i]\n",
    "        \n",
    "val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c6650-c2f8-4bed-9501-a3f8ec2201e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"USDCHF_Candlestick_4_Hour_ASK_05.05.2003-19.10.2019.csv\")\n",
    "df.tail()\n",
    "\n",
    "#Check if any zero volumes are available\n",
    "indexZeros = df[ df['volume'] == 0 ].index\n",
    "\n",
    "df.drop(indexZeros , inplace=True)\n",
    "df.loc[(df[\"volume\"] == 0 )]\n",
    "df.isna().sum()\n",
    "\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "#df.ta.indicators()\n",
    "#help(ta.atr)\n",
    "df['ATR'] = df.ta.atr(length=20)\n",
    "df['RSI'] = df.ta.rsi()\n",
    "df['Average'] = df.ta.midprice(length=1) #midprice\n",
    "df['MA40'] = df.ta.sma(length=40)\n",
    "df['MA80'] = df.ta.sma(length=80)\n",
    "df['MA160'] = df.ta.sma(length=160)\n",
    "\n",
    "from scipy.stats import linregress\n",
    "def get_slope(array):\n",
    "    y = np.array(array)\n",
    "    x = np.arange(len(y))\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "    return slope\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "backrollingN = 6\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df['slopeMA40'] = df['MA40'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['slopeMA80'] = df['MA80'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['slopeMA160'] = df['MA160'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['AverageSlope'] = df['Average'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "df['RSISlope'] = df['RSI'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "\n",
    "#Target flexible way\n",
    "pipdiff = 500*1e-5 #for TP\n",
    "SLTPRatio = 2 #pipdiff/Ratio gives SL\n",
    "\n",
    "def mytarget(barsupfront, df1):\n",
    "    length = len(df1)\n",
    "    high = list(df1['high'])\n",
    "    low = list(df1['low'])\n",
    "    close = list(df1['close'])\n",
    "    open = list(df1['open'])\n",
    "    trendcat = [None] * length\n",
    "    \n",
    "    for line in range (0,length-barsupfront-2):\n",
    "        valueOpenLow = 0\n",
    "        valueOpenHigh = 0\n",
    "        for i in range(1,barsupfront+2):\n",
    "            value1 = open[line+1]-low[line+i]\n",
    "            value2 = open[line+1]-high[line+i]\n",
    "            valueOpenLow = max(value1, valueOpenLow)\n",
    "            valueOpenHigh = min(value2, valueOpenHigh)\n",
    "\n",
    "            if ( (valueOpenLow >= pipdiff) and (-valueOpenHigh <= (pipdiff/SLTPRatio)) ):\n",
    "                trendcat[line] = 1 #-1 downtrend\n",
    "                break\n",
    "            elif ( (valueOpenLow <= (pipdiff/SLTPRatio)) and (-valueOpenHigh >= pipdiff) ):\n",
    "                trendcat[line] = 2 # uptrend\n",
    "                break\n",
    "            else:\n",
    "                trendcat[line] = 0 # no clear trend\n",
    "            \n",
    "    return trendcat\n",
    "\n",
    "# mytarget(barsfront to take into account, dataframe)\n",
    "df['mytarget'] = mytarget(16, df)\n",
    "df.head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (15,20))\n",
    "ax = fig.gca()\n",
    "df_model= df[['volume', 'ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', 'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope', 'mytarget']] \n",
    "df_model.hist(ax = ax)\n",
    "plt.show()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "df_up=df.RSI[ df['mytarget'] == 2 ]\n",
    "df_down=df.RSI[ df['mytarget'] == 1 ]\n",
    "df_unclear=df.RSI[ df['mytarget'] == 0 ]\n",
    "pyplot.hist(df_unclear, bins=100, alpha=0.5, label='unclear')\n",
    "pyplot.hist(df_down, bins=100, alpha=0.5, label='down')\n",
    "pyplot.hist(df_up, bins=100, alpha=0.5, label='up')\n",
    "\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n",
    "\n",
    "df_model=df_model.dropna()\n",
    "\n",
    "attributes=['ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', 'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "X = df_model[attributes]\n",
    "y = df_model[\"mytarget\"]\n",
    "\n",
    "print(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=200, weights='uniform', algorithm='kd_tree', leaf_size=30, p=1, metric='minkowski', metric_params=None, n_jobs=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Accuracy train: %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy test: %.2f%%\" % (accuracy_test * 100.0))\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "#--- How should I compare my accuracy ?\n",
    "print(df_model['mytarget'].value_counts()*100/df_model['mytarget'].count())\n",
    "\n",
    "# Random Model, gambler?\n",
    "pred_test = np.random.choice([0, 1, 2], len(y_pred_test))\n",
    "accuracy_test = accuracy_score(y_test, pred_test)\n",
    "print(\"Accuracy Gambler: %.2f%%\" % (accuracy_test * 100.0))\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print('****Train Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_train))\n",
    "print('****Test Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_test))\n",
    "\n",
    "#random sampling\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#sequential sampling\n",
    "train_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_index], X[train_index:]\n",
    "y_train, y_test = y[:train_index], y[train_index:]\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "acc_train = accuracy_score(y_train, pred_train)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print('****Train Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_train))\n",
    "print('****Test Results****')\n",
    "print(\"Accuracy: {:.4%}\".format(acc_test))\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "#plot feature importance\n",
    "plot_importance(model)\n",
    "pyplot.show()\n",
    "\n",
    "import joblib\n",
    "\n",
    "# save your ML model to disk\n",
    "filename = 'mymodel1.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "#load the model from disk\n",
    "loaded_model = joblib.load('mymodel1.sav')\n",
    "\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "import json\n",
    "from oandapyV20 import API\n",
    "import oandapyV20.endpoints.orders as orders\n",
    "from oandapyV20.contrib.requests import MarketOrderRequest\n",
    "from oanda_candles import Pair, Gran, CandleCollector\n",
    "from oandapyV20.contrib.requests import TakeProfitDetails, StopLossDetails\n",
    "\n",
    "\n",
    "import smtplib\n",
    "gmail_user = 'email@gmail.com'\n",
    "gmail_password = 'email password here'\n",
    "sent_from = gmail_user\n",
    "to = ['email@gmail.com']\n",
    "subject = 'info CHF'\n",
    "\n",
    "ModelPrediction = 0\n",
    "def some_job():\n",
    "    access_token=\"INSERT TOKEN HERE, YOU GET IT FROM YOUR OANDA ACCOUNT\"\n",
    "    collector = CandleCollector(access_token, Pair.USD_CHF, Gran.H4)\n",
    "    candles = collector.grab(2*161)\n",
    "\n",
    "    dfstream = pd.DataFrame(columns=['Open','Close','High','Low'])\n",
    "    i=0\n",
    "    for candle in candles:\n",
    "        dfstream.loc[i, ['Open']] = float(str(candle.bid.o))\n",
    "        dfstream.loc[i, ['Close']] = float(str(candle.bid.c))\n",
    "        dfstream.loc[i, ['High']] = float(str(candle.bid.h))\n",
    "        dfstream.loc[i, ['Low']] = float(str(candle.bid.l))\n",
    "        i=i+1\n",
    "\n",
    "    dfstream['Open'] = dfstream['Open'].astype(float)\n",
    "    dfstream['Close'] = dfstream['Close'].astype(float)\n",
    "    dfstream['High'] = dfstream['High'].astype(float)\n",
    "    dfstream['Low'] = dfstream['Low'].astype(float)\n",
    "\n",
    "    #dfstream['Average'] = (dfstream['High']+dfstream['Low'])/2\n",
    "    #dfstream['MA40'] = dfstream['Open'].rolling(window=40).mean()\n",
    "    #dfstream['MA80'] = dfstream['Open'].rolling(window=80).mean()\n",
    "    #dfstream['MA160'] = dfstream['Open'].rolling(window=160).mean()\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas_ta as ta\n",
    "    #attributes=['ATR', 'RSI', 'Average', \n",
    "    #'MA40', 'MA80', 'MA160', 'slopeMA40', \n",
    "    #'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "    dfstream['ATR'] = dfstream.ta.atr(length=20)\n",
    "    dfstream['RSI'] = dfstream.ta.rsi()\n",
    "    dfstream['Average'] = dfstream.ta.midprice(length=1) #midprice\n",
    "    dfstream['MA40'] = dfstream.ta.sma(length=40)\n",
    "    dfstream['MA80'] = dfstream.ta.sma(length=80)\n",
    "    dfstream['MA160'] = dfstream.ta.sma(length=160)\n",
    "\n",
    "#from scipy.stats import linregress\n",
    "#def get_slope(array):\n",
    "#    y = np.array(array)\n",
    "#    x = np.arange(len(y))\n",
    "#    slope, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "#    return slope\n",
    "\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    backrollingN = 6\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    dfstream['slopeMA40'] = dfstream['MA40'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['slopeMA80'] = dfstream['MA80'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['slopeMA160'] = dfstream['MA160'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['AverageSlope'] = dfstream['Average'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "    dfstream['RSISlope'] = dfstream['RSI'].rolling(window=backrollingN).apply(get_slope, raw=True)\n",
    "\n",
    "    #________________________________________________________________________________________________\n",
    "    X_stream = dfstream.iloc[[320]]# !!! Index takes last CLOSED candle\n",
    "    #attributes=['ATR', 'RSI', 'Average', 'MA40', 'MA80', 'MA160', \n",
    "    #'slopeMA40', 'slopeMA80', 'slopeMA160', 'AverageSlope', 'RSISlope']\n",
    "    X_model = X_stream[attributes]\n",
    "    \n",
    "    # Apply the model for predictions\n",
    "    ModelPrediction = loaded_model.predict(X_model)\n",
    "  \n",
    "    msg = str(ModelPrediction) # 0 no clear trend, 1 downtrend, 2 uptrend\n",
    "    #------------------------------------\n",
    "    # send email with \n",
    "    server = smtplib.SMTP_SSL('smtp.gmail.com', 465)\n",
    "    server.ehlo()\n",
    "    server.login(gmail_user, gmail_password)\n",
    "    server.sendmail(sent_from, to, msg)\n",
    "    server.close()\n",
    "    #________________________________________________________________________________________________\n",
    "    \n",
    "    \n",
    "    # EXECUTING ORDERS\n",
    "    accountID = \"1432-432-0000\" #use your account ID\n",
    "    client = API(access_token)\n",
    "\n",
    "    candles = collector.grab(1)\n",
    "#    for candle in candles:\n",
    "#        print(candle.bid.o)\n",
    "#        print(candle.bid.c)\n",
    "    \n",
    "    pipdiff = 500*1e-5 #for TP\n",
    "    SLTPRatio = 2 #pipdiff/Ratio gives SL\n",
    "    \n",
    "    TPBuy = float(str(candle.bid.o))+pipdiff\n",
    "    SLBuy = float(str(candle.bid.o))-(pipdiff/SLTPRatio)\n",
    "    TPSell = float(str(candle.bid.o))-pipdiff\n",
    "    SLSell = float(str(candle.bid.o))+(pipdiff/SLTPRatio)\n",
    "    \n",
    "    #Sell\n",
    "    if ModelPrediction == 1:\n",
    "        mo = MarketOrderRequest(instrument=\"USD_CHF\", units=-1000, takeProfitOnFill=TakeProfitDetails(price=TPSell).data, stopLossOnFill=StopLossDetails(price=SLSell).data)\n",
    "        r = orders.OrderCreate(accountID, data=mo.data)\n",
    "        rv = client.request(r)\n",
    "        print(rv)\n",
    "    #Buy\n",
    "    elif ModelPrediction == 2:\n",
    "        mo = MarketOrderRequest(instrument=\"USD_CHF\", units=1000, takeProfitOnFill=TakeProfitDetails(price=TPBuy).data, stopLossOnFill=StopLossDetails(price=SLBuy).data)\n",
    "        r = orders.OrderCreate(accountID, data=mo.data)\n",
    "        rv = client.request(r)\n",
    "        print(rv)\n",
    "        \n",
    "#some_job()\n",
    "\n",
    "###################################################################\n",
    "## Interval time job ##############################################\n",
    "scheduler = BlockingScheduler(job_defaults={'misfire_grace_time': 15*60})\n",
    "scheduler.add_job(some_job, 'cron', day_of_week='mon-fri', hour='*/4', minute=5, jitter=120, timezone='America/New_York')\n",
    "#scheduler.add_job(some_job, 'interval', hours=4)\n",
    "scheduler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c1536-17ed-409d-986e-fd0e7d1c8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"EURUSD_Candlestick_5_M_ASK_30.09.2019-30.09.2022.csv\")\n",
    "df\n",
    "\n",
    "df[\"Gmt time\"]=df[\"Gmt time\"].str.replace(\".000\",\"\")\n",
    "df['Gmt time']=pd.to_datetime(df['Gmt time'],format='%d.%m.%Y %H:%M:%S')\n",
    "df.set_index(\"Gmt time\", inplace=True)\n",
    "df=df[df.High!=df.Low]\n",
    "len(df)\n",
    "\n",
    "import pandas_ta as ta\n",
    "df[\"VWAP\"]=ta.vwap(df.High, df.Low, df.Close, df.Volume)\n",
    "df['RSI']=ta.rsi(df.Close, length=16)\n",
    "my_bbands = ta.bbands(df.Close, length=14, std=2.0)\n",
    "df=df.join(my_bbands)\n",
    "\n",
    "VWAPsignal = [0]*len(df)\n",
    "backcandles = 15\n",
    "\n",
    "for row in range(backcandles, len(df)):\n",
    "    upt = 1\n",
    "    dnt = 1\n",
    "    for i in range(row-backcandles, row+1):\n",
    "        if max(df.Open[i], df.Close[i])>=df.VWAP[i]:\n",
    "            dnt=0\n",
    "        if min(df.Open[i], df.Close[i])<=df.VWAP[i]:\n",
    "            upt=0\n",
    "    if upt==1 and dnt==1:\n",
    "        VWAPsignal[row]=3\n",
    "    elif upt==1:\n",
    "        VWAPsignal[row]=2\n",
    "    elif dnt==1:\n",
    "        VWAPsignal[row]=1\n",
    "\n",
    "df['VWAPSignal'] = VWAPsignal\n",
    "\n",
    "def TotalSignal(l):\n",
    "    if (df.VWAPSignal[l]==2\n",
    "        and df.Close[l]<=df['BBL_14_2.0'][l]\n",
    "        and df.RSI[l]<45):\n",
    "            return 2\n",
    "    if (df.VWAPSignal[l]==1\n",
    "        and df.Close[l]>=df['BBU_14_2.0'][l]\n",
    "        and df.RSI[l]>55):\n",
    "            return 1\n",
    "    return 0\n",
    "        \n",
    "TotSignal = [0]*len(df)\n",
    "for row in range(backcandles, len(df)): #careful backcandles used previous cell\n",
    "    TotSignal[row] = TotalSignal(row)\n",
    "df['TotalSignal'] = TotSignal\n",
    "\n",
    "df[df.TotalSignal!=0].count()\n",
    "\n",
    "import numpy as np\n",
    "def pointposbreak(x):\n",
    "    if x['TotalSignal']==1:\n",
    "        return x['High']+1e-4\n",
    "    elif x['TotalSignal']==2:\n",
    "        return x['Low']-1e-4\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df['pointposbreak'] = df.apply(lambda row: pointposbreak(row), axis=1)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "st=10400\n",
    "dfpl = df[st:st+350]\n",
    "dfpl.reset_index(inplace=True)\n",
    "fig = go.Figure(data=[go.Candlestick(x=dfpl.index,\n",
    "                open=dfpl['Open'],\n",
    "                high=dfpl['High'],\n",
    "                low=dfpl['Low'],\n",
    "                close=dfpl['Close']),\n",
    "                go.Scatter(x=dfpl.index, y=dfpl.VWAP, \n",
    "                           line=dict(color='blue', width=1), \n",
    "                           name=\"VWAP\"), \n",
    "                go.Scatter(x=dfpl.index, y=dfpl['BBL_14_2.0'], \n",
    "                           line=dict(color='green', width=1), \n",
    "                           name=\"BBL\"),\n",
    "                go.Scatter(x=dfpl.index, y=dfpl['BBU_14_2.0'], \n",
    "                           line=dict(color='green', width=1), \n",
    "                           name=\"BBU\")])\n",
    "\n",
    "fig.add_scatter(x=dfpl.index, y=dfpl['pointposbreak'], mode=\"markers\",\n",
    "                marker=dict(size=10, color=\"MediumPurple\"),\n",
    "                name=\"Signal\")\n",
    "fig.show()\n",
    "\n",
    "dfpl = df[:75000].copy()\n",
    "import pandas_ta as ta\n",
    "dfpl['ATR']=ta.atr(dfpl.High, dfpl.Low, dfpl.Close, length=7)\n",
    "#help(ta.atr)\n",
    "def SIGNAL():\n",
    "    return dfpl.TotalSignal\n",
    "\n",
    "from backtesting import Strategy\n",
    "from backtesting import Backtest\n",
    "\n",
    "class MyStrat(Strategy):\n",
    "    initsize = 0.99\n",
    "    mysize = initsize\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.signal1 = self.I(SIGNAL)\n",
    "\n",
    "    def next(self):\n",
    "        super().next()\n",
    "        slatr = 1.2*self.data.ATR[-1]\n",
    "        TPSLRatio = 1.5\n",
    "\n",
    "        if len(self.trades)>0:\n",
    "            if self.trades[-1].is_long and self.data.RSI[-1]>=90:\n",
    "                self.trades[-1].close()\n",
    "            elif self.trades[-1].is_short and self.data.RSI[-1]<=10:\n",
    "                self.trades[-1].close()\n",
    "        \n",
    "        if self.signal1==2 and len(self.trades)==0:\n",
    "            sl1 = self.data.Close[-1] - slatr\n",
    "            tp1 = self.data.Close[-1] + slatr*TPSLRatio\n",
    "            self.buy(sl=sl1, tp=tp1, size=self.mysize)\n",
    "        \n",
    "        elif self.signal1==1 and len(self.trades)==0:         \n",
    "            sl1 = self.data.Close[-1] + slatr\n",
    "            tp1 = self.data.Close[-1] - slatr*TPSLRatio\n",
    "            self.sell(sl=sl1, tp=tp1, size=self.mysize)\n",
    "\n",
    "bt = Backtest(dfpl, MyStrat, cash=100, margin=1/10, commission=0.00)\n",
    "stat = bt.run()\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ea51a-a325-4d02-9350-cb75a946cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = pd.DataFrame(0.0, index = ['3','2','1','0','-1','-2','-3'], columns = ['up', 'down', 'sum'])\n",
    "for i in range(3,len(indic)-1):\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == 3:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[0,0] = prob.iloc[0,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[0,1] = prob.iloc[0,1] + 1\n",
    "\n",
    "        prob.iloc[0,2] = prob.iloc[0,2] + 1\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == 2:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[1,0] = prob.iloc[1,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[1,1] = prob.iloc[1,1] + 1\n",
    "\n",
    "        prob.iloc[1,2] = prob.iloc[1,2] + 1\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == 1:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[2,0] = prob.iloc[2,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[2,1] = prob.iloc[2,1] + 1\n",
    "\n",
    "        prob.iloc[2,2] = prob.iloc[2,2] + 1\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == 0:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[3,0] = prob.iloc[3,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[3,1] = prob.iloc[3,1] + 1\n",
    "        \n",
    "        prob.iloc[3,2] = prob.iloc[3,2] + 1\n",
    "        \n",
    "    if sum(indic.iloc[i-3:i]) == -1:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[4,0] = prob.iloc[4,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[4,1] = prob.iloc[4,1] + 1\n",
    "        \n",
    "        prob.iloc[4,2] = prob.iloc[4,2] + 1\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == -2:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[5,0] = prob.iloc[5,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[5,1] = prob.iloc[5,1] + 1\n",
    "        \n",
    "        prob.iloc[5,2] = prob.iloc[5,2] + 1\n",
    "    \n",
    "    if sum(indic.iloc[i-3:i]) == -3:\n",
    "        if indic.iloc[i+1] == 1:\n",
    "            prob.iloc[6,0] = prob.iloc[6,0] + 1\n",
    "        elif indic.iloc[i+1] == -1:\n",
    "            prob.iloc[6,1] = prob.iloc[6,1] + 1\n",
    "        \n",
    "        prob.iloc[6,2] = prob.iloc[6,2] + 1\n",
    "            \n",
    "for i in range(len(prob)):\n",
    "    prob.iloc[i,0] = prob.iloc[i,0] / prob.iloc[i,2]\n",
    "    prob.iloc[i,1] = prob.iloc[i,1] / prob.iloc[i,2]\n",
    "    \n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ab70c-65a0-48b8-88b9-f30f5de519cc",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\n",
    "https://en.wikipedia.org/wiki/Digital_signal_processing\n",
    "\n",
    "daily return correlations\n",
    "\n",
    "PCA, GARCH, ARCH, ARIMA, ARMA\n",
    "\n",
    "\n",
    "Two comments.\n",
    "\n",
    "1. Using ONLY historical data is sub-optimal in vol prediction (i.e., a mistake) given the existence of forward looking data embedded in options, VIXes, etc. \n",
    "\n",
    "(Source: \"Sidebar: Volatility prediction and prediction in general\", pg 257 of \"Option Valuation under Stochastic Volatility II\").\n",
    "\n",
    "2. Having said that, you might want to consider the Hidden Markov Model (HMM) techniques discussed in the same cited chapter. Unlike GARCH, in many models volatility is a latent (unobserved) variable. HMM methods give tractable ways to approach the problems of smoothing, filtering, and prediction with latent states.\n",
    "\n",
    "\n",
    "Thank you. There is very important information which isn't included in the historical price, such as upcoming Non-Farm Payrolls, Rate Decisions, Dividend Payments and Company Reporting. From that perspective, a model based on past data will always be missing a trick.\n",
    "\n",
    "\n",
    "\n",
    "In the absence of any real autoregressive effects, if you sum the direct and lagged covariance, you end up with an (almost) unbiased estimate of the covariance between then two processes.  Then just divide by the 1-day standard deviations.  My estimate is that roughly 75-80% of the dependence is in the lagged return, so it's still a bit of an underestimate if you just use it.\n",
    "\n",
    "This is not the most efficient correlation estimator, but avoid longer horizon/overlapping returns unless you correct for overlap effects since the estimators are even noisier...  you may want to consider using a longer window though, 3M has a ~6.5% standard error $∼(1−ρ^2)/\\sqrt(N-1)$\n",
    " on the estimate even without the overlap effects -- the current correlation is ~65% I believe, so this is about 10% uncertainty.\n",
    "\n",
    "my experience anyway.\n",
    "\n",
    "\n",
    "\n",
    "In (academic) practice, people usually apply several basic SVJ and SVJJ models and see which does best. Solvable ones are typically \"affine jump-diffusions\": the ones you mentioned plus Bates model, plus basic SVJJ model. (By latter, I refer to the model in here, where it is called SVCJ)\n",
    "\n",
    "Best to try several to try to obtain more or less \"model independent\" conclusions. Once you've coded one, it's usually not much work to then do several."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988e762-2d93-4d45-a326-0d4d99b1313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_times(returns,100)\n",
    "mean_size_dist(returns, 10)\n",
    "\n",
    "Tickers = ['JPM','C','BAC','AAL','DAL','MSFT','INTC','NVDA','AMD','TSLA','AMZN','GOOG','META','AAPL','NFLX','SNAP','SPOT','PINS','BABA','PYPL','V','CRM','MA','AXP','ADBE','COF','ORCL','PANW','MDB','CRWD','SHOP','CSCO','IBM','DELL','GTLB','TSM','MU','QCOM','WAL','WMT','WBA']\n",
    "\n",
    "extremum_probabilities(Tickers)\n",
    "\n",
    "Tickers = ['JPM','C','BAC','AAL','DAL','MSFT','INTC','NVDA','AMD','TSLA','AMZN','GOOG','META','AAPL','NFLX','SNAP','SPOT','PINS','BABA','PYPL','V','CRM','MA','AXP','ADBE','COF','ORCL','PANW','MDB','CRWD','SHOP','CSCO','IBM','DELL','GTLB','TSM','MU','QCOM','WAL','WMT','WBA']\n",
    "\n",
    "month_probabilities(Tickers)\n",
    "\n",
    "import math\n",
    "\n",
    "s_count = np.zeros(10)\n",
    "s_bin = np.zeros(10)\n",
    "\n",
    "for t in Tickers:\n",
    "    Aktie = tf.sdata(t, twenty, today)\n",
    "    returns = tf.returns(Aktie, logs= False)['Close']\n",
    "\n",
    "    ms = mean_sizes(returns,30)\n",
    "    ms.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    for k in range(1, 29):\n",
    "        msn = ms[k]\n",
    "\n",
    "        lst = []\n",
    "        j = 0\n",
    "        for i in range(1, len(msn)):\n",
    "            if msn[i] > 0 and msn[i-1] < 0:\n",
    "                lst.append(j)\n",
    "                j = 1\n",
    "            elif msn[i] < 0 and msn[i-1] > 0:\n",
    "                lst.append(j)\n",
    "                j = 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "\n",
    "        count, _bin, ignored = plt.hist(lst, 30, density=False)\n",
    "        if k == 1:\n",
    "            counts = np.zeros(30)\n",
    "            bins = np.zeros(30)\n",
    "\n",
    "        for j in range(len(_count)):\n",
    "\n",
    "            if math.isnan(count[j]):\n",
    "                counts[j] = counts[j] + 0\n",
    "            else:\n",
    "                counts[j] = counts[j] + count[j]\n",
    "\n",
    "            bins[j] = bins[j] + _bin[j]\n",
    "    \n",
    "    bins = bins/30\n",
    "    counts = counts/30\n",
    "    count = []\n",
    "    for i in range(10):\n",
    "        s_count[i] = s_count[i] + sum(counts[3*i:3*i+3])\n",
    "        s_bin[i] = s_bin[i] + sum(bins[3*i:3*i+3])\n",
    "\n",
    "s_count = s_count/(len(Tickers) * len(returns))\n",
    "s_bin = s_bin/(len(Tickers) * 3)\n",
    "\n",
    "s_count\n",
    "\n",
    "s = 0\n",
    "for i in range(len(s_count)):\n",
    "    s = s + s_count[i] * s_bin[i]\n",
    "s*3\n",
    "\n",
    "plt.bar(s_bin, s_count, color ='maroon', \n",
    "        width = 0.4)\n",
    "plt.show()\n",
    "\n",
    "n = 100\n",
    "m = 30\n",
    "lst = list(np.linspace(-0.10, 0.10, n))\n",
    "EM = pd.DataFrame(0.0, index = lst, columns = [0])\n",
    "\n",
    "for t in Tickers:\n",
    "    EMS = pd.DataFrame(0.0, index = lst, columns = [0])\n",
    "\n",
    "    Aktie = tf.sdata(t, five, today)\n",
    "    returns = tf.returns(Aktie, logs= False)['Close']\n",
    "\n",
    "    MS = mean_sizes(returns, m) # when negative it'll draw down, when positive it will draw you up\n",
    "\n",
    "\n",
    "    #for t in Tickers:\n",
    "\n",
    "\n",
    "    for i in range(1, m-1):\n",
    "\n",
    "        for j in range(len(MS)):\n",
    "\n",
    "            for k in range(0, n):\n",
    "\n",
    "                if MS.iloc[j,i] >= lst[k-1] and MS.iloc[j,i] < lst[k]:\n",
    "                    EMS.iloc[k,0] = EMS.iloc[k,0] + 1\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(EMS)):\n",
    "        EMS.iloc[i,0] = EMS.iloc[i,0] / ((m-1) * len(MS))       \n",
    "    \n",
    "        EM.iloc[i,0] = EM.iloc[i,0] + EMS.iloc[i,0]\n",
    "\n",
    "        \n",
    "        \n",
    "for i in range(len(EM)):\n",
    "    EM.iloc[i,0] = EM.iloc[i,0] / len(Tickers)\n",
    "    \n",
    "plt.plot(EM.index, EM)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "s = 0\n",
    "for i in range(len(EM)):\n",
    "    s = s + EM.iloc[i,0] * EM.index[i]\n",
    "\n",
    "s*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfabd23-5478-4102-a470-148dacf17676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "y = data['indicator1']\n",
    "X = data.loc[:, data.columns != 'indicator1']\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Extract the 'Close' column from the data\n",
    "close_prices = data['Close']\n",
    "\n",
    "# Get the values from the 'Close' column as a NumPy array\n",
    "values = close_prices.values\n",
    "\n",
    "# Calculate the length of the training data by taking 80% of the total length of the 'values' array\n",
    "training_data_len = math.ceil(len(values) * 0.6)\n",
    "\n",
    "# Create a MinMaxScaler object and scale the values to the range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "# Split the scaled data into training and test sets\n",
    "train_data = scaled_data[0: training_data_len, :]\n",
    "test_data = scaled_data[training_data_len - 60: , : ]\n",
    "\n",
    "# Initialize empty lists for the training and test inputs and outputs\n",
    "x_train, y_train = [], []\n",
    "x_test = []\n",
    "\n",
    "# Loop through the training data and create input/output pairs\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i - 60: i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "# Convert the training inputs and outputs to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the training inputs to be 3D for use with an LSTM model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# Loop through the test data and create input sequences\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i - 60: i, 0])\n",
    "\n",
    "# Convert the test inputs to a NumPy array and reshape to be 3D\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Get the test outputs as a NumPy array from the 'values' array\n",
    "y_test = values[training_data_len:]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(layers.LSTM(100, return_sequences=False))\n",
    "model.add(layers.Dense(25))\n",
    "model.add(layers.Dense(1))\n",
    "# model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, batch_size= 1, epochs=10)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "print('Root mean squared error:', rmse)\n",
    "\n",
    "data = data.filter(['Close'])\n",
    "train = data[:training_data_len]\n",
    "validation = data[training_data_len:]\n",
    "validation['Predictions'] = predictions\n",
    "train.reset_index(inplace = True)\n",
    "validation.reset_index(inplace = True)\n",
    "\n",
    "# Visualize using Plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=train.Date, y=train.Close, mode='lines', name='Actual Price (train)'))\n",
    "fig.add_trace(go.Scatter(x=validation.Date, y=validation.Close, mode='lines', name='Actual Price (test)'))\n",
    "fig.add_trace(go.Scatter(x=validation.Date, y=validation.Predictions, mode='lines', name='Predicted price'))\n",
    "fig.update_layout(\n",
    "        title=\"Time series Forecasting using LSTM\",\n",
    "        xaxis_title=\"Date-Time\",\n",
    "        yaxis_title=\"Values\",\n",
    "        legend_title=\"Legend\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "close_prices = data2['Close'][-20:-1].copy()\n",
    "\n",
    "# Get the values from the 'Close' column as a NumPy array\n",
    "values = close_prices.values\n",
    "\n",
    "# Calculate the length of the training data by taking 80% of the total length of the 'values' array\n",
    "training_data_len = math.ceil(len(values) * 0.8)\n",
    "\n",
    "# Create a MinMaxScaler object and scale the values to the range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(values.reshape(-1, 1))\n",
    "\n",
    "# Split the scaled data into training and test sets\n",
    "train_data = scaled_data[0: training_data_len, :]\n",
    "test_data = scaled_data[training_data_len - 10: , : ]\n",
    "\n",
    "# Initialize empty lists for the training and test inputs and outputs\n",
    "x_train, y_train = [], []\n",
    "x_test = []\n",
    "\n",
    "# Loop through the training data and create input/output pairs\n",
    "for i in range(10, len(train_data)):\n",
    "    x_train.append(train_data[i - 10: i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "# Convert the training inputs and outputs to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the training inputs to be 3D for use with an LSTM model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "# Loop through the test data and create input sequences\n",
    "for i in range(10, len(test_data)):\n",
    "    x_test.append(test_data[i - 10: i, 0])\n",
    "\n",
    "# Convert the test inputs to a NumPy array and reshape to be 3D\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Get the test outputs as a NumPy array from the 'values' array\n",
    "y_test = values[training_data_len:]\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "print('Root mean squared error:', rmse)\n",
    "\n",
    "#validataion = pd.DataFrame()\n",
    "#validation['Close'] = close_prices\n",
    "#validation['Predictions'] = predictions\n",
    "#validation.reset_index(inplace = True)\n",
    "\n",
    "predictions\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from stocker.error import get\n",
    "\n",
    "\n",
    "def data(df, features=[]):\n",
    "    columns = ['Close']\n",
    "    if len(features) > 0:\n",
    "        for i in range(len(features)):\n",
    "            columns.append(features[i])\n",
    "\n",
    "    df = df[columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_lstm_input(data, steps=1):\n",
    "    samples = []\n",
    "    for i in range(steps, data.shape[0]):\n",
    "        features = []\n",
    "        for j in range(steps):\n",
    "            features.append(data[i - steps + j, :])\n",
    "        features.append(data[i, :])\n",
    "        samples.append(features)\n",
    "\n",
    "    features = []\n",
    "    for j in range(steps + 1):\n",
    "        features.append(data[-1, :])\n",
    "\n",
    "    samples.append(features)\n",
    "    samples = np.asarray(samples)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def run(df, features=[], steps=1, training=0.9, error_method='mape'):\n",
    "\n",
    "    new_df = data(df, features)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(new_df)\n",
    "    reframed = get_lstm_input(scaled, steps)\n",
    "\n",
    "    rows = round(len(df) * training)\n",
    "\n",
    "    train = reframed[:rows, :, :]\n",
    "    test = reframed[rows:, :, :]\n",
    "\n",
    "    train_x, train_y = train[:, :steps, :], train[:, steps, 0]\n",
    "    test_x, test_y = test[:, :steps, :], test[:-1, steps, 0]\n",
    "\n",
    "    # designing and fitting network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    model.fit(train_x, train_y, epochs=100, batch_size=70, verbose=0)\n",
    "\n",
    "    mod1 = rows + steps - 1\n",
    "    mod2 = rows + steps\n",
    "\n",
    "    # generate a prediction\n",
    "    prediction = model.predict(test_x)\n",
    "    new_scaled = np.copy(scaled)\n",
    "    for x in range(mod1, new_scaled.shape[0]):\n",
    "        new_scaled[x, 0] = prediction[x-mod1]\n",
    "\n",
    "    # invert normalized values\n",
    "    # for predictions\n",
    "    y_predicted = scaler.inverse_transform(new_scaled)\n",
    "    y_predicted = y_predicted[mod1:, 0]\n",
    "    # for real values\n",
    "    y = scaler.inverse_transform(scaled)\n",
    "    y = y[mod2:, 0]\n",
    "\n",
    "    finalprice = round(y_predicted[-1], 2)\n",
    "    y_predicted = y_predicted[:-1]\n",
    "\n",
    "    error = get(y, y_predicted, error_method)\n",
    "\n",
    "    result = [finalprice, error]\n",
    "\n",
    "    return result, y_predicted, new_df[-len(y):]\n",
    "\n",
    "\n",
    "from stocker.get_data import total\n",
    "from stocker.lstm import run\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def tomorrow(stock, features=None, steps=1, training=0.9, period=14, years=1, error_method='mape', plot=False):\n",
    "    \"\"\"\n",
    "    Function to predict the \"close price\" for the next day.\n",
    "\n",
    "    Arguments:\n",
    "        stock (str): stock label\n",
    "        features (list): ['Interest', 'Wiki_views', 'RSI', '%K', '%R']\n",
    "        steps (int): previous days to consider for generating the model.\n",
    "        training (float): fraction assigned for training the model\n",
    "        period (int): number of days considered for calculating indicators.\n",
    "        years (int or float): years of data to be considered\n",
    "        error_method (str): 'mape' or 'mse'\n",
    "        plot (bool): generate performance plot\n",
    "\n",
    "    Returns:\n",
    "        Result for the next business day. [price, error, date]\n",
    "    \"\"\"\n",
    "\n",
    "    if features is None:\n",
    "        features = []\n",
    "\n",
    "    # GET ALL THE DATA:\n",
    "    stock_data = total(stock, years=years, interest='Interest' in features, wiki_views='Wiki_views' in features,\n",
    "                       indicators='RSI' and '%K' and '%R' in features, period=period)\n",
    "\n",
    "    # SPLIT DATA, CREATE THE MODEL, GENERATE AND CALCULATE THE ERROR:\n",
    "    result, y_predicted, df = run(stock_data, features, steps, training, error_method)\n",
    "\n",
    "    date = (dt.datetime.today() + dt.timedelta(days=1))\n",
    "    while date.weekday() == 5 or date.weekday() == 6:\n",
    "        date = date + dt.timedelta(days=1)\n",
    "    date = date.strftime('%Y-%m-%d')\n",
    "    result.append(date)\n",
    "\n",
    "    if not plot:\n",
    "        return result\n",
    "\n",
    "    if plot:\n",
    "        dates = df.index.tolist()\n",
    "        from pandas.plotting import register_matplotlib_converters\n",
    "        register_matplotlib_converters()\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(dates, y_predicted)\n",
    "        plt.plot(dates, df.Close.tolist())\n",
    "        plt.title(stock + ' - %1.2f' % result[0] + ' - %1.3f' % result[1] + '% - ' + result[2])\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Close price (USD)')\n",
    "        plt.legend(['Predicted', 'True'])\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.show()\n",
    "\n",
    "        return result\n",
    "\n",
    "df = close_prices\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "def detect(signal, treshold = 2.0):\n",
    "    detected = []\n",
    "    for i in range(len(signal)):\n",
    "        if np.abs(signal[i]) > treshold:\n",
    "            detected.append(i)\n",
    "    return detected\n",
    "\n",
    "signal = np.copy(df.values)\n",
    "std_signal = (signal - np.mean(signal)) / np.std(signal)\n",
    "s = pd.Series(std_signal)\n",
    "s.describe(percentiles = [0.25, 0.5, 0.75, 0.95])\n",
    "\n",
    "outliers = detect(std_signal, 1.3)\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), signal)\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    signal,\n",
    "    'X',\n",
    "    label = 'outliers',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.xticks(\n",
    "    np.arange(len(signal))[::15], df['timestamp'][::15], rotation = 'vertical'\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler().fit(df[['Polarity', 'Sensitivity', 'Close_Price']])\n",
    "scaled = minmax.transform(df[['Polarity', 'Sensitivity', 'Close_Price']])\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 0], label = 'Scaled polarity')\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 1], label = 'Scaled sensitivity')\n",
    "plt.plot(np.arange(len(signal)), scaled[:, 2], label = 'Scaled closed price')\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    scaled[:, 0],\n",
    "    'X',\n",
    "    label = 'outliers polarity based on close',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(len(signal)),\n",
    "    scaled[:, 1],\n",
    "    'o',\n",
    "    label = 'outliers polarity based on close',\n",
    "    markevery = outliers,\n",
    "    c = 'r',\n",
    ")\n",
    "plt.xticks(\n",
    "    np.arange(len(signal))[::15], df['timestamp'][::15], rotation = 'vertical'\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.title('pearson correlation', y = 1.05, size = 16)\n",
    "\n",
    "mask = np.zeros_like(df.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    df.corr(),\n",
    "    mask = mask,\n",
    "    linewidths = 0.1,\n",
    "    vmax = 1.0,\n",
    "    square = True,\n",
    "    cmap = colormap,\n",
    "    linecolor = 'white',\n",
    "    annot = True,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "def df_shift(df, lag = 0, start = 1, skip = 1, rejected_columns = []):\n",
    "    df = df.copy()\n",
    "    if not lag:\n",
    "        return df\n",
    "    cols = {}\n",
    "    for i in range(start, lag + 1, skip):\n",
    "        for x in list(df.columns):\n",
    "            if x not in rejected_columns:\n",
    "                if not x in cols:\n",
    "                    cols[x] = ['{}_{}'.format(x, i)]\n",
    "                else:\n",
    "                    cols[x].append('{}_{}'.format(x, i))\n",
    "    for k, v in cols.items():\n",
    "        columns = v\n",
    "        dfn = pd.DataFrame(data = None, columns = columns, index = df.index)\n",
    "        i = 1\n",
    "        for c in columns:\n",
    "            dfn[c] = df[k].shift(periods = i)\n",
    "            i += 1\n",
    "        df = pd.concat([df, dfn], axis = 1, join_axes = [df.index])\n",
    "    return df\n",
    "\n",
    "df_new = df_shift(df, lag = 42, start = 7, skip = 7)\n",
    "df_new.shape\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize = (30, 20))\n",
    "ax = plt.subplot(111)\n",
    "plt.title('42 hours correlation', y = 1.05, size = 16)\n",
    "selected_column = [\n",
    "    col\n",
    "    for col in list(df_new)\n",
    "    if any([k in col for k in ['Polarity', 'Sensitivity', 'Close']])\n",
    "]\n",
    "\n",
    "sns.heatmap(\n",
    "    df_new[selected_column].corr(),\n",
    "    ax = ax,\n",
    "    linewidths = 0.1,\n",
    "    vmax = 1.0,\n",
    "    square = True,\n",
    "    cmap = colormap,\n",
    "    linecolor = 'white',\n",
    "    annot = True,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "def moving_average(signal, period):\n",
    "    buffer = [np.nan] * period\n",
    "    for i in range(period, len(signal)):\n",
    "        buffer.append(signal[i - period : i].mean())\n",
    "    return buffer\n",
    "\n",
    "signal = np.copy(df['Close_Price'].values)\n",
    "ma_7 = moving_average(signal, 7)\n",
    "ma_14 = moving_average(signal, 14)\n",
    "ma_30 = moving_average(signal, 30)\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "plt.plot(np.arange(len(signal)), ma_7, label = 'ma 7')\n",
    "plt.plot(np.arange(len(signal)), ma_14, label = 'ma 14')\n",
    "plt.plot(np.arange(len(signal)), ma_30, label = 'ma 30')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "num_layers = 1\n",
    "learning_rate = 0.005\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 500\n",
    "dropout_rate = 0.6\n",
    "\n",
    "dates = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "class Model:\n",
    "    def __init__(\n",
    "        self, learning_rate, num_layers, size, size_layer, forget_bias = 0.8\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(\n",
    "            self.outputs[-1],\n",
    "            size,\n",
    "            kernel_initializer = tf.glorot_uniform_initializer(),\n",
    "        )\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )\n",
    "\n",
    "minmax = MinMaxScaler().fit(\n",
    "    df[['Polarity', 'Sensitivity', 'Tweet_vol', 'Close_Price']].astype(\n",
    "        'float32'\n",
    "    )\n",
    ")\n",
    "df_scaled = minmax.transform(\n",
    "    df[['Polarity', 'Sensitivity', 'Tweet_vol', 'Close_Price']].astype(\n",
    "        'float32'\n",
    "    )\n",
    ")\n",
    "df_scaled = pd.DataFrame(df_scaled)\n",
    "df_scaled.head()\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "modelnn = Model(\n",
    "    learning_rate, num_layers, df_scaled.shape[1], size_layer, dropout_rate\n",
    ")\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())       \n",
    "        \n",
    "for i in range(epoch):\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    total_loss = 0\n",
    "    for k in range(0, (df_scaled.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x = np.expand_dims(\n",
    "            df_scaled.iloc[k : k + timestamp].values, axis = 0\n",
    "        )\n",
    "        batch_y = df_scaled.iloc[k + 1 : k + timestamp + 1].values\n",
    "        last_state, _, loss = sess.run(\n",
    "            [modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "            feed_dict = {\n",
    "                modelnn.X: batch_x,\n",
    "                modelnn.Y: batch_y,\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        total_loss += loss\n",
    "    total_loss /= df.shape[0] // timestamp\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('epoch:', i + 1, 'avg loss:', total_loss)  \n",
    "        \n",
    "def predict_future(future_count, df, dates, indices = {}):\n",
    "    date_ori = dates[:]\n",
    "    cp_df = df.copy()\n",
    "    output_predict = np.zeros((cp_df.shape[0] + future_count, cp_df.shape[1]))\n",
    "    output_predict[0] = cp_df.iloc[0]\n",
    "    upper_b = (cp_df.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    for k in range(0, (df.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    cp_df.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "    out_logits, last_state = sess.run(\n",
    "        [modelnn.logits, modelnn.last_state],\n",
    "        feed_dict = {\n",
    "            modelnn.X: np.expand_dims(cp_df.iloc[upper_b:], axis = 0),\n",
    "            modelnn.hidden_layer: init_value,\n",
    "        },\n",
    "    )\n",
    "    init_value = last_state\n",
    "    output_predict[upper_b + 1 : cp_df.shape[0] + 1] = out_logits\n",
    "    cp_df.loc[cp_df.shape[0]] = out_logits[-1]\n",
    "    date_ori.append(date_ori[-1] + timedelta(hours = 1))\n",
    "    if indices:\n",
    "        for key, item in indices.items():\n",
    "            cp_df.iloc[-1, key] = item\n",
    "    for i in range(future_count - 1):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(cp_df.iloc[-timestamp:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[cp_df.shape[0]] = out_logits[-1]\n",
    "        cp_df.loc[cp_df.shape[0]] = out_logits[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(hours = 1))\n",
    "        if indices:\n",
    "            for key, item in indices.items():\n",
    "                cp_df.iloc[-1, key] = item\n",
    "    return {'date_ori': date_ori, 'df': cp_df.values}      \n",
    "\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer\n",
    "\n",
    "predict_30 = predict_future(30, df_scaled, dates)\n",
    "predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.plot(\n",
    "    np.arange(len(predict_30['date_ori'])),\n",
    "    anchor(predict_30['df'][:, -1], 0.5),\n",
    "    label = 'predict signal',\n",
    ")\n",
    "plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scaled_polarity = (minmax.data_max_[0] * 2 - minmax.data_min_[0]) / (\n",
    "    minmax.data_max_[0] - minmax.data_min_[0]\n",
    ")\n",
    "scaled_polarity\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "\n",
    "for retry in range(3):\n",
    "    plt.subplot(3, 1, retry + 1)\n",
    "    predict_30 = predict_future(\n",
    "        30, df_scaled, dates, indices = {0: scaled_polarity}\n",
    "    )\n",
    "    predict_30['df'] = minmax.inverse_transform(predict_30['df'])\n",
    "    plt.plot(\n",
    "        np.arange(len(predict_30['date_ori'])),\n",
    "        anchor(predict_30['df'][:, -1], 0.5),\n",
    "        label = 'predict signal',\n",
    "    )\n",
    "    plt.plot(np.arange(len(signal)), signal, label = 'real signal')\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "url = '/content/gdrive/MyDrive/IIM K-Effects of crypto on economic indicators of India /datasets/bitcoin.txt'\n",
    "dat = np.genfromtxt(url, skip_header=19)\n",
    "title = 'Bitcoin'\n",
    "label = 'Bitcoin'\n",
    "units = ''\n",
    "t0 = 2014.0\n",
    "dt = 0.10  # In years\n",
    "N = dat.size\n",
    "t = numpy.arange(0, N) * dt + t0\n",
    "p = numpy.polyfit(t - t0, dat, 1)\n",
    "dat_notrend = dat - numpy.polyval(p, t - t0)\n",
    "std = dat_notrend.std()  # Standard deviation\n",
    "var = std ** 2  # Variance\n",
    "dat_norm = dat_notrend / std  # Normalized dataset\n",
    "mother = wavelet.Morlet(6)\n",
    "s0 = 4* dt  # Starting scale, in this case 2 * 0.25 years = 6 months\n",
    "dj = 1 / 12  # Twelve sub-octaves per octaves\n",
    "J = 7 / dj  # Seven powers of two with dj sub-octaves\n",
    "alpha, _, _ = wavelet.ar1(dat)  # Lag-1 autocorrelation for red noise\n",
    "\n",
    "\n",
    "wave, scales, freqs, coi, fft, fftfreqs = wavelet.cwt(dat_norm, dt, dj, s0, J,\n",
    "                                                      mother)\n",
    "\n",
    "iwave = wavelet.icwt(wave, scales, dt, dj, mother) * std\n",
    "power = (numpy.abs(wave)) ** 2\n",
    "fft_power = numpy.abs(fft) ** 2\n",
    "period = 1 / freqs\n",
    "power = (numpy.abs(wave)) ** 2\n",
    "fft_power = numpy.abs(fft) ** 2\n",
    "period = 1 / freqs\n",
    "power /= scales[:, None]\n",
    "signif, fft_theor = wavelet.significance(1.0, dt, scales, 0, alpha,\n",
    "                                         significance_level=0.95,\n",
    "                                         wavelet=mother)\n",
    "sig95 = numpy.ones([1, N]) * signif[:, None]\n",
    "sig95 = power / sig95\n",
    "glbl_power = power.mean(axis=1)\n",
    "dof = N - scales  # Correction for padding at edges\n",
    "glbl_signif, tmp = wavelet.significance(var, dt, scales, 1, alpha,\n",
    "                                        significance_level=0.95, dof=dof,\n",
    "                                        wavelet=mother)\n",
    "sel = find((period >= 2) & (period < 8))\n",
    "Cdelta = mother.cdelta\n",
    "scale_avg = (scales * numpy.ones((N, 1))).transpose()\n",
    "scale_avg = power / scale_avg  # As in Torrence and Compo (1998) equation 24\n",
    "scale_avg = var * dj * dt / Cdelta * scale_avg[sel, :].sum(axis=0)\n",
    "scale_avg_signif, tmp = wavelet.significance(var, dt, scales, 2, alpha,\n",
    "                                             significance_level=0.95,\n",
    "                                             dof=[scales[sel[0]],\n",
    "                                                  scales[sel[-1]]],\n",
    "                                             wavelet=mother)\n",
    "# Prepare the figure\n",
    "pyplot.close('all')\n",
    "pyplot.ioff()\n",
    "figprops = dict(figsize=(11, 8), dpi=72)\n",
    "fig = pyplot.figure(**figprops)\n",
    "\n",
    "# First sub-plot, the original time series anomaly and inverse wavelet\n",
    "# transform.\n",
    "ax = pyplot.axes([0.1, 0.75, 0.65, 0.2])\n",
    "ax.plot(t, iwave, '-', linewidth=1, color=[0.5, 0.5, 0.5])\n",
    "ax.plot(t, dat, 'k', linewidth=1.5)\n",
    "ax.set_title('a) {}'.format(title))\n",
    "ax.set_ylabel(r'{} [{}]'.format(label, units))\n",
    "\n",
    "# Second sub-plot, the normalized wavelet power spectrum and significance\n",
    "# level contour lines and cone of influece hatched area. Note that period\n",
    "# scale is logarithmic.\n",
    "bx = pyplot.axes([0.1, 0.37, 0.65, 0.28], sharex=ax)\n",
    "levels = [0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16]\n",
    "bx.contourf(t, numpy.log2(period), numpy.log2(power), numpy.log2(levels),\n",
    "            extend='both', cmap=pyplot.cm.viridis)\n",
    "extent = [t.min(), t.max(), 0, max(period)]\n",
    "bx.contour(t, numpy.log2(period), sig95, [-99, 1], colors='k', linewidths=2,\n",
    "           extent=extent)\n",
    "bx.fill(numpy.concatenate([t, t[-1:] + dt, t[-1:] + dt,\n",
    "                           t[:1] - dt, t[:1] - dt]),\n",
    "        numpy.concatenate([numpy.log2(coi), [1e-9], numpy.log2(period[-1:]),\n",
    "                           numpy.log2(period[-1:]), [1e-9]]),\n",
    "        'k', alpha=0.3, hatch='x')\n",
    "bx.set_title('BITCOIN')\n",
    "bx.set_ylabel('Period (years)')\n",
    "#\n",
    "Yticks = 2 ** numpy.arange(numpy.ceil(numpy.log2(period.min())),\n",
    "                           numpy.ceil(numpy.log2(period.max())))\n",
    "bx.set_yticks(numpy.log2(Yticks))\n",
    "bx.set_yticklabels(Yticks)\n",
    "\n",
    "# Third sub-plot, the global wavelet and Fourier power spectra and theoretical\n",
    "# noise spectra. Note that period scale is logarithmic.\n",
    "cx = pyplot.axes([0.77, 0.37, 0.2, 0.28], sharey=bx)\n",
    "cx.plot(glbl_signif, numpy.log2(period), 'k--')\n",
    "cx.plot(var * fft_theor, numpy.log2(period), '--', color='#cccccc')\n",
    "cx.plot(var * fft_power, numpy.log2(1./fftfreqs), '-', color='#cccccc',\n",
    "        linewidth=1.)\n",
    "cx.plot(var * glbl_power, numpy.log2(period), 'k-', linewidth=1.5)\n",
    "#cx.set_title('c) Global Wavelet Spectrum')\n",
    "cx.set_xlabel(r'Power [({})^2]'.format(units))\n",
    "cx.set_xlim([0, glbl_power.max() + var])\n",
    "cx.set_ylim(numpy.log2([period.min(), period.max()]))\n",
    "cx.set_yticks(numpy.log2(Yticks))\n",
    "cx.set_yticklabels(Yticks)\n",
    "pyplot.setp(cx.get_yticklabels(), visible=False)\n",
    "\n",
    "# Fourth sub-plot, the scale averaged wavelet spectrum.\n",
    "dx = pyplot.axes([0.1, 0.07, 0.65, 0.2], sharex=ax)\n",
    "dx.axhline(scale_avg_signif, color='k', linestyle='--', linewidth=1.)\n",
    "dx.plot(t, scale_avg, 'k-', linewidth=1.5)\n",
    "#dx.set_title('d) {}--{} year scale-averaged power'.format(2, 8))\n",
    "dx.set_xlabel('Time (year)')\n",
    "dx.set_ylabel(r'Average variance [{}]'.format(units))\n",
    "ax.set_xlim([t.min(), t.max()])\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "dat = data['Close'].shift(1)\n",
    "dat[0] = 0\n",
    "y = np.array(data['Close']).reshape(-1,1)\n",
    "X = np.array(data['acceleration']).reshape(-1,1)\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, y)\n",
    "\n",
    "xfit = np.linspace(np.min(X), np.max(X), 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xfit, yfit, color = \"r\");\n",
    "#plt.ylim(-100,100)\n",
    "\n",
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "print(\"Model R2: \", model.score(X, y))\n",
    "\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(X)\n",
    "\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())\n",
    "\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "xfit = np.linspace(0, np.max(X), 1000)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xfit, yfit, color = \"r\");\n",
    "\n",
    "print('x_train shape:', X.shape)\n",
    "print('y_train shape:', y.shape)\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)\n",
    "    \n",
    "gauss_model = make_pipeline(GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "\n",
    "gauss_model.fit(X, y)\n",
    "xfit = np.linspace(0, np.max(X), 1000)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xfit, yfit, color = \"r\")\n",
    "\n",
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]), color = \"r\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "y = data['indicator1']\n",
    "X = data.loc[:, data.columns != 'indicator1']\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "clf1 = clf1.fit(X, y)\n",
    "clf2 = clf2.fit(X, y)\n",
    "clf3 = clf3.fit(X, y)\n",
    "eclf = eclf.fit(X, y)\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Decision Tree', 'K neighbors', 'SVC', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "    \n",
    "\n",
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "reg1.fit(X, y)\n",
    "reg2.fit(X, y)\n",
    "reg3.fit(X, y)\n",
    "\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)\n",
    "\n",
    "#for clf, label in zip([reg1, reg2, reg3, ereg], ['GB Regression', 'Random Forest regression', 'Linear regression', 'Ensemble']):\n",
    "    #scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "\n",
    "xt = X[:20]\n",
    "\n",
    "pred4 = ereg.predict(xt)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pred4, \"r*\", ms=10, label=\"VotingRegressor\")\n",
    "\n",
    "plt.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.xlabel(\"training samples\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Regressor predictions and their average\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5287f-2467-431f-9bc9-523153c7ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TMRW.FINANCE\n",
    "\n",
    "Created on Mon Apr 15 22:44:02 2024\n",
    "\n",
    "Version 1.0.0 bundled on Fri Aug 09 12:00:00 2024\n",
    "\n",
    "@Author: Mark Daniel Balle Brezina\n",
    "@Contributors: Magnus Faber, Jonathan Emil Balle Brezina\n",
    "\"\"\"\n",
    "import TMRW\n",
    "import TMRW.FINANCE as tf\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import linregress\n",
    "from scipy import integrate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sdata(x, y, z):\n",
    "    # x is a stock symbol\n",
    "    # y is a start date\n",
    "    # z is an end date\n",
    "    # kan bruges på commodities, currencies og aktier\n",
    "    \n",
    "    #MSFT er en aktier der kan bruges\n",
    "    #USDEUR=X er en currency der kan bruges\n",
    "    #GC=F er guld priser\n",
    "    \n",
    "    st = pd.DataFrame()\n",
    "    t = yf.Ticker(x)\n",
    "    st = t.history(start=y, end=z)\n",
    "    st.index = pd.to_datetime(st.index).tz_localize(None)\n",
    "    return(st)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "\n",
    "long_signal_dates = DATA[DATA['CONV_H'] == 2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "\n",
    "long_signal_dates = DATA[DATA['CONV_L'] == -2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "#                        Moving averages and stds\n",
    "#                        volume weighted averages and stds\n",
    "#                        bollinger bands?\n",
    "###############################################################################\n",
    "\n",
    "# Time-weighted Moving Average\n",
    "def twa(data, window = 20, typ = \"sma\", x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving average\n",
    "    \"\"\"\n",
    "    if typ == \"sma\": # simple moving average\n",
    "        if len(data) < window+1:\n",
    "            raise ValueError(\"not enough data\")\n",
    "            \n",
    "        if x == True:\n",
    "            \n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "                if i < window:\n",
    "                    df.append(data[i])\n",
    "                else:\n",
    "                    df.append(np.mean(data[i-window:i]))\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 0]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "        elif x == False:\n",
    "\n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "\n",
    "                ma = np.zeros(window)\n",
    "\n",
    "                for j in range(1,window):\n",
    "\n",
    "                    if i < window:\n",
    "                        ma[j] = data[j]\n",
    "\n",
    "\n",
    "                    if i >= window:\n",
    "\n",
    "                        ma[j] = np.mean(data[i-j:i])\n",
    "\n",
    "                df.append(ma)\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "    elif typ == \"ema\": # exponential moving average\n",
    "\n",
    "        df = pd.DataFrame(index=data.index, dtype=np.float64)\n",
    "        df.loc[:,0] = data.ewm(span=window, min_periods=0, adjust=True, ignore_na=False).mean().values.flatten() # ????\n",
    "    \n",
    "        return(df)\n",
    "      \n",
    "#Time-Price Moving STD\n",
    "def twstd(data, window = 20, x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving standard deviation of prices\n",
    "    also called volatility\n",
    "    \"\"\"\n",
    "    if len(data) < window+1:\n",
    "        raise ValueError(\"not enough data\")\n",
    "\n",
    "    if x == True:\n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "            if i < window:\n",
    "                df.append(data[i])\n",
    "            else:\n",
    "                df.append(np.std(data[i-window:i]))\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 0]\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "    elif x == False:  \n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            mstd = np.zeros(window)\n",
    "\n",
    "            for j in range(1,window):\n",
    "\n",
    "                if i >= j:\n",
    "\n",
    "                    mstd[j] = np.std(data[i-j:i])\n",
    "\n",
    "            df.append(mstd)\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "    \n",
    "        return(df)\n",
    "   \n",
    "    \n",
    "# Volume-weighted Moving Average\n",
    "def vwa(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving average\n",
    "    \"\"\"\n",
    "    \n",
    "    vwa = np.zeros(window)\n",
    "    vwa = list(vwa)\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i < window:\n",
    "            \n",
    "            vwa[i] = data[i]\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            v_sum = sum(weights[i-window:i])\n",
    "            v_w_avg = 0\n",
    "            for j in range(window):\n",
    "                v_w_avg = v_w_avg + data[i-j] * (weights[i-j] / v_sum)\n",
    "\n",
    "            vwa.append(v_w_avg)\n",
    "    \n",
    "    df = pd.DataFrame(vwa)\n",
    "    return(df)\n",
    "\n",
    "#Volume-weighted Moving STD\n",
    "def vwstd(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving standard deviations\n",
    "    \"\"\"\n",
    "    \n",
    "    v_w_avg = 0\n",
    "    vws = np.zeros(window)\n",
    "    vws = list(vws)\n",
    "    for i in range(window, len(data)):\n",
    "    \n",
    "        v_sum = sum(weights[i-window:i])\n",
    "        v_w_std = 0\n",
    "        for j in range(window):\n",
    "            v_w_std = v_w_std + ((data[i-j] - np.mean(data[i-window:i]))**2 )* (weights[i-j] / v_sum)\n",
    "\n",
    "        vws.append(np.sqrt(v_w_std))\n",
    "    \n",
    "    df = pd.DataFrame(vws)\n",
    "    return(df)\n",
    "\n",
    "###############################################################################\n",
    "#                        Technical indicators\n",
    "#                        RSI, Stochastic Oscillator\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def RSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    delta = data.diff().dropna() # Close_now - Close_yesterday\n",
    "    delta = delta.reset_index(drop = True)\n",
    "\n",
    "    u = pd.DataFrame(np.zeros(len(delta))) # make an array of 0s for the up returns\n",
    "    u = u[0]\n",
    "    d = u.copy() # make an array of 0s for the down returns   \n",
    "\n",
    "    u[delta > 0] = delta[delta > 0] # for all the days where delta is up, transfer them to U\n",
    "    d[delta < 0] = -delta[delta < 0] # for all the days where delta is down, transfer them to D\n",
    "\n",
    "    u[u.index[window-1]] = np.mean( u[:window] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    d[d.index[window-1]] = np.mean( d[:window] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    RS = pd.DataFrame.ewm(u, com=window-1, adjust=False).mean() / pd.DataFrame.ewm(d, com=window-1, adjust=False).mean() # EMA(up) / EMA(down)\n",
    "\n",
    "    RSI_ = 100 - (100 / (1 + RS))\n",
    "    return(RSI_)\n",
    "\n",
    "def FRSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    inverse fisher transform on RSI = 0.1*(rsi-50)\n",
    "    fisher rsi = (np.exp(2*rsi)-1) / (np.exp(2*rsi)+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    RSI_ = 0.1 * (RSI(data, window) - 50)\n",
    "    F_RSI = (np.exp(2*RSI_)-1) / (np.exp(2*RSI_)+1)\n",
    "    return(F_RSI)\n",
    "\n",
    "def BB(data, window = 20, std = 2.5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, window):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(window,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-window:i])\n",
    "        std_lst[i] = np.std(data[i-window:i])\n",
    "        \n",
    "    up = mean_lst + std * std_lst\n",
    "    down = mean_lst - std * std_lst\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Upper'] = up\n",
    "    df['Lower'] = down\n",
    "    return(df)\n",
    "\n",
    "def STO(data, N=14, M=3):\n",
    "    assert 'Low' in data.columns\n",
    "    assert 'High' in data.columns\n",
    "    assert 'Close' in data.columns\n",
    "    \n",
    "    data_ = pd.DataFrame()\n",
    "    data_['low_N'] = data['Low'].rolling(N).min()\n",
    "    data_['high_N'] = data['High'].rolling(N).max()\n",
    "    data_['K'] = 100 * (data['Close'] - data_['low_N']) / \\\n",
    "        (data_['high_N'] - data_['low_N']) # The stochastic oscillator\n",
    "    data_['D'] = data_['K'].rolling(M).mean() # the slow and smoothed K\n",
    "    return data_\n",
    "\n",
    "def ADX(high, low, close, lookback):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.rolling(lookback).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha = 1/lookback).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha = 1/lookback).mean() / atr))\n",
    "    dx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = ((dx.shift(1) * (lookback - 1)) + dx) / lookback\n",
    "    adx_smooth = adx.ewm(alpha = 1/lookback).mean()\n",
    "    df['+DI'] = plus_di\n",
    "    df['-DI'] = minus_di\n",
    "    df['ADX'] = adx_smooth\n",
    "    \n",
    "    return df\n",
    "\n",
    "def MACD(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'Close':'macd'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macd':'signal'})\n",
    "    hist = pd.DataFrame(macd['macd'] - signal['signal']).rename(columns = {0:'hist'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df\n",
    "\n",
    "def SuperTrend(high, low, close, lookback, multiplier):\n",
    "    # ATR\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.ewm(lookback).mean()\n",
    "    \n",
    "    # H/L AVG AND BASIC UPPER & LOWER BAND\n",
    "    \n",
    "    hl_avg = (high + low) / 2\n",
    "    upper_band = (hl_avg + multiplier * atr).dropna()\n",
    "    lower_band = (hl_avg - multiplier * atr).dropna()\n",
    "    \n",
    "    # FINAL UPPER BAND\n",
    "    \n",
    "    final_bands = pd.DataFrame(columns = ['upper', 'lower'])\n",
    "    final_bands.iloc[:,0] = [x for x in upper_band - upper_band]\n",
    "    final_bands.iloc[:,1] = final_bands.iloc[:,0]\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i,0] = 0\n",
    "        else:\n",
    "            if (upper_band[i] < final_bands.iloc[i-1,0]) | (close[i-1] > final_bands.iloc[i-1,0]):\n",
    "                final_bands.iloc[i,0] = upper_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,0] = final_bands.iloc[i-1,0]\n",
    "    \n",
    "    # FINAL LOWER BAND\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i, 1] = 0\n",
    "        else:\n",
    "            if (lower_band[i] > final_bands.iloc[i-1,1]) | (close[i-1] < final_bands.iloc[i-1,1]):\n",
    "                final_bands.iloc[i,1] = lower_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,1] = final_bands.iloc[i-1,1]\n",
    "    \n",
    "    # SUPERTREND\n",
    "    \n",
    "    supertrend = pd.DataFrame(columns = [f'supertrend_{lookback}'])\n",
    "    supertrend.iloc[:,0] = [x for x in final_bands['upper'] - final_bands['upper']]\n",
    "    \n",
    "    for i in range(len(supertrend)):\n",
    "        if i == 0:\n",
    "            supertrend.iloc[i, 0] = 0\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] < final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] > final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] > final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] < final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "    \n",
    "    supertrend = supertrend.set_index(upper_band.index)\n",
    "    supertrend = supertrend.dropna()[1:]\n",
    "    \n",
    "    # ST UPTREND/DOWNTREND\n",
    "    \n",
    "    upt = []\n",
    "    dt = []\n",
    "    close = close.iloc[len(close) - len(supertrend):]\n",
    "\n",
    "    for i in range(len(supertrend)):\n",
    "        if close[i] > supertrend.iloc[i, 0]:\n",
    "            upt.append(supertrend.iloc[i, 0])\n",
    "            dt.append(np.nan)\n",
    "        elif close[i] < supertrend.iloc[i, 0]:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(supertrend.iloc[i, 0])\n",
    "        else:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(np.nan)\n",
    "            \n",
    "    st, upt, dt = pd.Series(supertrend.iloc[:, 0]), pd.Series(upt), pd.Series(dt)\n",
    "    upt.index, dt.index = supertrend.index, supertrend.index\n",
    "    \n",
    "    df['ST'] = st\n",
    "    df['UPT'] = upt\n",
    "    df['DT'] = dt\n",
    "    \n",
    "    return df\n",
    "\n",
    "def PRICE_plot(data, MA = False):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    #ax = plt.gca()\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    ax1.plot(data.index, data.Close, color = \"deepskyblue\")\n",
    "    #ax1.set_xticks([])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def STO_plot(data):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    ax1.plot(data.index, data.D, color = \"deepskyblue\")\n",
    "    ax1.plot(data.index, np.repeat(80,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    ax1.plot(data.index, np.repeat(20,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    plt.show()\n",
    "\n",
    "def ADX_plot(data, components = False):    \n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    if components == False:    \n",
    "        ax1.plot(data.index, data['ADX'], color = \"deepskyblue\")\n",
    "    \n",
    "    elif components == True:\n",
    "        ax1.plot(data.index, data['-DI'], color = \"r\")\n",
    "        ax1.plot(data.index, data['+DI'], color = \"g\")\n",
    "    \n",
    "    ax1.plot(data.index, np.repeat(40,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    ax1.plot(data.index, np.repeat(20,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    plt.show()\n",
    "\n",
    "def MACD_plot(prices, macd, signal, hist):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "\n",
    "    for i in range(len(prices)):\n",
    "        if str(hist[i])[0] == '-':\n",
    "            ax1.bar(prices.index[i], hist[i], color = 'gold')\n",
    "        else:\n",
    "            ax1.bar(prices.index[i], hist[i], color = 'deepskyblue')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def TRADE_plot(strategy):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(23, 12))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    ax1.plot(strategy.index, strategy['Close'], linewidth = 2, color = \"deepskyblue\")\n",
    "    \n",
    "    buy_price = strategy_df[strategy_df['position'] == 1]\n",
    "    sell_price = strategy_df[strategy_df['position'] == -1]\n",
    "    short_price = strategy_df[strategy_df['position'] == -2]\n",
    "    \n",
    "    ax1.plot(buy_price.index, buy_price['Close'], marker = 'o', color = 'lime', markersize = 8, linewidth = 0)\n",
    "    ax1.plot(sell_price.index, sell_price['Close'], marker = 'X', color = 'tomato', markersize = 8, linewidth = 0)\n",
    "    ax1.plot(short_price.index, short_price['Close'], marker = 'X', color = 'purple', markersize = 8, linewidth = 0)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# very useful   \n",
    "def SUM_plot(data, strategy):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(23, 12))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    rets = data.Close / data.Close[0]\n",
    "    \n",
    "    \n",
    "    #ax1.plot(rets.index, rets, color = 'deepskyblue', linewidth = 2)\n",
    "    #ax1.plot(data.index, rets, color = 'deepskyblue', linewidth = 2)\n",
    "    ax1.plot(strategy.index, np.repeat(1, len(strategy)), color = 'r', linewidth = 2)\n",
    "    ax1.plot(strategy.index, strategy.acc, color = 'w', linewidth = 2)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from binance.enums import *\n",
    "import numpy as np\n",
    "\n",
    "quantile = 0.05\n",
    "inter = \"1day\"\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) + timedelta(days = 1) \n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -1820) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "tick = \"ADAUSDT\"\n",
    "    \n",
    "klines = client.get_historical_klines(tick, Client.KLINE_INTERVAL_15MINUTE, day7, todays)\n",
    "DATA = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "DATA .Open = DATA.Open.astype(float)\n",
    "DATA .High = DATA.High.astype(float)\n",
    "DATA .Low = DATA.Low.astype(float)\n",
    "DATA .Close = DATA.Close.astype(float)\n",
    "DATA .Volume = DATA.Volume.astype(float)\n",
    "DATA  = DATA [['Open','High','Low','Close','Volume']]\n",
    "\n",
    "# Base data, Open, Low, High, Close, Volume, date points\n",
    "#DATA = sdata('BA','2021-09-01','2024-09-03')\n",
    "interval = 4*24\n",
    "\n",
    "#Stochastic oscillator of DATA\n",
    "DATA_S = STO(DATA, N = 20, M = 3)\n",
    "\n",
    "#ADX of DATA\n",
    "DATA_AD = ADX(DATA['High'], DATA['Low'], DATA['Close'], 20)\n",
    "\n",
    "#MACD of DATA\n",
    "DATA_MACD = MACD(DATA['Close'], 26, 12, 9)\n",
    "\n",
    "#SuperTrend of DATA\n",
    "#DATA_SUP = SuperTrend(DATA['High'], DATA['Low'], DATA['Close'], 20, 2)\n",
    "\n",
    "DATA['value'] = (DATA['High'] + DATA['Low']) / 2\n",
    "DATA.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "DATA = DATA.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['Close'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['c_returns'] = a\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['High'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['h_returns'] = a\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['Low'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['l_returns'] = a\n",
    "\n",
    "ret = []\n",
    "for i in range(len(DATA)):\n",
    "    ret.append((DATA['c_returns'][i]+DATA['h_returns'][i]+DATA['l_returns'][i])/3)       \n",
    "DATA['returns'] = ret\n",
    "\n",
    "# skal smoothes\n",
    "a = list(TMRW.FINANCE.returns(DATA['returns'])['returns'])\n",
    "if len(a) != len(DATA):\n",
    "    for i in range(len(DATA) - len(a)):\n",
    "        a.insert(0,0)\n",
    "DATA['acceleration'] = a\n",
    "DATA.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "#\n",
    "DATA['MR_10'] = DATA['returns'].rolling(window=10).mean().values.flatten()\n",
    "DATA['MR_20'] = DATA['returns'].rolling(window=20).mean().values.flatten()\n",
    "DATA['MR_50'] = DATA['returns'].rolling(window=50).mean().values.flatten()\n",
    "DATA['MR_100'] = DATA['returns'].rolling(window=100).mean().values.flatten()\n",
    "DATA['MR_150'] = DATA['returns'].rolling(window=150).mean().values.flatten()\n",
    "\n",
    "#\n",
    "DATA['AC_48'] = DATA['value'].rolling(window=8).apply(lambda x: x.autocorr())\n",
    "DATA['AC_128'] = DATA['value'].rolling(window=28).apply(lambda x: x.autocorr())\n",
    "DATA['AC_480'] = DATA['value'].rolling(window=80).apply(lambda x: x.autocorr())\n",
    "\n",
    "# pos/neg counter\n",
    "pos_neg = []\n",
    "for i in range(len(DATA)):\n",
    "    if DATA['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif DATA['returns'][i] < 0:\n",
    "        pos_neg.append(-1)        \n",
    "DATA['PN_counter'] = pos_neg\n",
    "\n",
    "\n",
    "DATA['Lower'] = list(TMRW.FINANCE.BB(DATA['Open'],20, std = 3.5)['Lower'])\n",
    "DATA['Upper'] = list(TMRW.FINANCE.BB(DATA['Open'],20, std = 3.5)['Upper'])\n",
    "\n",
    "# moving averages pris\n",
    "DATA['MA5'] = TMRW.FINANCE.twa(DATA['value'], 5*interval) # 5 day\n",
    "DATA['MA20'] = TMRW.FINANCE.twa(DATA['value'], 20*interval) # 20 day\n",
    "DATA['MA40'] = TMRW.FINANCE.twa(DATA['value'], 40*interval) # 40 day\n",
    "\n",
    "interval = 20\n",
    "\n",
    "Q95 = np.zeros(len(DATA))\n",
    "Q90 = np.zeros(len(DATA))\n",
    "Q10 = np.zeros(len(DATA))\n",
    "Q5 = np.zeros(len(DATA))\n",
    "for i in range(interval,len(DATA)):\n",
    "    Q95[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.98)\n",
    "    Q90[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.9)\n",
    "    Q10[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.1)\n",
    "    Q5[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.02)\n",
    "\n",
    "DATA['Q98'] = Q95\n",
    "DATA['Q90'] = Q90\n",
    "DATA['Q10'] = Q10\n",
    "DATA['Q2'] = Q5\n",
    "\n",
    "UD3 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "for j in range(5, len(DATA)):\n",
    "    string = \"\"\n",
    "    for k in range(5):\n",
    "        if DATA['PN_counter'][j-(5-k)] == 1:\n",
    "            string = string + \"U\"\n",
    "        elif DATA['PN_counter'][j-(5-k)] == -1:\n",
    "            string = string + \"D\"\n",
    "\n",
    "    if j > 2:\n",
    "        UD3.append(string[2:5])\n",
    "\n",
    "    if j > 4:\n",
    "        UD5.append(string)\n",
    "\n",
    "DATA['UD3'] = UD3\n",
    "DATA['UD5'] = UD5\n",
    "\n",
    "UD3_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD3_U\")\n",
    "UD3_U.index = UD3_U['Unnamed: 0']\n",
    "UD3_U = UD3_U.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD5_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD5_U\")\n",
    "UD5_U.index = UD5_U['Unnamed: 0']\n",
    "UD5_U = UD5_U.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD3_D = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD3_D\")\n",
    "UD3_D.index = UD3_D['Unnamed: 0']\n",
    "UD3_D = UD3_D.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD5_D = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD5_D\")\n",
    "UD5_D.index = UD5_D['Unnamed: 0']\n",
    "UD5_D = UD5_D.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = DATA [['Open','High','Low','Close','Volume']]\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(5, len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "                \n",
    "UD3_U['ACC'] = 0\n",
    "UD3_D['ACC'] = 0\n",
    "UD5_U['ACC'] = 0\n",
    "UD5_D['ACC'] = 0\n",
    "UD3_U['N'] = 0\n",
    "UD3_D['N'] = 0\n",
    "UD5_U['N'] = 0\n",
    "UD5_D['N'] = 0\n",
    "\n",
    "for i in range(5, len(DATA)-1):\n",
    "    \n",
    "    if DATA['Close'][i] < DATA['MA20'][i] and DATA['Close'][i] < DATA['MA40'][i] and DATA['Close'][i] > DATA['MA5'][i]: # expected to U\n",
    "        \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD3_U['UP'][DATA['UD3'][i]] > UD3_U['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_U['ACC'][DATA['UD3'][i]] = UD3_U['ACC'][DATA['UD3'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD3_U['UP'][DATA['UD3'][i]] < UD3_U['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_U['ACC'][DATA['UD3'][i]] = UD3_U['ACC'][DATA['UD3'][i]] + 1\n",
    "            \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD5_U['UP'][DATA['UD5'][i]] > UD5_U['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_U['ACC'][DATA['UD5'][i]] = UD5_U['ACC'][DATA['UD5'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD5_U['UP'][DATA['UD5'][i]] < UD5_U['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_U['ACC'][DATA['UD5'][i]] = UD5_U['ACC'][DATA['UD5'][i]] + 1\n",
    "            \n",
    "        UD3_U['N'][DATA['UD3'][i]] = UD3_U['N'][DATA['UD3'][i]] + 1\n",
    "        UD5_U['N'][DATA['UD5'][i]] = UD5_U['N'][DATA['UD5'][i]] + 1\n",
    "    \n",
    "    elif DATA['Close'][i] > DATA['MA20'][i] and DATA['Close'][i] > DATA['MA40'][i] and DATA['Close'][i] < DATA['MA5'][i]: # expected to D\n",
    "        \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD3_D['UP'][DATA['UD3'][i]] > UD3_D['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_D['ACC'][DATA['UD3'][i]] = UD3_D['ACC'][DATA['UD3'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD3_D['UP'][DATA['UD3'][i]] < UD3_D['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_D['ACC'][DATA['UD3'][i]] = UD3_D['ACC'][DATA['UD3'][i]] + 1\n",
    "            \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD5_D['UP'][DATA['UD5'][i]] > UD5_D['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_D['ACC'][DATA['UD5'][i]] = UD5_D['ACC'][DATA['UD5'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD5_D['UP'][DATA['UD5'][i]] < UD5_D['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_D['ACC'][DATA['UD5'][i]] = UD5_D['ACC'][DATA['UD5'][i]] + 1\n",
    "            \n",
    "        UD3_D['N'][DATA['UD3'][i]] = UD3_D['N'][DATA['UD3'][i]] + 1\n",
    "        UD5_D['N'][DATA['UD5'][i]] = UD5_D['N'][DATA['UD5'][i]] + 1\n",
    "\n",
    "\n",
    "for i in range(len(UD3_U)):\n",
    "    s = UD3_U.iloc[i, 0] + UD3_U.iloc[i, 1]\n",
    "    UD3_U.iloc[i, 0] = UD3_U.iloc[i, 0] / s\n",
    "    UD3_U.iloc[i, 1] = UD3_U.iloc[i, 1] / s\n",
    "    UD3_U.iloc[i, 2] = UD3_U.iloc[i, 2] / UD3_U.iloc[i, 3]\n",
    "    \n",
    "    s = UD3_D.iloc[i, 0] + UD3_D.iloc[i, 1]\n",
    "    UD3_D.iloc[i, 0] = UD3_D.iloc[i, 0] / s\n",
    "    UD3_D.iloc[i, 1] = UD3_D.iloc[i, 1] / s\n",
    "    UD3_D.iloc[i, 2] = UD3_D.iloc[i, 2] / UD3_D.iloc[i, 3]\n",
    "    \n",
    "\n",
    "for i in range(len(UD5_U)):\n",
    "    s = UD5_U.iloc[i, 0] + UD5_U.iloc[i, 1]\n",
    "    UD5_U.iloc[i, 0] = UD5_U.iloc[i, 0] / s\n",
    "    UD5_U.iloc[i, 1] = UD5_U.iloc[i, 1] / s\n",
    "    UD5_U.iloc[i, 2] = UD5_U.iloc[i, 2] / UD5_U.iloc[i, 3]\n",
    "    \n",
    "    s = UD5_D.iloc[i, 0] + UD5_D.iloc[i, 1]\n",
    "    UD5_D.iloc[i, 0] = UD5_D.iloc[i, 0] / s\n",
    "    UD5_D.iloc[i, 1] = UD5_D.iloc[i, 1] / s\n",
    "    UD5_D.iloc[i, 2] = UD5_D.iloc[i, 2] / UD5_D.iloc[i, 3]\n",
    "\n",
    "np.mean(UD5_U['ACC'])\n",
    "\n",
    "DATA['STATE'] = \"\"\n",
    "interval_size = 96\n",
    "m_i = 5\n",
    "m_i2 = 2\n",
    "for i in range(len(DATA)):\n",
    "\n",
    "    l_state = \"\"\n",
    "    m_state = \"\"\n",
    "    s_state = \"\"\n",
    "    st = \"\"\n",
    "    \n",
    "    if i <= m_i*interval_size:\n",
    "        DATA['STATE'][i] = \"\"\n",
    "\n",
    "    elif i > m_i*interval_size:\n",
    "        \n",
    "        \n",
    "        # long state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) > 0.05:\n",
    "            l_state = \"lu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) < 0:\n",
    "            l_state = \"ld\"\n",
    "\n",
    "\n",
    "        # medium state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) > 0.02:\n",
    "            m_state = \"mu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) < 0:\n",
    "            m_state = \"md\"\n",
    "\n",
    "            \n",
    "        # short state check    \n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) > 0.01:\n",
    "            s_state = \"su\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) < -0.05:\n",
    "            s_state = \"sd\"\n",
    "        \n",
    "        \n",
    "        DATA['STATE'][i] = l_state + m_state + s_state\n",
    "\n",
    "        \n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['STATE'] == \"lumusu\"].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "long_signal_dates = DATA[DATA['STATE'] == \"ldmdsd\"].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()\n",
    "\n",
    "#for i in range(len(DATA)):\n",
    "    \n",
    "    #if i > 1920:\n",
    "\n",
    "        #if np.std(DATA['Close'][i-(20*interval_size):i]) > 0.03 * np.mean(DATA['Close'][i-(20*interval_size):i]):\n",
    "            #if np.std(DATA['Close'][i-(10*interval_size):i]) > 0.03 * np.mean(DATA['Close'][i-(10*interval_size):i]): \n",
    "                #if np.std(DATA['Close'][i-(2*interval_size):i]) > 0.015 * np.mean(DATA['Close'][i-(2*interval_size):i]):\n",
    "                    #DATA['STATE'][i] = DATA['STATE'][i] + \"ST\"\n",
    "            \n",
    "            \n",
    "        #if np.std(DATA['returns'][i-(20*interval_size):i]) > 0.007:\n",
    "            #if np.std(DATA['returns'][i-(10*interval_size):i]) > 0.007:\n",
    "                #if np.std(DATA['returns'][i-(2*interval_size):i]) > 0.007:\n",
    "                    #DATA['STATE'][i] = DATA['STATE'][i] + \"ST\"\n",
    "                    \n",
    "DATA['CONV_H'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if np.sum(DATA['h_returns'][i-700:i]) > 0 and np.sum(DATA['h_returns'][i-700:i-500]) > np.sum(DATA['h_returns'][i-500:i-250]) and np.sum(DATA['h_returns'][i-250:i-125]) > np.sum(DATA['h_returns'][i-125:i-50]) and np.sum(DATA['h_returns'][i-125:i-50]) > np.sum(DATA['h_returns'][i-50:i]):\n",
    "        if np.sum(DATA['l_returns'][i-700:i]) > 0 and np.sum(DATA['l_returns'][i-700:i-500]) > np.sum(DATA['l_returns'][i-500:i-250]) and np.sum(DATA['l_returns'][i-250:i-125]) > np.sum(DATA['l_returns'][i-125:i-50]) and np.sum(DATA['l_returns'][i-125:i-50]) > np.sum(DATA['l_returns'][i-50:i]):\n",
    "            DATA['CONV_H'][i] = 2\n",
    "#\n",
    "DATA['CONV_L'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if np.sum(DATA['l_returns'][i-700:i]) < 0 and np.sum(DATA['l_returns'][i-700:i-500]) < np.sum(DATA['l_returns'][i-500:i-250]) and np.sum(DATA['l_returns'][i-250:i-125]) < np.sum(DATA['l_returns'][i-125:i-50]) and np.sum(DATA['l_returns'][i-125:i-50]) < np.sum(DATA['l_returns'][i-50:i]):\n",
    "        if np.sum(DATA['h_returns'][i-700:i]) < 0 and np.sum(DATA['h_returns'][i-700:i-500]) < np.sum(DATA['h_returns'][i-500:i-250]) and np.sum(DATA['h_returns'][i-250:i-125]) < np.sum(DATA['h_returns'][i-125:i-50]) and np.sum(DATA['h_returns'][i-125:i-50]) < np.sum(DATA['h_returns'][i-50:i]):\n",
    "            DATA['CONV_L'][i] = -2\n",
    "            \n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['CONV_H'] == 2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "long_signal_dates = DATA[DATA['CONV_L'] == -2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()\n",
    "\n",
    "DATA['MPN'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if (np.mean(DATA['PN_counter'][i-96:i]) > 0 and sum(DATA['returns'][i-96:i]) > 0) or (np.mean(DATA['PN_counter'][i-48:i]) > 0.05 and sum(DATA['returns'][i-48:i]) > 0.05):\n",
    "        DATA['MPN'][i] = 1\n",
    "    elif (np.mean(DATA['PN_counter'][i-96:i]) < 0 and sum(DATA['returns'][i-96:i]) < 0) or (np.mean(DATA['PN_counter'][i-48:i]) < -0.01 and sum(DATA['returns'][i-48:i]) < -0.01):\n",
    "        DATA['MPN'][i] = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['MPN'] == 1].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "#long_signal_dates = DATA[DATA['MPN'] == -1].index\n",
    "#for date in long_signal_dates:\n",
    "    #ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def MA_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and data['value'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and data['MA20'][i] < data['MA40'][i]:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and data['value'][i] > data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and data['MA20'][i] > data['MA40'][i]:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "\n",
    "def Q_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and sum(data['returns'][i-20:i]) < data['Q1'][i]:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and sum(data['returns'][i-20:i]) > data['Q2'][i]:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "def PN_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and np.mean(data['PN_counter'][i-30:i]) > 0:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and np.mean(data['PN_counter'][i-30:i]) < 0:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def C_signal(data, interval_size = 1):\n",
    "    signal = []\n",
    "    l_state = None\n",
    "    s_state = None\n",
    "    sqr = None\n",
    "    buy = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i <= 50 * interval_size:\n",
    "            signal.append(0)\n",
    "        \n",
    "        elif i > 50 * interval_size:\n",
    "        \n",
    "        \n",
    "            # long state checks\n",
    "            if np.mean(data['PN_counter'][i-(20 * interval_size):i]) > 0.01:\n",
    "                l_state = \"up\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(20 * interval_size):i]) < -0.01:\n",
    "                l_state = \"down\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(20 * interval_size):i]) < 0.01 and np.mean(data['PN_counter'][i-(20 * interval_size):i]) > -0.01:\n",
    "                l_state = \"flat\"\n",
    "\n",
    "            else:\n",
    "                l_state = None\n",
    "\n",
    "\n",
    "            # short state checks\n",
    "            if np.mean(data['PN_counter'][i-(7 * interval_size):i]) > 0.02:\n",
    "                s_state = \"up\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(7 * interval_size):i]) < -0.02:\n",
    "                s_state = \"down\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(7 * interval_size):i]) < 0.02 and np.mean(data['PN_counter'][i-(7 * interval_size):i]) > -0.02:\n",
    "                s_state = \"flat\"\n",
    "\n",
    "            else:\n",
    "                s_state = None\n",
    "\n",
    "\n",
    "\n",
    "            #special events\n",
    "\n",
    "            if data['Close'][i] > np.quantile(data['Close'][i-(50 * interval_size):i], 0.99) or \\\n",
    "            data['Open'][i] > np.quantile(data['Open'][i-(50 * interval_size):i], 0.99):\n",
    "                s_state = \"down\"\n",
    "                l_state = \"down\"\n",
    "            \n",
    "            elif data['Close'][i] > np.quantile(data['Close'][i-(50 * interval_size):i], 0.01) or \\\n",
    "            data['Open'][i] > np.quantile(data['Open'][i-(50 * interval_size):i], 0.01):\n",
    "                s_state = \"up\"\n",
    "                l_state = \"up\"\n",
    "                \n",
    "            #np.mean(data['PN_counter'][i-20:i]) > 0 and np.mean(data['PN_counter'][i-5:i]) > 0 \n",
    "            #np.mean(data['PN_counter'][i-20:i]) < 0 and np.mean(data['PN_counter'][i-5:i]) < 0\n",
    "\n",
    "\n",
    "            #data['Open'][i-2] > data['Open'][i-1] and data['Open'][i-1] < data['Open'][i]\n",
    "            #data['Open'][i-2] < data['Open'][i-1] and data['Open'][i-1] > data['Open'][i]\n",
    "\n",
    "            # buy signal\n",
    "            if buy == False and \\\n",
    "            l_state == \"up\" and s_state == \"up\":\n",
    "                signal.append(1)\n",
    "                buy = True\n",
    "                \n",
    "            elif buy == False and \\\n",
    "            l_state == \"down\" and s_state == \"up\":\n",
    "                signal.append(1)\n",
    "                buy = True\n",
    "\n",
    "            \n",
    "            # sell signal\n",
    "            elif buy == True and \\\n",
    "            l_state == \"up\" and s_state == \"down\" :\n",
    "                signal.append(-1)\n",
    "                buy = False\n",
    "                \n",
    "            elif buy == True and \\\n",
    "            l_state == \"down\" and s_state == \"down\" :\n",
    "                signal.append(-1)\n",
    "                buy = False\n",
    "\n",
    "            else:\n",
    "                signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "#buy\n",
    "data['Open'][i] < data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and \\\n",
    "#sell\n",
    "data['Open'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and \\\n",
    "\n",
    "def sharpe(data):\n",
    "    return (((val-100)/100)/(np.std(data['Close'])/np.mean(data['Close'])))\n",
    "\n",
    "def sharpe2(data):\n",
    "    s = data.iloc[0,0]\n",
    "    std = np.std(data.iloc[:,0])\n",
    "    s = (data.iloc[-1,0] - s) / std\n",
    "    return s\n",
    "    \n",
    "def strategy(data, signal, fee):\n",
    "\n",
    "    strategy_df = pd.DataFrame()\n",
    "    position = np.zeros(len(data))\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(signal)):\n",
    "        \n",
    "        if signal[i] == 1 and buy == False:\n",
    "            position[i-1] = 1\n",
    "            buy = True\n",
    "\n",
    "        if signal[i] == -2 and buy == False:\n",
    "            position[i-1] = -2\n",
    "            buy = True\n",
    "            \n",
    "        if signal[i] == -1 and buy == True:\n",
    "            position[i-1] = -1\n",
    "            buy = False\n",
    "\n",
    "        if signal[i] == 0:\n",
    "            position[i-1] = 0\n",
    "\n",
    "\n",
    "    strategy_df['Open'] = data['Open']\n",
    "    strategy_df['High'] = data['High']\n",
    "    strategy_df['Low'] = data['Low']\n",
    "    strategy_df['Close'] = data['Close']\n",
    "    strategy_df['Volume'] = data['Volume']    \n",
    "    strategy_df['position'] = position\n",
    "    strategy_df['acc'] = position\n",
    "    \n",
    "    acc = 1\n",
    "    #val = 1\n",
    "    strategy_df['acc'][0] = acc\n",
    "    buy_price = strategy_df['Close'][0]\n",
    "    buy = False\n",
    "    short = False\n",
    "    \n",
    "    for i in range(1,len(strategy_df)):\n",
    "        \n",
    "        tck = acc / strategy_df['Close'][i]\n",
    "\n",
    "        if strategy_df['position'][i] == 1 and buy == False and short == False:\n",
    "            buy_price = strategy_df['Close'][i]\n",
    "            acc = acc * (1-fee)\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            #val = val * (acc / buy_price) * (1-fee)\n",
    "            strategy_df['acc'][i] = tck * strategy_df['Close'][i]\n",
    "            buy = True\n",
    "        \n",
    "        elif strategy_df['position'][i] == -1 and buy == True:\n",
    "            sell_price = strategy_df['Close'][i]\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            dp = (sell_price - buy_price) * tck #/ buy_price\n",
    "            acc = (acc + dp) * (1-fee)\n",
    "            strategy_df['acc'][i] = acc\n",
    "            buy = False\n",
    "            \n",
    "        if strategy_df['position'][i] == -2 and buy == False and short == False:\n",
    "            buy_price = strategy_df['Close'][i]\n",
    "            acc = acc * (1-fee)\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            #val = val * (acc / buy_price) * (1-fee)\n",
    "            strategy_df['acc'][i] = tck * strategy_df['Close'][i]\n",
    "            short = True\n",
    "            #sit\n",
    "            \n",
    "        elif strategy_df['position'][i] == -1 and short == True:\n",
    "            sell_price = strategy_df['Close'][i]\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            dp = (buy_price - sell_price) * tck #/ buy_price\n",
    "            acc = (acc + dp) * (1-fee)\n",
    "            strategy_df['acc'][i] = acc\n",
    "            short = False\n",
    "\n",
    "        else:\n",
    "            if buy == True:\n",
    "                strategy_df['acc'][i] = acc #+ (strategy_df['Close'][i] - buy_price) / buy_price\n",
    "            elif buy == False:\n",
    "                strategy_df['acc'][i] = acc #+ (strategy_df['Close'][i] - sell_price)*tck #/ sell_price\n",
    "    \n",
    "    return strategy_df\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def C_signal(data):\n",
    "    \n",
    "    buy = False\n",
    "    signal = [] # 1 long, -2 short, -1 close position\n",
    "    market_state = \"None\"\n",
    "    temp_state = \"None\"\n",
    "    prev_state = \"None\"\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "            \n",
    "        \n",
    "            if data['STATE'][i] == \"lumusu\" or data['STATE'][i] == \"lumusuST\":\n",
    "                market_state = \"UP\"\n",
    "                j = i\n",
    "                \n",
    "            elif data['STATE'][i]  == \"ldmdsd\" or data['STATE'][i]  == \"ldmdsdST\":\n",
    "                market_state = \"DOWN\"\n",
    "                j = i\n",
    "                \n",
    "            elif data['STATE'][i]  == \"ee\" or data['STATE'][i]  == \"eeST\":\n",
    "                market_state = \"EE\"\n",
    "                j = i\n",
    "                \n",
    "            elif i > j + 4*74 and data['STATE'][i] not in ['lumusu','lumusuST','ldmdsd','ldmdsdST','ee','eeST']:\n",
    "                market_state = \"FLAT\"\n",
    "                j = i\n",
    "            \n",
    "            if market_state != prev_state:\n",
    "                prev_state = temp_state\n",
    "            \n",
    "            \n",
    "            if market_state == \"UP\":\n",
    "                if buy == False and np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(1)\n",
    "                    \n",
    "                elif buy == True and np.mean(data['Open'][i-96:i-50]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(-1)\n",
    "                    \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "                \n",
    "                \n",
    "            if market_state == \"DOWN\":\n",
    "                if buy == True and np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(-1)\n",
    "                    buy = False\n",
    "                    \n",
    "                #elif buy == False and np.mean(data['Open'][i-96:i-76]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                    #signal.append(-2)\n",
    "                    #buy = True\n",
    "                    \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "                \n",
    "            \n",
    "            if market_state == \"EE\":\n",
    "                signal.append(0)\n",
    "                #if prev_state == \"UP\" and buy == True:\n",
    "                    #signal.append(-1)\n",
    "                    #buy = False\n",
    "                        \n",
    "                #elif prev_state == \"DOWN\" and buy == False:\n",
    "                    #signal.append(1)\n",
    "                    #buy = True\n",
    "                    \n",
    "                #else:\n",
    "                    #signal.append(0)\n",
    "                \n",
    "                \n",
    "            if market_state == \"FLAT\":\n",
    "                \n",
    "                if buy == False and data['Close'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and data['MA20'][i] < data['MA40'][i]:\n",
    "                    if np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                        signal.append(1)\n",
    "                        buy = True\n",
    "                \n",
    "                elif buy == True and data['Close'][i] > data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and data['MA20'][i] > data['MA40'][i]:\n",
    "                    if np.mean(data['Open'][i-96:i-50]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                        signal.append(-1)\n",
    "                        buy = False\n",
    "                        \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "            \n",
    "            temp_state = market_state\n",
    "                        \n",
    "    return(signal)\n",
    "\n",
    "R = np.zeros(len(DATA))\n",
    "R2 = RSI(DATA['Close'])\n",
    "for i in range(20, len(DATA)-1):\n",
    "    R[i] = R2[i]\n",
    "DATA['RSI'] = R\n",
    "\n",
    "def UP_COND(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(3 * interval_size):i]) > 0.01 and sum(data['returns'][i-(3 * interval_size):i]) > 0.01 and data['acceleration'][i] > 1/100000000:\n",
    "        return True\n",
    "    \n",
    "def UP_COND2(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(20 * interval_size):i]) > 0.005 and sum(data['returns'][i-(20 * interval_size):i]) > 0.005:\n",
    "        return True\n",
    "\n",
    "def O_TANG(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i]) > np.mean(data['Close'][i-(2 * interval_size):i]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i]) < np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "        \n",
    "def ma_cond1(data, i):\n",
    "    if data['Close'][i] > data['MA5'][i] and data['Close'][i] < data['MA20'][i] and data['Close'][i] < data['MA40'][i]:\n",
    "        return True\n",
    "    \n",
    "if np.mean(data['Close'][i-(3 * interval_size):i-(1 * interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]) and data['Close'][i] < np.mean(data['Close'][i-(1 * interval_size):i]) \n",
    "\n",
    "def UP_SELL(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(3 * interval_size):i]) < -0.01 and np.mean(data['returns'][i-(3 * interval_size):i]) < 0:\n",
    "        return True\n",
    "    \n",
    "def DOWN_COND(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(20 * interval_size):i]) < 0 and sum(data['returns'][i-(20 * interval_size):i]) < 0:\n",
    "        if np.mean(data['PN_counter'][i-(3 * interval_size):i]) < -0.01 and sum(data['returns'][i-(3 * interval_size):i]) < -0.01:\n",
    "            return True\n",
    "        \n",
    "if np.mean(data['Close'][i-(3 * interval_size):i-(2 * interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]):\n",
    "            if np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]) > np.mean(data['Close'][i-(1 * interval_size):i]):\n",
    "                #if np.mean(data['Close'][i-(1 * interval_size):i]) < np.mean(data['Close'][i-(3 * interval_size):i-(2 * interval_size)]):\n",
    "                \n",
    "def MA_SELL_COND(data, i):\n",
    "    if data['Close'][i] < data['MA5'][i] and data['Close'][i] > data['MA20'][i] and data['Close'][i] > data['MA40'][i]:\n",
    "        return True\n",
    "    \n",
    "def IO_TANG(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i]) < np.mean(data['Close'][i-(2 * interval_size):i]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i]) > np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "        \n",
    "def IO_TANG2(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i-(2*interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1*interval_size)]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i-(1*interval_size)]) > np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def A_signal(data, interval_size = 1):\n",
    "    signal = np.zeros(len(data))\n",
    "    buy = False\n",
    "    buy_price = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i > 100: #* interval_size:\n",
    "        \n",
    "            #if data['STATE'][i] == \"lumusu\":\n",
    "\n",
    "                #if np.mean(data['PN_counter'][i-(10 * interval_size):i]) > 0.01 and np.mean(data['PN_counter'][i-(3 * interval_size):i]) > -0.02 and sum(data['returns'][i-(20 * interval_size):i]) > 0.005:\n",
    "\n",
    "            if buy == False and ma_cond1(data,i) == True:#UP_COND(data, i, interval_size) == True and  UP_COND2(data,i, interval_size) == True and O_TANG(data, i, interval_size) == True:\n",
    "                signal[i] = 1\n",
    "                buy = True\n",
    "                buy_price = data['Close'][i]\n",
    "\n",
    "                #elif buy == False and data['Close'][i-24] == min(data['Close'][200:i]):\n",
    "                    #signal[i] = 1\n",
    "                    #buy = True\n",
    "                    #buy_price = data['Close'][i]\n",
    "\n",
    "                #if buy == False and DOWN_COND(data, i , interval_size) == True:\n",
    "                    #signal[i] = -1\n",
    "                    #buy = True\n",
    "                    #buy_price = data['Close'][i]\n",
    "\n",
    "\n",
    "            if buy == True and MA_SELL_COND(data,i) == True and (DATA['Close'][i] > 1.05 * buy_price or DATA['Close'][i] < 0.98 * buy_price):#UP_SELL(data,i,interval_size):\n",
    "                #if IO_TANG(data,i,interval_size) == True or IO_TANG2(data,i, interval_size) == True:\n",
    "                signal[i] = -1\n",
    "                buy = False\n",
    "\n",
    "            #elif data['Close'][i] > 1.5 * buy_price and buy == True:\n",
    "                #signal[i] = -1\n",
    "                #buy = False\n",
    "\n",
    "            elif data['Close'][i] < 0.95 * buy_price and buy == True:\n",
    "                signal[i] = -1\n",
    "                buy = False\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "sig_ = A_signal(DATA,96)\n",
    "strategy_df = strategy(DATA, sig_, 0.005)\n",
    "\n",
    "#price curve\n",
    "#PRICE_plot(DATA)\n",
    "# stochastic oscillator plot\n",
    "#STO_plot(DATA_S)\n",
    "# Average directional index plot\n",
    "#ADX_plot(DATA_AD)\n",
    "\n",
    "#MACD_plot(DATA.Close, DATA_MACD['macd'], DATA_MACD['signal'], DATA_MACD['hist'])\n",
    "\n",
    "print(np.std(strategy_df.acc),(strategy_df['acc'][len(strategy_df)-1]-1))\n",
    "\n",
    "TRADE_plot(strategy_df)\n",
    "\n",
    "SUM_plot(DATA, strategy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15fc42-185c-4100-be78-adfb915bec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "\n",
    "from arch import arch_model\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def log_ret(s):                # daily log-return\n",
    "    return np.log(s/ s.shift(1))\n",
    "\n",
    "def realised_vol(ser, w=21):   # close-to-close σ_annualised\n",
    "    return log_ret(ser).rolling(w).std() * np.sqrt(252)\n",
    "\n",
    "def parkinson(df, w=21):\n",
    "    rs = (1.0 / (4.0 * np.log(2))) * (np.log(df[\"High\"] / df[\"Low\"]))**2\n",
    "    f = lambda x: np.sqrt(252 * x.mean())\n",
    "    return rs.rolling(w).apply(f, raw=False)\n",
    "\n",
    "def garman_klass(df, w=21):\n",
    "    log_hl = np.log(df[\"High\"] / df[\"Low\"])\n",
    "    log_co = np.log(df[\"Close\"] / df[\"Open\"])\n",
    "    rs = 0.5 * log_hl**2 - (2 * np.log(2) - 1) * log_co**2\n",
    "    f = lambda x: np.sqrt(252 * x.mean())\n",
    "    return rs.rolling(w).apply(f, raw=False)\n",
    "\n",
    "def ewma_sigma(ret, lam=0.94):\n",
    "    ewma = np.zeros_like(ret)\n",
    "    ewma[0] = ret.var()\n",
    "    for t in range(1, len(ret)):\n",
    "        ewma[t] = lam * ewma[t-1] + (1 - lam) * ret.iloc[t-1]**2\n",
    "    return np.sqrt(ewma * 252)\n",
    "\n",
    "def garch_forecast(ret):\n",
    "    am = arch_model(ret * 100, p=1, q=1, mean=\"zero\")\n",
    "    res = am.fit(disp=\"off\")\n",
    "    # 1-step ahead forecast (annualised)\n",
    "    return np.sqrt(res.forecast(horizon=1).variance.values[-1,0] * 252) / 100\n",
    "\n",
    "\n",
    "# HAR-RV (daily/weekly/monthly realised σ)\n",
    "\n",
    "\n",
    "k = 20\n",
    "for i in range(k+1, len(data)):\n",
    "    hist, _ = np.histogram(data['close_price'].pct_change().iloc[i-k:i], bins=10, density=True)\n",
    "    data.loc[i, 'Entropy'] = entropy(hist + 1e-6)  # avoid zero probabilities\n",
    "    \n",
    "if entropy is low the low volatility\n",
    "if entropy is high than high volatility\n",
    "\n",
    "# 5-10 short version\n",
    "# 20, 63 - 253 for regimes\n",
    "\n",
    "# standardise, log-transform, ...\n",
    "\n",
    "# absolute returns, squared returns, intraday realised variance\n",
    "# implied vol - VIX, ATM-IV, 25 Δ risk-reversal, DVOL (BTC)\n",
    "\n",
    "t = yf.Ticker(\"TSLA\")\n",
    "data = t.history(period='10y')\n",
    "data = data.reset_index(drop = True)\n",
    "data = data[['Open', 'Low', 'High','Close', 'Volume']]\n",
    "data['ITR'] = (data['High'] - data['Low']) / (data['Close'] - data['Open'])\n",
    "data['ITR'].loc[data['ITR'].isna()] = 0\n",
    "data['ITR'][np.isinf(data['ITR'])] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44b337-617a-4ae4-ba95-256e33786c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['returns'] = data.Close.diff()/data.Close.shift(1)\n",
    "data['returns'].loc[data['returns'].isna()] = 0\n",
    "\n",
    "# log transform\n",
    "data['log returns'] = log_ret(data.Close)\n",
    "state = []\n",
    "for i in range(len(data)):\n",
    "    if i < 20:\n",
    "        state.append(\"low\")\n",
    "    \n",
    "    else:\n",
    "        if abs(data['returns'][i]) >= 1.25 * np.mean(abs(data['returns'][0:i])):\n",
    "            state.append(\"high\")\n",
    "        elif abs(data['returns'][i]) < 1.25 * np.mean(abs(data['returns'][0:i])) and abs(data['returns'][i]) >= 0.75 * np.mean(abs(data['returns'][0:i])):\n",
    "            state.append(\"medium\")\n",
    "        else:\n",
    "            state.append(\"low\")\n",
    "\n",
    "data[\"r_state\"] = state\n",
    "\n",
    "data['acc'] = data.returns.diff()/data.returns.shift(1)\n",
    "data['acc'].loc[data['acc'].isna()] = 0\n",
    "data['acc'][np.isinf(data['acc'])] = 0\n",
    "\n",
    "state = []\n",
    "for i in range(len(data)):\n",
    "    if i < 20:\n",
    "        state.append(\"neutral\")\n",
    "    else:\n",
    "        if np.mean(data['acc'][i-20:i]) >= 2.5:\n",
    "            state.append(\"positive\")\n",
    "        elif np.mean(data['acc'][i-20:i]) < -2.5:\n",
    "            state.append(\"negative\")\n",
    "        else:\n",
    "            state.append(\"neutral\")\n",
    "\n",
    "data[\"acc_state\"] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777093c-b8db-4c08-9f4f-ba1f38a04eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a81228-1af2-4bc9-8628-e5271d5b4f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e11d3-b192-48e9-b34e-b761772226df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total momentum\\\n",
    "#measure the velocity of its center of mass\\\n",
    "#the direction of the total momentum vector\\\n",
    "#total (mass- or energy-weighted) momentum vector\n",
    "\n",
    "def momentum(data):\n",
    "    # take order distributions, volume weight them on a closed interval, like a day or an hour. ...\n",
    "    # order distribution\n",
    "    # v = returns between steps and on bid-ask spread\n",
    "    # mass = order size\n",
    "    # integrate over interval.\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f461c-d28b-4a64-bd70-321100198ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns\n",
    "data['returns'] = data.close_price.diff()/data.close_price.shift(1)\n",
    "data['returns'].loc[data['returns'].isna()] = 0\n",
    "\n",
    "# log transform of returns\n",
    "data['log returns'] = log_ret(data.close_price)\n",
    "\n",
    "# acceleration\n",
    "data['acc'] = data.returns.diff()/data.returns.shift(1)\n",
    "data['acc'].loc[data['acc'].isna()] = 0\n",
    "data['acc'][np.isinf(data['acc'])] = 0\n",
    "\n",
    "# smoothed acceleration\n",
    "data['acc6'] = data.acc.rolling(window=6).mean()\n",
    "data['acc20'] = data.acc.rolling(window=20).mean()\n",
    "data['acc40']= data.acc.rolling(window=40).mean()\n",
    "\n",
    "# smoothed price\n",
    "data['ma6'] = data.close_price.rolling(window=6).mean()\n",
    "data['ma20'] = data.close_price.rolling(window=20).mean()\n",
    "data['ma40']= data.close_price.rolling(window=40).mean()\n",
    "data['ma120'] = data.close_price.rolling(window=120).mean()\n",
    "\n",
    "# Compute MACD line and Signal line\n",
    "short_ema = data['close_price'].ewm(span=12, adjust=False).mean()\n",
    "long_ema  = data['close_price'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = short_ema - long_ema\n",
    "data['Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# ???\n",
    "#avg_gain = gain.ewm(alpha=1/period, min_periods=period, adjust=False).mean()\n",
    "#avg_loss = loss.ewm(alpha=1/period, min_periods=period, adjust=False).mean()\n",
    "\n",
    "# ITR - interday true range\n",
    "data['ITR'] = (data['high_price'] - data['low_price']) / (data['close_price'] - data['open_price'])\n",
    "data['ITR'].loc[data['ITR'].isna()] = 0\n",
    "data['ITR'][np.isinf(data['ITR'])] = 0\n",
    "\n",
    "# RSI - relative strength index\n",
    "data['RSI'] = RSI(data)\n",
    "\n",
    "# ROC - rate of change\n",
    "n = 24  # e.g. 24-hour lookback\n",
    "data['ROC'] = (data.close_price - data.close_price.shift(n)) / data.close_price.shift(n) * 100\n",
    "\n",
    "# timeseries momentum\n",
    "data['t-momentum'] = compute_time_series_momentum(data['close_price'])\n",
    "data['velocity'] = data['close_price'].diff()\n",
    "data['KineticMomentum'] = 0.5 * data['volume'] * (data['velocity']**2)\n",
    "n = 12  # 12-hour momentum\n",
    "data['MomentumSignal'] = np.sign(data.close_price/ data.close_price.shift(n) - 1)\n",
    "\n",
    "n = 24\n",
    "data['volatility'] = data['returns'].rolling(window=n).std()\n",
    "data['VolAdjMomentum'] = data['returns'] / data['volatility']\n",
    "\n",
    "# Stoch OSC\n",
    "data['%K'] = stoch_osc_k(data)\n",
    "data['%D'] = stoch_osc_d(data)\n",
    "\n",
    "\n",
    "\n",
    "# order-pressure 1\n",
    "lst = [0]\n",
    "for i in range(1,len(data)):\n",
    "    if (data['Taker_buy_base_asset_volume'][i])/ data['volume'][i] >= 0.5:\n",
    "        lst.append(1)\n",
    "    else:\n",
    "        lst.append(0)\n",
    "\n",
    "data['order pressure'] = lst\n",
    "data['order pressure'].loc[data['order pressure'].isna()] = 0\n",
    "\n",
    "# order-pressure distribution\n",
    "lst = [0]\n",
    "for i in range(1,len(data)):\n",
    "    ret_buy = (data['low_price'][i] - data['close_price'][i-1]) / data['close_price'][i-1]\n",
    "    ret_sell = (data['high_price'][i] - data['close_price'][i-1]) / data['close_price'][i-1]\n",
    "    buy_press = (data['Taker_buy_base_asset_volume'][i])/ data['volume'][i]\n",
    "    sell_press = (data['volume'][i] - data['Taker_buy_base_asset_volume'][i])/data['volume'][i]\n",
    "    \n",
    "    lst.append(ret_buy * buy_press + ret_sell * sell_press)\n",
    "    \n",
    "data['order-pressure distribution'] = lst\n",
    "data['order-pressure distribution'].loc[data['order-pressure distribution'].isna()] = 0\n",
    "\n",
    "\n",
    "\n",
    "# ARIMA forecast of price\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "    \n",
    "## wavelet\n",
    "coeffs = pywt.wavedec(data['close_price'], 'db4', level=4)   # decompose\n",
    "# Zero out the smallest-scale detail coefficients (index -1 or -2)\n",
    "coeffs_filtered = coeffs[:-2] + [None]*2\n",
    "trend = pywt.waverec(coeffs_filtered, 'db4')\n",
    "data['WaveletTrend'] = trend[6:]\n",
    "\n",
    "## HilbertPhase\n",
    "analytic = hilbert(data['close_price'])\n",
    "data['HilbertPhase'] = np.unwrap(np.angle(analytic))\n",
    "\n",
    "\n",
    "\n",
    "up_move = df['high'].diff()\n",
    "down_move = -df['low'].diff()\n",
    "plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)\n",
    "minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)\n",
    "tr = TrueRange(high, low, close)  # classic ATR’s True Range\n",
    "plus_di = 100 * ema(plus_dm, period) / ema(tr, period)\n",
    "minus_di = 100 * ema(minus_dm, period) / ema(tr, period)\n",
    "dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "df['ADX'] = ema(dx, period)\n",
    "\n",
    "# OBV\n",
    "#df['OBV'] = 0\n",
    "#for i in range(1, len(df)):\n",
    "    #if df['close'].iloc[i] > df['close'].iloc[i-1]:\n",
    "        #df['OBV'].iloc[i] = df['OBV'].iloc[i-1] + df['volume'].iloc[i]\n",
    "    #elif df['close'].iloc[i] < df['close'].iloc[i-1]:\n",
    "        #df['OBV'].iloc[i] = df['OBV'].iloc[i-1] - df['volume'].iloc[i]\n",
    "    #else:\n",
    "        #df['OBV'].iloc[i] = df['OBV'].iloc[i-1]\n",
    "\n",
    "\n",
    "\n",
    "## COG, useful version of MA\n",
    "n = 20\n",
    "prices = data['close_price']\n",
    "weights = np.arange(1, n+1)\n",
    "data['COG'] = (prices.rolling(n).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True))\n",
    "data['COG'].loc[data['COG'].isna()] = 0\n",
    "\n",
    "lst = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    mom = 0\n",
    "    \n",
    "    #MA indicators\n",
    "    if ((data.close_price[i] > data.ma40[i]) & (data.close_price[i] > data.ma120[i])).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    \n",
    "    elif ((data.close_price[i] < data.ma40[i]) & (data.close_price[i] < data.ma120[i])).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "    \n",
    "    #MACD\n",
    "    if (data.MACD[i] > data.Signal[i]).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    elif (data.MACD[i] < data.Signal[i]).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "    \n",
    "    # RSI\n",
    "    if (data['RSI'][i] < 30).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    \n",
    "    elif (data['RSI'][i] > 70).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "    \n",
    "    # Stoch osc\n",
    "    if ((data['%D'][i] > 80) & (abs(data['%D'][i] - data['%K'][i]) < 30)).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    elif ((data['%D'][i] < 20) & (abs(data['%D'][i] - data['%K'][i]) < 30)).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "        \n",
    "    # ROC\n",
    "    if (data['ROC'][i] > 2.5).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    \n",
    "    elif (data['ROC'][i] < -2.5).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "        \n",
    "    # volatility adjusted momentum\n",
    "    if (data['VolAdjMomentum'][i] < -1).astype(int) == 1:\n",
    "        mom = mom + 1\n",
    "    \n",
    "    elif (data['VolAdjMomentum'][i] > 1).astype(int) == 1:\n",
    "        mom = mom - 1\n",
    "    \n",
    "    # Momentum signal\n",
    "    if data.MomentumSignal[i] == 1:\n",
    "        mom = mom + 1\n",
    "    elif data.MomentumSignal[i] == -1:\n",
    "        mom = mom - 1\n",
    "        \n",
    "    # Timeseries momentum\n",
    "    if data[\"t-momentum\"][i] == 1:\n",
    "        mom = mom + 1\n",
    "    elif data[\"t-momentum\"][i] == -1:\n",
    "        mom = mom - 1\n",
    "        \n",
    "        \n",
    "    if (data['KineticMomentum'][i] > 0.005).astype(int) == 1:\n",
    "        if data['returns'][i] > 0:\n",
    "            mom = mom + 1\n",
    "        elif data['returns'][i] < 0:\n",
    "            mom = mom - 1\n",
    "    \n",
    "    elif (data['KineticMomentum'][i] < 0.0001).astype(int) == 1:\n",
    "        next\n",
    "        \n",
    "    lst.append(mom)\n",
    "data['state'] = lst\n",
    "\n",
    "#plt.figure(figsize = (14,8))\n",
    "\n",
    "plt.plot(data.index, data.close_price, color = \"black\", linewidth = 2) \n",
    "#plt.plot(data.index, data.COG, linewidth = 0.5, color = \"red\")\n",
    "#plt.plot(data.index, data.ma20, linewidth = 0.5, color = \"blue\")\n",
    "#plt.plot(data.index, data.HilbertPhase, linewidth = 0.5)\n",
    "#plt.xlim(100,600)\n",
    "plt.show()\n",
    "\n",
    "lst = [0,0,0,0,0]\n",
    "for i in range(5,len(data)):\n",
    "    model = ARIMA(data['close_price'][i-3:i], order=(1,1,1)).fit()\n",
    "    forecast = model.forecast(steps=1)\n",
    "    lst.append(list(forecast)[0])\n",
    "    \n",
    "data['forecast'] = lst\n",
    "\n",
    "#plt.figure(figsize = (14,8))\n",
    "\n",
    "#plt.plot(data.index, data.close_price, color = \"black\", linewidth = 2) \n",
    "#plt.plot(data.index, data['Taker_buy_base_asset_volume']/ data['volume'], color = \"red\", linewidth = 1)\n",
    "plt.plot(data.index, data[\"order-pressure distribution\"], color = \"blue\", linewidth = 1) \n",
    "\n",
    "#plt.plot(data.index, data.COG, linewidth = 0.5, color = \"red\")\n",
    "#plt.plot(data.index, data.HilbertPhase, linewidth = 0.5)\n",
    "\n",
    "plt.xlim(100,350)\n",
    "#plt.ylim(0.022,0.03)\n",
    "plt.show()\n",
    "\n",
    "np.mean(data[\"order-pressure distribution\"][720:760])*100 - 0.05\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "\n",
    "plt.plot(data.index, data.close_price, color = \"black\", linewidth = 2) \n",
    "\n",
    "#data.long_buy = ( (data.close_price < data.ma40) & (data.close_price < data.ma120)).astype(int)  # 1=bull, 0=bear\n",
    "#data.long_buy = ((data['Taker_buy_base_asset_volume']/ data['volume']) > 0.6).astype(int)\n",
    "#data.long_buy = (data[\"order-pressure distribution\"] > 0.005).astype(int)\n",
    "data.long_buy = (data[\"state\"] < 0).astype(int)\n",
    "b = data[data.long_buy == 1]\n",
    "plt.plot(b.index, b.close_price, \"go\", markersize = 3)\n",
    "\n",
    "#data.long_sell = ((data.close_price > data.ma40) & (data.close_price > data.ma120)).astype(int)  # 1=bull, 0=bear\n",
    "#data.long_sell = ((data['Taker_buy_base_asset_volume']/ data['volume']) < 0.4).astype(int)\n",
    "#data.long_sell = (data[\"order-pressure distribution\"] < 0).astype(int)\n",
    "data.long_sell = (data[\"state\"] > 0).astype(int)\n",
    "a = data[data.long_sell == 1]\n",
    "plt.plot(a.index, a.close_price, \"ro\", markersize = 3) \n",
    "\n",
    "\n",
    "\n",
    "#plt.plot(data.index, data.ma6, linewidth = 0.5)\n",
    "#plt.plot(data.index, data.ma20, linewidth = 0.5)\n",
    "#plt.plot(data.index, data.ma40, linewidth = 0.5)\n",
    "#plt.xlim(720,760)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40035857-882b-41da-bd8b-64dbe5e5d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------\n",
    "# helper functions\n",
    "# ----------------------------\n",
    "def total_momentum(masses, velocities):\n",
    "    \"\"\"\n",
    "    masses: (N,) array\n",
    "    velocities: (N, d) array\n",
    "    returns:\n",
    "        P : (d,) total momentum\n",
    "        n_hat : (d,) unit direction of momentum (NaNs if |P|==0)\n",
    "    \"\"\"\n",
    "    P = np.sum(masses[:, None] * velocities, axis=0)\n",
    "    norm = np.linalg.norm(P)\n",
    "    n_hat = P / norm if norm > 0 else np.full_like(P, np.nan)\n",
    "    return P, n_hat\n",
    "\n",
    "def centre_of_mass(masses, positions):\n",
    "    \"\"\"\n",
    "    masses: (N,) array\n",
    "    positions: (N,d) array\n",
    "    returns R_CM (d,)\n",
    "    \"\"\"\n",
    "    M = np.sum(masses)\n",
    "    return np.sum(masses[:, None] * positions, axis=0) / M\n",
    "\n",
    "# -----------------------------\n",
    "# example simulation\n",
    "# -----------------------------\n",
    "np.random.seed(0)\n",
    "N = 10          # number of particles\n",
    "d = 2           # spatial dimension\n",
    "\n",
    "masses = np.random.uniform(0.5, 2.0, N)   # random masses between 0.5 and 2\n",
    "positions = np.random.uniform(-10, 10, (N, d))\n",
    "velocities = np.random.uniform(-1, 1, (N, d))\n",
    "\n",
    "steps = 200\n",
    "dt = 0.1\n",
    "\n",
    "R_cm_history = np.zeros((steps, d))\n",
    "n_hat_history = np.zeros((steps, d))\n",
    "\n",
    "for t in range(steps):\n",
    "    # record\n",
    "    R_cm_history[t] = centre_of_mass(masses, positions)\n",
    "    P, n_hat = total_momentum(masses, velocities)\n",
    "    n_hat_history[t] = n_hat\n",
    "    \n",
    "    # propagate (free flight)\n",
    "    positions += velocities * dt   # no external force for demo\n",
    "\n",
    "# -----------------------------\n",
    "# results\n",
    "# -----------------------------\n",
    "\n",
    "# (1) show the first 5 time–points of COM and direction\n",
    "df = pd.DataFrame({\n",
    "    \"time\": np.arange(5)*dt,\n",
    "    \"R_cm_x\": R_cm_history[:5,0],\n",
    "    \"R_cm_y\": R_cm_history[:5,1],\n",
    "    \"dir_x\": n_hat_history[:5,0],\n",
    "    \"dir_y\": n_hat_history[:5,1]\n",
    "})\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(\"COM & direction (first 5 steps)\", df)\n",
    "\n",
    "# (2) plot the COM trajectory together with the momentum direction at final time\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(R_cm_history[:,0], R_cm_history[:,1], label=\"COM path\")\n",
    "# draw an arrow for the final direction\n",
    "final_cm = R_cm_history[-1]\n",
    "arrow_scale = 5.0\n",
    "plt.arrow(final_cm[0], final_cm[1],\n",
    "          n_hat_history[-1,0]*arrow_scale,\n",
    "          n_hat_history[-1,1]*arrow_scale,\n",
    "          head_width=0.3, length_includes_head=True)\n",
    "plt.scatter(R_cm_history[0,0], R_cm_history[0,1], marker=\"o\", label=\"start\")\n",
    "plt.scatter(final_cm[0], final_cm[1], marker=\"x\", label=\"end\")\n",
    "plt.title(\"Centre–of–Mass trajectory & current momentum direction\")\n",
    "plt.axis(\"equal\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# inside your main simulation loop\n",
    "P, n_hat = total_momentum(masses, velocities)\n",
    "com      = centre_of_mass(masses, positions)\n",
    "log.append((t, *com, *P, *n_hat))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This script simulates the motion of particles under gravity and calculates\n",
    "# the center of mass velocity to determine the general direction of movement.\n",
    "# It plots the particles and an arrow indicating the direction at each time step.\n",
    "\n",
    "# Define masses of the particles\n",
    "masses = np.array([1.0, 1.0, 2.0])\n",
    "\n",
    "# Initial positions: [x, y] for each particle\n",
    "positions = np.array([[0.0, 0.0], [1.0, 1.0], [2.0, 0.0]])\n",
    "\n",
    "# Initial velocities: [vx, vy] for each particle\n",
    "velocities = np.array([[1.0, 0.0], [0.0, 1.0], [-1.0, 0.0]])\n",
    "\n",
    "# Gravity acceleration vector\n",
    "g = np.array([0.0, -9.8])\n",
    "\n",
    "# Time step for simulation\n",
    "dt = 0.01\n",
    "\n",
    "# Total number of simulation steps\n",
    "n_steps = 1000\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots()\n",
    "plt.ion()  # Turn on interactive mode for live updating\n",
    "\n",
    "# Simulation loop\n",
    "for step in range(n_steps):\n",
    "    # Update velocities with gravity\n",
    "    velocities += g * dt\n",
    "    # Update positions based on velocities\n",
    "    positions += velocities * dt\n",
    "    # Plot every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        # Calculate total mass\n",
    "        total_mass = np.sum(masses)\n",
    "        # Calculate center of mass position\n",
    "        com_position = np.sum(masses[:, np.newaxis] * positions, axis=0) / total_mass\n",
    "        # Calculate center of mass velocity\n",
    "        com_velocity = np.sum(masses[:, np.newaxis] * velocities, axis=0) / total_mass\n",
    "        # Calculate the direction angle of com_velocity\n",
    "        if np.linalg.norm(com_velocity) > 0:\n",
    "            theta = np.arctan2(com_velocity[1], com_velocity[0]) * 180 / np.pi\n",
    "        else:\n",
    "            theta = 0\n",
    "        print(f\"Step {step}: COM velocity direction = {theta:.2f} degrees\")\n",
    "        # Clear the previous plot\n",
    "        ax.clear()\n",
    "        # Plot the particles with sizes proportional to masses\n",
    "        ax.scatter(positions[:, 0], positions[:, 1], s=masses*100)\n",
    "        # Plot an arrow indicating the direction of com_velocity\n",
    "        if np.linalg.norm(com_velocity) > 0:\n",
    "            direction = com_velocity / np.linalg.norm(com_velocity)\n",
    "            arrow_length = 1.0\n",
    "            ax.arrow(com_position[0], com_position[1],\n",
    "                     direction[0]*arrow_length, direction[1]*arrow_length,\n",
    "                     head_width=0.2, head_length=0.2, fc='red', ec='red')\n",
    "        # Set plot limits\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(-10, 10)\n",
    "        # Set title\n",
    "        ax.set_title(f\"Step {step}\")\n",
    "        # Pause to update the plot\n",
    "        plt.pause(0.1)\n",
    "\n",
    "# Turn off interactive mode and show the final plot\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_com_position(masses, positions):\n",
    "    \"\"\"Calculates the Center of Mass (CoM) position vector.\n",
    "\n",
    "    Args:\n",
    "        masses (np.ndarray): 1D array of particle masses [m1, m2,..., mN].\n",
    "        positions (np.ndarray): 2D array of particle position vectors,\n",
    "                                  where each row is a position vector\n",
    "                                  [[x1, y1, z1],..., [xN, yN, zN]].\n",
    "                                  Works for 1D, 2D, or 3D positions.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The position vector of the Center of Mass [x_cm, y_cm, z_cm].\n",
    "                  Returns None if the total mass is zero or inputs are invalid.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    masses = np.asarray(masses)\n",
    "    positions = np.asarray(positions)\n",
    "\n",
    "    # Input validation: Check dimensions\n",
    "    if masses.ndim!= 1 or positions.ndim!= 2 or masses.shape!= positions.shape:\n",
    "        print(\"Error: Input dimensions mismatch.\")\n",
    "        print(f\"Masses shape: {masses.shape}, Positions shape: {positions.shape}\")\n",
    "        return None\n",
    "\n",
    "    # Calculate total mass\n",
    "    total_mass = np.sum(masses)\n",
    "\n",
    "    # Input validation: Check for non-positive total mass\n",
    "    if total_mass <= 0:\n",
    "        print(\"Error: Total mass must be positive.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the weighted average of positions.\n",
    "    # np.average performs the sum(m_i * r_i) / sum(m_i) calculation efficiently.\n",
    "    # axis=0 ensures averaging is done column-wise (for x, y, z components).\n",
    "    com_position = np.average(positions, axis=0, weights=masses)\n",
    "\n",
    "    return com_position\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_com_velocity(masses, velocities):\n",
    "    \"\"\"Calculates the Center of Mass (CoM) velocity vector.\n",
    "\n",
    "    Args:\n",
    "        masses (np.ndarray): 1D array of particle masses [m1, m2,..., mN].\n",
    "        velocities (np.ndarray): 2D array of particle velocity vectors,\n",
    "                                  where each row is a velocity vector\n",
    "                                  [[vx1, vy1, vz1],..., [vxN, vyN, vzN]].\n",
    "                                  Works for 1D, 2D, or 3D velocities.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The velocity vector of the Center of Mass [vx_cm, vy_cm, vz_cm].\n",
    "                  Returns None if the total mass is zero or inputs are invalid.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays\n",
    "    masses = np.asarray(masses)\n",
    "    velocities = np.asarray(velocities)\n",
    "\n",
    "    # Input validation: Check dimensions\n",
    "    if masses.ndim!= 1 or velocities.ndim!= 2 or masses.shape!= velocities.shape:\n",
    "        print(\"Error: Input dimensions mismatch.\")\n",
    "        print(f\"Masses shape: {masses.shape}, Velocities shape: {velocities.shape}\")\n",
    "        return None\n",
    "\n",
    "    # Calculate total mass\n",
    "    total_mass = np.sum(masses)\n",
    "\n",
    "    # Input validation: Check for non-positive total mass\n",
    "    if total_mass <= 0:\n",
    "        print(\"Error: Total mass must be positive.\")\n",
    "        return None\n",
    "\n",
    "    # Method 1: Calculate total momentum first\n",
    "    # Calculate total momentum P_sys = sum(m_i * v_i)\n",
    "    # Multiply each velocity vector (row) by its corresponding mass (scalar)\n",
    "    # Use masses[:, np.newaxis] for broadcasting: (N,) with (N, D) -> (N, D)\n",
    "    # where N is number of particles, D is number of dimensions.\n",
    "    individual_momenta = masses[:, np.newaxis] * velocities\n",
    "    total_momentum = np.sum(individual_momenta, axis=0)\n",
    "\n",
    "    # Calculate CoM velocity V_cm = P_sys / M_sys\n",
    "    com_velocity_method1 = total_momentum / total_mass\n",
    "\n",
    "    # Method 2: Directly use np.average (mathematically equivalent)\n",
    "    # np.average calculates sum(weights * values) / sum(weights)\n",
    "    com_velocity_method2 = np.average(velocities, axis=0, weights=masses)\n",
    "\n",
    "    # Both methods yield the same result, method 2 is more concise\n",
    "    return com_velocity_method2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Define the functions calculate_com_position and calculate_com_velocity ---\n",
    "# (Insert the function code from sections 2.2 and 3.3 here)\n",
    "#...\n",
    "\n",
    "# --- Define the system properties based on the table ---\n",
    "masses = np.array([1.0, 2.0, 0.5])\n",
    "positions = np.array([1.0, 2.0],\n",
    "    [-1.0, -1.0],\n",
    "    [3.0, 0.0])\n",
    "velocities = np.array([0.5, -0.5],\n",
    "    [-0.2, 1.0],\n",
    "    [0.0, 2.0])\n",
    "\n",
    "# --- Calculate CoM Position ---\n",
    "com_pos = calculate_com_position(masses, positions)\n",
    "\n",
    "# --- Calculate CoM Velocity ---\n",
    "com_vel = calculate_com_velocity(masses, velocities)\n",
    "\n",
    "# --- Print the results ---\n",
    "print(\"--- System Definition ---\")\n",
    "print(f\"Masses: {masses}\")\n",
    "print(f\"Positions:\\n{positions}\")\n",
    "print(f\"Velocities:\\n{velocities}\")\n",
    "print(\"\\n--- Calculated Center of Mass Properties ---\")\n",
    "if com_pos is not None:\n",
    "    print(f\"CoM Position (R_cm) [m]: {com_pos}\")\n",
    "if com_vel is not None:\n",
    "    print(f\"CoM Velocity (V_cm) [m/s]: {com_vel}\")\n",
    "\n",
    "# --- Verification Calculation (Manual steps for clarity) ---\n",
    "total_mass = np.sum(masses) # 1.0 + 2.0 + 0.5 = 3.5 kg\n",
    "\n",
    "# R_cm = (m1*r1 + m2*r2 + m3*r3) / M_sys\n",
    "r_cm_calc = (1.0 * np.array([1.0, 2.0]) +\n",
    "             2.0 * np.array([-1.0, -1.0]) +\n",
    "             0.5 * np.array([3.0, 0.0])) / total_mass\n",
    "# r_cm_calc = ([1.0, 2.0] + [-2.0, -2.0] + [1.5, 0.0]) / 3.5\n",
    "# r_cm_calc = [0.5, 0.0] / 3.5 = [0.142857..., 0.0]\n",
    "\n",
    "# V_cm = (m1*v1 + m2*v2 + m3*v3) / M_sys\n",
    "v_cm_calc = (1.0 * np.array([0.5, -0.5]) +\n",
    "             2.0 * np.array([-0.2, 1.0]) +\n",
    "             0.5 * np.array([0.0, 2.0])) / total_mass\n",
    "# v_cm_calc = ([0.5, -0.5] + [-0.4, 2.0] + [0.0, 1.0]) / 3.5\n",
    "# v_cm_calc = [0.1, 2.5] / 3.5 = [0.02857..., 0.714285...]\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "print(f\"Manual R_cm: {r_cm_calc}\")\n",
    "print(f\"Manual V_cm: {v_cm_calc}\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_com_velocity(masses, velocities):\n",
    "    \"\"\"Calculate center-of-mass velocity for a system of particles.\"\"\"\n",
    "    total_mass = np.sum(masses)\n",
    "    momentum = np.sum(masses[:, np.newaxis] * velocities, axis=0)\n",
    "    com_velocity = momentum / total_mass\n",
    "    return com_velocity\n",
    "\n",
    "# Example: Particles with varying masses and velocities\n",
    "masses = np.array([2.0, 3.0, 5.0])  # kg\n",
    "velocities = np.array([[3.0, 0.0], [-1.0, 4.0], [2.0, -2.0]])  # m/s\n",
    "\n",
    "com_vel = compute_com_velocity(masses, velocities)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Classical System: Center of Mass Velocity\")\n",
    "plt.quiver(*np.zeros(2), *com_vel, scale=10, color='r', label='COM Velocity')\n",
    "for i, (m, v) in enumerate(zip(masses, velocities)):\n",
    "    plt.quiver(0, 0, *v, scale=10, alpha=0.5, label=f'Particle {i+1}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def quantum_momentum_expectation(wavefunction, x_grid, hbar=1.0):\n",
    "    \"\"\"Calculate ⟨p⟩ for a 1D wavefunction.\"\"\"\n",
    "    dx = x_grid[1] - x_grid[0]\n",
    "    gradient = np.gradient(wavefunction, dx)\n",
    "    integrand = np.conj(wavefunction) * (-1j * hbar * gradient)\n",
    "    expectation_p = np.trapz(integrand, x_grid)\n",
    "    return np.real(expectation_p)  # For real-space visualization\n",
    "\n",
    "# Example: Gaussian wave packet moving with momentum k0\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "k0 = 2.0  # Initial momentum\n",
    "psi = np.exp(-0.5*(x/2)**2 + 1j*k0*x)  # Wavefunction\n",
    "psi /= np.sqrt(np.trapz(np.abs(psi)**2, x))  # Normalize\n",
    "\n",
    "⟨p⟩ = quantum_momentum_expectation(psi, x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Quantum System: Momentum Expectation\")\n",
    "plt.plot(x, np.abs(psi)**2, label='Probability Density')\n",
    "plt.axvline(⟨p⟩, color='r', linestyle='--', label=f'⟨p⟩ = {⟨p⟩:.2f} ħ')\n",
    "plt.xlabel(\"Position\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def astrophysical_bulk_velocity(visible_masses, visible_velocities, dark_matter_fraction=0.85):\n",
    "    \"\"\"Compute bulk velocity, accounting for dark matter's gravitational influence.\"\"\"\n",
    "    total_visible_mass = np.sum(visible_masses)\n",
    "    dark_mass = total_visible_mass * dark_matter_fraction / (1 - dark_matter_fraction)\n",
    "    total_mass = total_visible_mass + dark_mass\n",
    "\n",
    "    visible_momentum = np.sum(visible_masses[:, np.newaxis] * visible_velocities, axis=0)\n",
    "    # Assume dark matter follows visible matter's velocity distribution (simplified)\n",
    "    bulk_velocity = visible_momentum / total_mass\n",
    "    return bulk_velocity\n",
    "\n",
    "# Example: Galaxy with stars and dark matter\n",
    "star_masses = np.array([1e10, 2e10, 1.5e10])  # Solar masses\n",
    "star_velocities = np.array([[200, 50], [-100, 150], [0, -75]])  # km/s\n",
    "\n",
    "bulk_vel = astrophysical_bulk_velocity(star_masses, star_velocities)\n",
    "print(f\"Bulk Velocity (km/s): {bulk_vel}\")\n",
    "\n",
    "def plot_direction(velocities, labels, title):\n",
    "    \"\"\"Plot velocity vectors for comparison.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(title)\n",
    "    for vel, label in zip(velocities, labels):\n",
    "        plt.quiver(0, 0, *vel, angles='xy', scale_units='xy', scale=1, label=label)\n",
    "    plt.xlim(-1.5*np.max(np.abs(velocities)), 1.5*np.max(np.abs(velocities)))\n",
    "    plt.ylim(-1.5*np.max(np.abs(velocities)), 1.5*np.max(np.abs(velocities)))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Compare classical COM and astrophysical bulk velocity\n",
    "velocities = [com_vel, bulk_vel]\n",
    "labels = ['Classical COM', 'Astrophysical Bulk']\n",
    "plot_direction(velocities, labels, \"General Motion Direction Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d30e1-c622-4e8c-9011-952102cfde08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1f75-0637-4b71-8284-3760e6dc9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "returns = data['returns']  # e.g. daily returns array\n",
    "mod = sm.tsa.MarkovRegression(data['returns'], k_regimes=2, trend='c', switching_variance=True)\n",
    "res = mod.fit()\n",
    "regimes = res.smoothed_marginal_probabilities[0]  # probability of regime=1\n",
    "data['stat0'] = regimes\n",
    "regimes = res.smoothed_marginal_probabilities[1]  # probability of regime=1\n",
    "data['stat1'] = regimes\n",
    "#regimes = res.smoothed_marginal_probabilities[2]  # probability of regime=2\n",
    "#data['stat2'] = regimes\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "s1 = data[data.stat0 > 0.9]\n",
    "s2 = data[data.stat1 > 0.9]\n",
    "#s3 = data[data.stat2 > 0.9]\n",
    "\n",
    "plt.plot(data.index, data.close_price, linewidth = 0.5) \n",
    "plt.plot(s1.index, s1.close_price, \"go\", markersize = 2) \n",
    "plt.plot(s2.index, s2.close_price, \"ro\", markersize = 2)\n",
    "#plt.plot(s3.index, s3.Close, \"ro\", markersize = 2)\n",
    "#plt.xlim(500,1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511760c-8f03-4082-9790-a56a0ba06466",
   "metadata": {},
   "outputs": [],
   "source": [
    "### regime shift detection - x\n",
    "\n",
    "regime shift methods aim to detect abrupt shifts in the time series distribution. \\\n",
    "Techniques like the Pruned Exact Linear Time (PELT) algorithm find points where the mean/variance changes. \\\n",
    "These are not “bull/bear” per se, but identify breaks in trend or level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b940a-a479-4053-8db7-494fbc4a46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruptures as rpt\n",
    "\n",
    "#prices = list(data.Close)\n",
    "\n",
    "# Detect changes in level/mean\n",
    "algo = rpt.Pelt(model=\"rbf\").fit(data)\n",
    "breakpoints = algo.predict(pen=5)  # tune penalty\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "#s1 = data[data.stat0 > 0.9]\n",
    "s2 = data[data.index.isin(breakpoints)]\n",
    "#s3 = data[data.stat2 > 0.9]\n",
    "\n",
    "plt.plot(data.index, data.close_price, linewidth = 0.5) \n",
    "#plt.plot(s1.index, s1.Close, \"go\", markersize = 2) \n",
    "plt.plot(s2.index, s2.close_price, \"ro\", markersize = 2)\n",
    "#plt.plot(s3.index, s3.Close, \"ro\", markersize = 2)\n",
    "#plt.xlim(500,1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58240df4-7b6c-43a4-bcbb-2c60766a0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hidden markov model - seriously needs improvement\n",
    "\n",
    "Hidden Markov Models generalize Markov-switching by allowing multivariate observations and more flexible inference. An HMM treats regimes as latent states with observations (price, returns, or features) emitted from state-dependent distributions\n",
    "\n",
    "Typically daily returns or log-returns. Often multiple features are used: for example volatility, momentum indicators, or cross-asset returns can be jointly modeled. (One HMM approach used 3 hidden states and two features: 10-day average volatility and daily S&P500 return​\n",
    "medium.com\n",
    ").\n",
    "\n",
    "Still requires choosing the number of states in advance. Results can be sensitive to initial parameters and local optima. States may not correspond cleanly to intuitive labels (the model might split “bear” into two states, etc.). Interpreting states often requires examining their parameter values (means, variances). HMM assumes stationary transition probabilities (like MS models) and may struggle if market dynamics change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ba8d2-03cf-4cc3-aed8-5409ec1043ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM\n",
    "X = np.array(lst) # 2D array: each row is [volatility_feature, return_feature]\n",
    "model = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=100)\n",
    "model.fit(X)\n",
    "hidden_states = model.predict(X)  # e.g. array of 0,1,2 states\n",
    "\n",
    "data['states'] = hidden_states\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "s1 = data[data.states == 0]\n",
    "s2 = data[data.states == 1]\n",
    "s3 = data[data.states == 2]\n",
    "\n",
    "plt.plot(data.index, data.close_price, linewidth = 0.5) \n",
    "#plt.plot(s1.index, s1.close_price, \"go\", markersize = 2) \n",
    "plt.plot(s2.index, s2.close_price, \"yo\", markersize = 2)\n",
    "#plt.plot(s3.index, s3.close_price, \"ro\", markersize = 2)\n",
    "#plt.xlim(500,1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f04aa-3541-49a1-831b-2555f5855832",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering methods - dunno\n",
    "\n",
    "Unsupervised clustering can group market periods by similarity of features (e.g. returns, volatility, volume). Each cluster can then be labeled (post hoc) as bull, bear, or neutral. Common algorithms include K-Means and Agglomerative (hierarchical) clustering.\n",
    "\n",
    "Require pre-specifying number of clusters (for K-Means) or stopping criteria. Sensitive to feature scaling and outliers. K-Means assumes spherical clusters and equal variance; Agglomerative can be slow on very large data. Clusters may mix states (e.g. a “bull” cluster could contain some bear data if features overlap). Interpretation of clusters often requires manual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e8c8a-9ef7-4a09-96b0-7031d2feed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "features = data\n",
    "\n",
    "...  # e.g. N×d array of features (d dimensions)\n",
    "# K-Means (e.g., K=3)\n",
    "km = KMeans(n_clusters=2, random_state=0).fit(features)\n",
    "labels_km = km.labels_\n",
    "# Agglomerative\n",
    "agg = AgglomerativeClustering(n_clusters=2).fit(features)\n",
    "labels_ag = agg.labels_\n",
    "\n",
    "data['K-state'] = labels_km\n",
    "data['agg-state'] = labels_ag\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "s1 = data[data['agg-state'] == 0]\n",
    "s2 = data[data['agg-state'] == 1]\n",
    "#s3 = data[data['agg-state'] == 2]\n",
    "\n",
    "plt.plot(data.index, data.close_price, linewidth = 0.5) \n",
    "plt.plot(s1.index, s1.close_price, \"go\", markersize = 2) \n",
    "plt.plot(s2.index, s2.close_price, \"yo\", markersize = 2)\n",
    "#plt.plot(s3.index, s3.Close, \"ro\", markersize = 2)\n",
    "#plt.xlim(500,1000)\n",
    "#plt.ylim(30,45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74c48f-881d-4c35-8962-06780eea69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gaussian mixture models\n",
    "\n",
    "A Gaussian Mixture Model is a probabilistic clustering: it assumes data arise from a mixture of Gaussian distributions\n",
    ". In regime detection, each Gaussian component corresponds to a regime, and points are assigned to regimes by posterior probability.\n",
    "\n",
    "Assumes each regime’s features are Gaussian (real markets may be skewed). Needs specifying number of components. EM may converge to local optima. If non-Gaussian clusters exist, performance degrades. Like K-Means, also sensitive to initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a2fa2-b873-421b-a96f-5c6f7b97859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)\n",
    "gmm.fit(features)\n",
    "states = gmm.predict(features)          # hard assignments\n",
    "probs = gmm.predict_proba(features)     # probabilities of each state\n",
    "\n",
    "data['state'] = states\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "s1 = data[data['state'] == 0]\n",
    "s2 = data[data['state'] == 1]\n",
    "s3 = data[data['state'] == 2]\n",
    "\n",
    "plt.plot(data.index, data.close_price, linewidth = 0.5) \n",
    "plt.plot(s1.index, s1.close_price, \"ro\", markersize = 2) \n",
    "plt.plot(s2.index, s2.close_price, \"yo\", markersize = 2)\n",
    "plt.plot(s3.index, s3.close_price, \"go\", markersize = 2)\n",
    "#plt.xlim(500,1000)\n",
    "#plt.ylim(30,45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f92d2-f31b-4d53-807c-fd2dc530f655",
   "metadata": {},
   "source": [
    "### Deep learning\n",
    "\n",
    "Deep learning approaches have been applied to regime detection or trend classification. Recurrent Neural Networks (RNNs), especially LSTM/GRU networks, can in principle learn temporal patterns that correspond to different market states.\n",
    "\n",
    "Requires a large amount of data, especially for training a complex model. Supervised training needs reliable regime labels (which are itself a challenge!). Without labels, using RNNs is non-trivial. Overfitting is a major risk, especially when regimes change. Models are black boxes and lack interpretability. In practice, pure deep learning regimes are less common than simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1eec0-e222-49bd-82b9-68f07d85c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(window_length, num_features)),\n",
    "    Dense(3, activation='softmax')  # for 3 regimes\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d854045b-ff12-48d7-a038-1fd059f48135",
   "metadata": {},
   "outputs": [],
   "source": [
    "### other\n",
    "\n",
    "Other Modern Approaches\n",
    "Self-Organizing Map (SOM): An unsupervised neural network that projects multi-dimensional market data onto a 2D grid, clustering similar behavior together. SOMs can visualize regime transitions (e.g. areas of the map representing bull vs bear)​\n",
    "bluechipalgos.com\n",
    ". They are mostly used in research, as tuning can be tricky.\n",
    "Autoencoder / Anomaly Detection: One might train an autoencoder on “normal” market behavior; large reconstruction error may indicate entry into an unusual regime (e.g. crash). This approach is more common in risk monitoring than explicit bull/bear labeling.\n",
    "Composite and Ensemble Models: Some firms combine multiple methods. For example, using both an HMM and an MA filter, and requiring agreement before signaling a regime shift. Others use clustering on transformed features (e.g. PCA-reduced components)​\n",
    "bluechipalgos.com\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f922aa-d07b-4bc3-a366-fd3cc248b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_streamer.py\n",
    "import asyncio\n",
    "import ccxt.async_support as ccxt\n",
    "import redis\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "class DataStreamer:\n",
    "    def __init__(self, exchanges: List[str], symbols: List[str], redis_url: str):\n",
    "        \"\"\"\n",
    "        Initialize exchange connections and data storage\n",
    "        \n",
    "        Args:\n",
    "            exchanges: List of exchange IDs (e.g., 'binance', 'okx', 'coinbase')\n",
    "            symbols: List of trading pairs (e.g., 'BTC/USDT')\n",
    "            redis_url: Connection string for Redis\n",
    "        \"\"\"\n",
    "        self.exchanges = {}\n",
    "        for exchange_id in exchanges:\n",
    "            exchange_class = getattr(ccxt, exchange_id)\n",
    "            self.exchanges[exchange_id] = exchange_class({\n",
    "                'enableRateLimit': True,\n",
    "                'options': {'defaultType': 'future'}  # For exchanges with multiple markets\n",
    "            })\n",
    "        \n",
    "        self.symbols = symbols\n",
    "        self.redis = redis.from_url(redis_url)\n",
    "        self.orderbook_depth = 20  # Depth for orderbook snapshots\n",
    "        \n",
    "    async def fetch_orderbook(self, exchange_id: str, symbol: str):\n",
    "        \"\"\"Fetch and process orderbook data\"\"\"\n",
    "        try:\n",
    "            exchange = self.exchanges[exchange_id]\n",
    "            orderbook = await exchange.fetch_order_book(symbol, self.orderbook_depth)\n",
    "            \n",
    "            # Process orderbook to calculate imbalance metrics\n",
    "            bids = orderbook['bids']\n",
    "            asks = orderbook['asks']\n",
    "            \n",
    "            # Calculate orderbook imbalance using volume-weighted method\n",
    "            bid_volume = sum(bid[1] for bid in bids)\n",
    "            ask_volume = sum(ask[1] for ask in asks)\n",
    "            imbalance = (bid_volume - ask_volume) / (bid_volume + ask_volume)\n",
    "            \n",
    "            # Calculate expected fill price for market orders\n",
    "            mid_price = (bids[0][0] + asks[0][0]) / 2\n",
    "            spread = asks[0][0] - bids[0][0]\n",
    "            spread_pct = spread / mid_price\n",
    "            \n",
    "            # Store processed data in Redis\n",
    "            data = {\n",
    "                'timestamp': orderbook['timestamp'],\n",
    "                'exchange': exchange_id,\n",
    "                'symbol': symbol,\n",
    "                'mid_price': mid_price,\n",
    "                'imbalance': imbalance,\n",
    "                'spread_pct': spread_pct,\n",
    "                'bids': bids[:5],  # Store top 5 levels\n",
    "                'asks': asks[:5]\n",
    "            }\n",
    "            \n",
    "            # Publish to Redis streams for ultra-low-latency access\n",
    "            self.redis.xadd(f\"orderbook:{exchange_id}:{symbol}\", data)\n",
    "            \n",
    "            # Also store to a time-series format for later analysis\n",
    "            key = f\"ts:orderbook:{exchange_id}:{symbol}:{orderbook['timestamp']}\"\n",
    "            self.redis.hmset(key, data)\n",
    "            self.redis.expire(key, 86400)  # Keep for 24 hours\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching orderbook for {exchange_id}:{symbol}: {e}\")\n",
    "    \n",
    "    async def fetch_trades(self, exchange_id: str, symbol: str):\n",
    "        \"\"\"Fetch and process recent trades\"\"\"\n",
    "        try:\n",
    "            exchange = self.exchanges[exchange_id]\n",
    "            trades = await exchange.fetch_trades(symbol, limit=100)\n",
    "            \n",
    "            # Process trades to extract flow metrics\n",
    "            buy_volume = sum(trade['amount'] for trade in trades if trade['side'] == 'buy')\n",
    "            sell_volume = sum(trade['amount'] for trade in trades if trade['side'] == 'sell')\n",
    "            flow_ratio = buy_volume / (buy_volume + sell_volume) if (buy_volume + sell_volume) > 0 else 0.5\n",
    "            \n",
    "            # Calculate trade-based momentum\n",
    "            prices = [trade['price'] for trade in trades]\n",
    "            if len(prices) > 1:\n",
    "                momentum = (prices[-1] / prices[0]) - 1\n",
    "            else:\n",
    "                momentum = 0\n",
    "                \n",
    "            # Store processed data\n",
    "            data = {\n",
    "                'timestamp': int(pd.Timestamp.now().timestamp() * 1000),\n",
    "                'exchange': exchange_id,\n",
    "                'symbol': symbol,\n",
    "                'flow_ratio': flow_ratio,\n",
    "                'momentum': momentum,\n",
    "                'trade_count': len(trades)\n",
    "            }\n",
    "            \n",
    "            # Publish to Redis\n",
    "            self.redis.xadd(f\"trades:{exchange_id}:{symbol}\", data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching trades for {exchange_id}:{symbol}: {e}\")\n",
    "    \n",
    "    async def stream_all_data(self):\n",
    "        \"\"\"Main loop to continuously stream all data\"\"\"\n",
    "        while True:\n",
    "            tasks = []\n",
    "            for exchange_id in self.exchanges:\n",
    "                for symbol in self.symbols:\n",
    "                    tasks.append(self.fetch_orderbook(exchange_id, symbol))\n",
    "                    tasks.append(self.fetch_trades(exchange_id, symbol))\n",
    "            \n",
    "            await asyncio.gather(*tasks)\n",
    "            await asyncio.sleep(0.1)  # Stream refresh rate: 100ms\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"Initialize and run the data streamer\"\"\"\n",
    "        try:\n",
    "            # Connect to exchanges\n",
    "            for exchange_id, exchange in self.exchanges.items():\n",
    "                await exchange.load_markets()\n",
    "                print(f\"Connected to {exchange_id}\")\n",
    "            \n",
    "            # Start streaming\n",
    "            await self.stream_all_data()\n",
    "        finally:\n",
    "            # Close connections when done\n",
    "            for exchange in self.exchanges.values():\n",
    "                await exchange.close()\n",
    "                \n",
    "    def close(self):\n",
    "        \"\"\"Close Redis connection\"\"\"\n",
    "        self.redis.close()\n",
    "\n",
    "# Run the data streamer\n",
    "if __name__ == \"__main__\":\n",
    "    exchanges = ['binance', 'okx', 'coinbase']\n",
    "    symbols = ['BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'BNB/USDT']\n",
    "    redis_url = \"redis://localhost:6379/0\"\n",
    "    \n",
    "    streamer = DataStreamer(exchanges, symbols, redis_url)\n",
    "    \n",
    "    try:\n",
    "        asyncio.run(streamer.run())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Data streaming stopped\")\n",
    "    finally:\n",
    "        streamer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc872a8-3fdc-4976-a4d7-ff978852e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_engineering.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "import redis\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import networkx as nx\n",
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self, db_config: Dict, redis_url: str):\n",
    "        \"\"\"\n",
    "        Initialize the feature engineering pipeline\n",
    "        \n",
    "        Args:\n",
    "            db_config: Database connection parameters\n",
    "            redis_url: Redis connection string\n",
    "        \"\"\"\n",
    "        self.db_config = db_config\n",
    "        self.redis = redis.from_url(redis_url)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def connect_db(self):\n",
    "        \"\"\"Connect to PostgreSQL/TimescaleDB\"\"\"\n",
    "        return psycopg2.connect(**self.db_config)\n",
    "    \n",
    "    def fetch_market_data(self, symbols: List[str], lookback_days: int = 30) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Fetch OHLCV and orderbook data for feature generation\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            for symbol in symbols:\n",
    "                # Fetch OHLCV data\n",
    "                query = \"\"\"\n",
    "                SELECT timestamp, open, high, low, close, volume\n",
    "                FROM ohlcv \n",
    "                WHERE symbol = %s AND timestamp > NOW() - INTERVAL %s DAY\n",
    "                ORDER BY timestamp\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=(symbol, lookback_days))\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                # Fetch orderbook imbalance data\n",
    "                query = \"\"\"\n",
    "                SELECT timestamp, mid_price, imbalance, spread_pct\n",
    "                FROM orderbook_metrics\n",
    "                WHERE symbol = %s AND timestamp > NOW() - INTERVAL %s DAY\n",
    "                ORDER BY timestamp\n",
    "                \"\"\"\n",
    "                ob_df = pd.read_sql_query(query, conn, params=(symbol, lookback_days))\n",
    "                ob_df['timestamp'] = pd.to_datetime(ob_df['timestamp'])\n",
    "                ob_df.set_index('timestamp', inplace=True)\n",
    "                \n",
    "                # Resample to 1-hour intervals for medium-frequency features\n",
    "                df_1h = df.resample('1H').agg({\n",
    "                    'open': 'first',\n",
    "                    'high': 'max',\n",
    "                    'low': 'min',\n",
    "                    'close': 'last',\n",
    "                    'volume': 'sum'\n",
    "                }).dropna()\n",
    "                \n",
    "                ob_df_1h = ob_df.resample('1H').agg({\n",
    "                    'mid_price': 'last',\n",
    "                    'imbalance': 'mean',\n",
    "                    'spread_pct': 'mean'\n",
    "                }).dropna()\n",
    "                \n",
    "                # Merge OHLCV and orderbook data\n",
    "                combined = pd.merge(df_1h, ob_df_1h, left_index=True, right_index=True, how='outer').dropna()\n",
    "                results[symbol] = combined\n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def fetch_on_chain_data(self, assets: List[str], lookback_days: int = 30) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Fetch on-chain metrics for crypto assets\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            for asset in assets:\n",
    "                base_asset = asset.split('/')[0]  # Extract BTC from BTC/USDT\n",
    "                \n",
    "                query = \"\"\"\n",
    "                SELECT timestamp, active_addresses, transaction_count, nvt_ratio, \n",
    "                       funding_rate, liquidations, network_hash_rate\n",
    "                FROM onchain_metrics\n",
    "                WHERE asset = %s AND timestamp > NOW() - INTERVAL %s DAY\n",
    "                ORDER BY timestamp\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=(base_asset, lookback_days))\n",
    "                \n",
    "                if not df.empty:\n",
    "                    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                    df.set_index('timestamp', inplace=True)\n",
    "                    \n",
    "                    # Resample to 1-hour intervals\n",
    "                    df_1h = df.resample('1H').last().fillna(method='ffill')\n",
    "                    results[asset] = df_1h\n",
    "        finally:\n",
    "            conn.close()\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def create_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate technical indicators from OHLCV data\"\"\"\n",
    "        # Price-based indicators\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        \n",
    "        # Moving averages\n",
    "        for window in [10, 20, 50, 100]:\n",
    "            df[f'sma_{window}'] = df['close'].rolling(window=window).mean()\n",
    "            df[f'ema_{window}'] = df['close'].ewm(span=window, adjust=False).mean()\n",
    "        \n",
    "        # Relative strength\n",
    "        df['rsi_14'] = self._calculate_rsi(df['close'], window=14)\n",
    "        \n",
    "        # Volatility\n",
    "        df['atr_14'] = self._calculate_atr(df, window=14)\n",
    "        df['bbands_upper'], df['bbands_middle'], df['bbands_lower'] = self._calculate_bollinger_bands(df['close'], window=20)\n",
    "        \n",
    "        # Volume indicators\n",
    "        df['volume_sma_20'] = df['volume'].rolling(window=20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "        \n",
    "        # Add orderbook features\n",
    "        df['imbalance_ma_10'] = df['imbalance'].rolling(window=10).mean()\n",
    "        df['imbalance_std_10'] = df['imbalance'].rolling(window=10).std()\n",
    "        df['spread_ma_10'] = df['spread_pct'].rolling(window=10).mean()\n",
    "        \n",
    "        return df.dropna()\n",
    "    \n",
    "    def create_cross_asset_features(self, dfs: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Generate cross-asset correlation and volatility spillover features\"\"\"\n",
    "        results = {}\n",
    "        symbols = list(dfs.keys())\n",
    "        \n",
    "        # Create a reference timeline from all dataframes\n",
    "        all_timestamps = set()\n",
    "        for df in dfs.values():\n",
    "            all_timestamps.update(df.index)\n",
    "        \n",
    "        timeline = pd.DataFrame(index=sorted(all_timestamps))\n",
    "        \n",
    "        # Merge all closing prices to create correlation features\n",
    "        price_df = timeline.copy()\n",
    "        for symbol, df in dfs.items():\n",
    "            price_df[symbol] = df['close']\n",
    "        \n",
    "        price_df = price_df.fillna(method='ffill')\n",
    "        \n",
    "        # Calculate rolling correlations between assets\n",
    "        for i, symbol1 in enumerate(symbols):\n",
    "            enhanced_df = dfs[symbol1].copy()\n",
    "            \n",
    "            for j, symbol2 in enumerate(symbols):\n",
    "                if i != j:\n",
    "                    # Add correlation features\n",
    "                    returns1 = price_df[symbol1].pct_change()\n",
    "                    returns2 = price_df[symbol2].pct_change()\n",
    "                    corr = returns1.rolling(window=24).corr(returns2)\n",
    "                    enhanced_df[f'corr_{symbol2}_24h'] = corr\n",
    "                    \n",
    "                    # Add relative strength features\n",
    "                    rel_strength = price_df[symbol1] / price_df[symbol2]\n",
    "                    enhanced_df[f'rel_strength_{symbol2}'] = rel_strength\n",
    "                    enhanced_df[f'rel_strength_{symbol2}_z'] = (rel_strength - rel_strength.rolling(30).mean()) / rel_strength.rolling(30).std()\n",
    "            \n",
    "            results[symbol1] = enhanced_df\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_graph_features(self, dfs: Dict[str, pd.DataFrame], lookback: int = 72) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Extract graph-based features using temporal graph attention networks approach\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Create network of assets based on return correlations\n",
    "        for symbol, df in dfs.items():\n",
    "            # Start with original dataframe\n",
    "            enhanced_df = df.copy()\n",
    "            \n",
    "            # For each time step, create a rolling graph representation\n",
    "            for i in range(lookback, len(df)):\n",
    "                # Get the subset window\n",
    "                window = df.iloc[i-lookback:i]\n",
    "                \n",
    "                # Create correlation network from this window\n",
    "                returns = window['returns'].dropna()\n",
    "                if len(returns) > 10:  # Ensure enough data points\n",
    "                    # Calculate network metrics\n",
    "                    try:\n",
    "                        # Use a placeholder for graph metrics (would be replaced with actual T-GAT model)\n",
    "                        enhanced_df.iloc[i, enhanced_df.columns.get_loc('graph_centrality')] = 0.5\n",
    "                        enhanced_df.iloc[i, enhanced_df.columns.get_loc('graph_clustering')] = 0.3\n",
    "                    except:\n",
    "                        # Add columns if they don't exist\n",
    "                        enhanced_df['graph_centrality'] = np.nan\n",
    "                        enhanced_df['graph_clustering'] = np.nan\n",
    "                        enhanced_df.iloc[i, enhanced_df.columns.get_loc('graph_centrality')] = 0.5\n",
    "                        enhanced_df.iloc[i, enhanced_df.columns.get_loc('graph_clustering')] = 0.3\n",
    "            \n",
    "            results[symbol] = enhanced_df\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def prepare_transformer_features(self, df: pd.DataFrame, seq_length: int = 60) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare sequential data for transformer models\"\"\"\n",
    "        # Select relevant features\n",
    "        feature_cols = [\n",
    "            'returns', 'volume_ratio', 'rsi_14', 'imbalance', \n",
    "            'imbalance_ma_10', 'spread_pct', 'atr_14'\n",
    "        ]\n",
    "        \n",
    "        # Add any cross-asset or graph features if they exist\n",
    "        for col in df.columns:\n",
    "            if col.startswith('corr_') or col.startswith('rel_strength_') or col.startswith('graph_'):\n",
    "                feature_cols.append(col)\n",
    "        \n",
    "        # Prepare features and target\n",
    "        features = df[feature_cols].values\n",
    "        target = df['returns'].shift(-1).values  # Next period returns as target\n",
    "        \n",
    "        # Create sequences for the transformer\n",
    "        X, y = [], []\n",
    "        for i in range(len(df) - seq_length - 1):\n",
    "            X.append(features[i:i+seq_length])\n",
    "            y.append(target[i+seq_length])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def _calculate_rsi(self, prices: pd.Series, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        \n",
    "        rs = gain / loss\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    def _calculate_atr(self, df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "        \"\"\"Calculate Average True Range\"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "        \n",
    "        tr1 = high - low\n",
    "        tr2 = (high - close.shift()).abs()\n",
    "        tr3 = (low - close.shift()).abs()\n",
    "        \n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(window=window).mean()\n",
    "        \n",
    "        return atr\n",
    "    \n",
    "    def _calculate_bollinger_bands(self, prices: pd.Series, window: int = 20, num_std: float = 2.0) -> Tuple[pd.Series, pd.Series, pd.Series]:\n",
    "        \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "        middle_band = prices.rolling(window=window).mean()\n",
    "        std_dev = prices.rolling(window=window).std()\n",
    "        \n",
    "        upper_band = middle_band + (std_dev * num_std)\n",
    "        lower_band = middle_band - (std_dev * num_std)\n",
    "        \n",
    "        return upper_band, middle_band, lower_band\n",
    "    \n",
    "    def process_all_features(self, symbols: List[str]):\n",
    "        \"\"\"Main method to process all features and store for strategy use\"\"\"\n",
    "        # Fetch market data\n",
    "        market_data = self.fetch_market_data(symbols)\n",
    "        \n",
    "        # Fetch on-chain data for crypto assets\n",
    "        crypto_assets = [s for s in symbols if '/' in s]  # Filter for trading pairs\n",
    "        onchain_data = self.fetch_on_chain_data(crypto_assets)\n",
    "        \n",
    "        # Generate technical features\n",
    "        tech_features = {}\n",
    "        for symbol, df in market_data.items():\n",
    "            tech_features[symbol] = self.create_technical_features(df)\n",
    "        \n",
    "        # Generate cross-asset features\n",
    "        cross_features = self.create_cross_asset_features(tech_features)\n",
    "        \n",
    "        # Merge with on-chain data for crypto assets\n",
    "        merged_features = {}\n",
    "        for symbol, df in cross_features.items():\n",
    "            if symbol in onchain_data:\n",
    "                merged_df = pd.merge(df, onchain_data[symbol], left_index=True, right_index=True, how='left')\n",
    "                merged_df = merged_df.fillna(method='ffill')\n",
    "            else:\n",
    "                merged_df = df\n",
    "            \n",
    "            merged_features[symbol] = merged_df\n",
    "        \n",
    "        # Store processed features in the database\n",
    "        self.store_features(merged_features)\n",
    "        \n",
    "        # Also push the latest features to Redis for real-time access\n",
    "        self.push_latest_to_redis(merged_features)\n",
    "        \n",
    "        return merged_features\n",
    "    \n",
    "    def store_features(self, feature_dfs: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Store processed features in the database\"\"\"\n",
    "        conn = self.connect_db()\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            for symbol, df in feature_dfs.items():\n",
    "                # Reset index to get timestamp as column\n",
    "                df_to_store = df.reset_index()\n",
    "                \n",
    "                # Prepare column names and data\n",
    "                columns = ['timestamp', 'symbol'] + list(df_to_store.columns.drop('timestamp'))\n",
    "                \n",
    "                # Add symbol column\n",
    "                df_to_store['symbol'] = symbol\n",
    "                \n",
    "                # Reorder columns to match database schema\n",
    "                df_to_store = df_to_store[columns]\n",
    "                \n",
    "                # Convert to list of tuples for bulk insert\n",
    "                records = list(df_to_store.itertuples(index=False, name=None))\n",
    "                \n",
    "                # Insert using execute_values for better performance\n",
    "                insert_query = f\"\"\"\n",
    "                INSERT INTO processed_features ({', '.join(columns)}) \n",
    "                VALUES %s\n",
    "                ON CONFLICT (timestamp, symbol) DO UPDATE SET \n",
    "                {', '.join([f\"{col} = EXCLUDED.{col}\" for col in columns if col not in ['timestamp', 'symbol']])}\n",
    "                \"\"\"\n",
    "                \n",
    "                execute_values(cursor, insert_query, records)\n",
    "            \n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing features: {e}\")\n",
    "            conn.rollback()\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def push_latest_to_redis(self, feature_dfs: Dict[str, pd.DataFrame]):\n",
    "        \"\"\"Push the latest features to Redis for real-time access\"\"\"\n",
    "        for symbol, df in feature_dfs.items():\n",
    "            if not df.empty:\n",
    "                # Get the latest row\n",
    "                latest = df.iloc[-1].to_dict()\n",
    "                \n",
    "                # Convert any non-serializable objects\n",
    "                for k, v in latest.items():\n",
    "                    if isinstance(v, (np.integer, np.floating)):\n",
    "                        latest[k] = float(v)\n",
    "                    elif isinstance(v, (np.ndarray, pd.Series)):\n",
    "                        latest[k] = v.tolist()\n",
    "                    elif pd.isna(v):\n",
    "                        latest[k] = None\n",
    "                \n",
    "                # Store in Redis\n",
    "                self.redis.hmset(f\"latest_features:{symbol}\", latest)\n",
    "                \n",
    "                # Set expiry to 1 hour (features should be refreshed by then)\n",
    "                self.redis.expire(f\"latest_features:{symbol}\", 3600)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    db_config = {\n",
    "        'host': 'localhost',\n",
    "        'database': 'crypto_trading',\n",
    "        'user': 'trader',\n",
    "        'password': 'password'\n",
    "    }\n",
    "    \n",
    "    redis_url = \"redis://localhost:6379/0\"\n",
    "    symbols = ['BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'BNB/USDT']\n",
    "    \n",
    "    feature_engineer = FeatureEngineer(db_config, redis_url)\n",
    "    features = feature_engineer.process_all_features(symbols)\n",
    "    \n",
    "    print(f\"Processed features for {len(features)} symbols\")\n",
    "    for symbol, df in features.items():\n",
    "        print(f\"{symbol}: {df.shape[0]} rows, {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ebfdf-91ad-47d1-a505-4e17b1cd8989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# market_making.py\n",
    "import numpy as np\n",
    "import redis\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "class AvellanedaStoikovMarketMaker:\n",
    "    \"\"\"\n",
    "    Implementation of Avellaneda-Stoikov market making model with adaptations\n",
    "    for cryptocurrency markets and order book imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 symbol: str,\n",
    "                 exchanges: List[str],\n",
    "                 redis_url: str,\n",
    "                 risk_aversion: float = 0.1,\n",
    "                 inventory_target: float = 0.0,\n",
    "                 max_inventory: float = 1.0,\n",
    "                 order_size: float = 0.01,\n",
    "                 min_spread: float = 0.0002,\n",
    "                 reservation_spread: float = 0.001,\n",
    "                 order_refresh_time: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the market making strategy\n",
    "        \n",
    "        Args:\n",
    "            symbol: Trading pair (e.g., 'BTC/USDT')\n",
    "            exchanges: List of exchanges to monitor and trade on\n",
    "            redis_url: Redis connection string for market data\n",
    "            risk_aversion: Risk aversion parameter (gamma in A-S model)\n",
    "            inventory_target: Target inventory position (usually 0)\n",
    "            max_inventory: Maximum allowed inventory (absolute value)\n",
    "            order_size: Size of each order\n",
    "            min_spread: Minimum bid-ask spread\n",
    "            reservation_spread: Base spread for reservation price calculation\n",
    "            order_refresh_time: How often to refresh orders (seconds)\n",
    "        \"\"\"\n",
    "        self.symbol = symbol\n",
    "        self.exchanges = exchanges\n",
    "        self.redis = redis.from_url(redis_url)\n",
    "        \n",
    "        # Strategy parameters\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.inventory_target = inventory_target\n",
    "        self.max_inventory = max_inventory\n",
    "        self.order_size = order_size\n",
    "        self.min_spread = min_spread\n",
    "        self.reservation_spread = reservation_spread\n",
    "        self.order_refresh_time = order_refresh_time\n",
    "        \n",
    "        # State variables\n",
    "        self.inventory = 0.0\n",
    "        self.mid_price = 0.0\n",
    "        self.volatility = 0.0\n",
    "        self.imbalance = 0.0\n",
    "        self.orderbook_levels = {}\n",
    "        self.active_orders = {}\n",
    "        \n",
    "        # Recent trade history for volatility estimation\n",
    "        self.price_history = deque(maxlen=100)\n",
    "        self.last_order_time = 0\n",
    "        \n",
    "    def get_market_data(self) -> Dict:\n",
    "        \"\"\"Fetch latest market data from Redis\"\"\"\n",
    "        data = {}\n",
    "        \n",
    "        # Get latest mid price and orderbook data\n",
    "        for exchange in self.exchanges:\n",
    "            # Get the latest orderbook entry from Redis stream\n",
    "            stream_key = f\"orderbook:{exchange}:{self.symbol}\"\n",
    "            latest = self.redis.xrevrange(stream_key, '+', '-', count=1)\n",
    "            \n",
    "            if latest:\n",
    "                msg_id, msg_data = latest[0]\n",
    "                \n",
    "                # Convert Redis byte strings to Python types\n",
    "                parsed_data = {k.decode(): (v.decode() if isinstance(v, bytes) else v) \n",
    "                              for k, v in msg_data.items()}\n",
    "                \n",
    "                # Convert string numbers to float\n",
    "                for key in ['mid_price', 'imbalance', 'spread_pct']:\n",
    "                    if key in parsed_data:\n",
    "                        parsed_data[key] = float(parsed_data[key])\n",
    "                \n",
    "                # Parse bids and asks\n",
    "                if 'bids' in parsed_data and isinstance(parsed_data['bids'], str):\n",
    "                    parsed_data['bids'] = json.loads(parsed_data['bids'])\n",
    "                \n",
    "                if 'asks' in parsed_data and isinstance(parsed_data['asks'], str):\n",
    "                    parsed_data['asks'] = json.loads(parsed_data['asks'])\n",
    "                \n",
    "                data[exchange] = parsed_data\n",
    "                \n",
    "                # Update price history for volatility calculation\n",
    "                if 'mid_price' in parsed_data:\n",
    "                    self.price_history.append(parsed_data['mid_price'])\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def calculate_volatility(self) -> float:\n",
    "        \"\"\"Calculate short-term volatility estimate\"\"\"\n",
    "        if len(self.price_history) < 10:\n",
    "            return 0.001  # Default value if not enough data\n",
    "        \n",
    "        # Calculate return series\n",
    "        prices = np.array(self.price_history)\n",
    "        returns = np.diff(np.log(prices))\n",
    "        \n",
    "        # Annualized volatility (assuming data points are ~1 second apart)\n",
    "        seconds_in_year = 365 * 24 * 60 * 60\n",
    "        vol = np.std(returns) * np.sqrt(seconds_in_year)\n",
    "        \n",
    "        return max(vol, 0.001)  # Minimum volatility floor\n",
    "    \n",
    "    def calculate_optimal_spreads(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate optimal bid and ask prices using Avellaneda-Stoikov model\n",
    "        with orderbook imbalance adjustment\n",
    "        \"\"\"\n",
    "        # Get latest data from primary exchange (first in list)\n",
    "        market_data = self.get_market_data()\n",
    "        if not market_data or self.exchanges[0] not in market_data:\n",
    "            return None, None\n",
    "        \n",
    "        primary_data = market_data[self.exchanges[0]]\n",
    "        self.mid_price = primary_data.get('mid_price', self.mid_price)\n",
    "        self.imbalance = primary_data.get('imbalance', 0)\n",
    "        \n",
    "        # Update volatility estimate\n",
    "        self.volatility = self.calculate_volatility()\n",
    "        \n",
    "        # Calculate inventory risk component\n",
    "        inventory_skew = self.risk_aversion * self.volatility * (self.inventory - self.inventory_target)\n",
    "        \n",
    "        # Base reservation price from A-S model\n",
    "        reservation_price = self.mid_price - inventory_skew\n",
    "        \n",
    "        # Calculate base half spread from model\n",
    "        base_half_spread = self.min_spread + self.reservation_spread * self.volatility\n",
    "        \n",
    "        # Adjust spread based on order book imbalance\n",
    "        imbalance_factor = 0.3  # How strongly imbalance affects the spread\n",
    "        imbalance_adjustment = base_half_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af901f-8945-43a3-b3ff-6be39647e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "import time\n",
    "from websocket import create_connection\n",
    "import json\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, filename='trading.log')\n",
    "\n",
    "# Exchange setup\n",
    "exchange = ccxt.binance({\n",
    "    'apiKey': 'YOUR_API_KEY',\n",
    "    'secret': 'YOUR_SECRET',\n",
    "    'enableRateLimit': True,\n",
    "})\n",
    "\n",
    "# Ultra-Low Latency: Avellaneda-Stoikov Market Making\n",
    "def avellaneda_stoikov(reservation_price, spread, gamma=0.1, sigma=0.01):\n",
    "    bid_price = reservation_price - spread / 2 - (gamma * sigma**2) / 2\n",
    "    ask_price = reservation_price + spread / 2 + (gamma * sigma**2) / 2\n",
    "    return bid_price, ask_price\n",
    "\n",
    "def market_making_strategy(symbol='BTC/USDT'):\n",
    "    ws = create_connection(f\"wss://stream.binance.com:9443/ws/{symbol.lower()}@depth\")\n",
    "    while True:\n",
    "        data = json.loads(ws.recv())\n",
    "        bids = data['bids'][0]\n",
    "        asks = data['asks'][0]\n",
    "        best_bid, best_ask = float(bids[0]), float(asks[0])\n",
    "        reservation_price = (best_bid + best_ask) / 2\n",
    "        spread = best_ask - best_bid\n",
    "        \n",
    "        bid_price, ask_price = avellaneda_stoikov(reservation_price, spread)\n",
    "        amount = 0.001  # Adjust based on capital\n",
    "        \n",
    "        try:\n",
    "            exchange.create_limit_buy_order(symbol, amount, bid_price)\n",
    "            exchange.create_limit_sell_order(symbol, amount, ask_price)\n",
    "            logging.info(f\"Placed bid: {bid_price}, ask: {ask_price}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Order placement failed: {e}\")\n",
    "        \n",
    "        time.sleep(0.01)  # Control latency\n",
    "\n",
    "# Medium-Frequency: LSTM-Based Trend Prediction\n",
    "def lstm_trend_strategy(symbol='BTC/USDT', timeframe='1h'):\n",
    "    # Fetch historical data\n",
    "    ohlcv = exchange.fetch_ohlcv(symbol, timeframe, limit=1000)\n",
    "    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    \n",
    "    # Feature engineering\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['rsi'] = talib.RSI(df['close'], timeperiod=14)\n",
    "    df['ma_short'] = df['close'].rolling(window=10).mean()\n",
    "    df['ma_long'] = df['close'].rolling(window=50).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    features = ['returns', 'rsi', 'ma_short', 'ma_long']\n",
    "    X = df[features].values\n",
    "    y = (df['close'].shift(-1) > df['close']).astype(int)  # Predict price increase\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Reshape for LSTM [samples, timesteps, features]\n",
    "    X_lstm = np.array([X_scaled[i-10:i] for i in range(10, len(X_scaled))])\n",
    "    y_lstm = y[10:]\n",
    "    \n",
    "    # Build LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(10, len(features))))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_lstm, y_lstm, epochs=10, batch_size=32, verbose=0წ\n",
    "\n",
    "    # Predict\n",
    "    latest_data = X_scaled[-10:]\n",
    "    latest_data = latest_data.reshape(1, 10, len(features))\n",
    "    prediction = model.predict(latest_data)\n",
    "    signal = 1 if prediction[0] > 0.5 else -1\n",
    "    \n",
    "    # Execute trades\n",
    "    if signal == 1:\n",
    "        exchange.create_market_buy_order(symbol, 0.001)\n",
    "        logging.info(f\"Bought at {df['close'].iloc[-1]}\")\n",
    "    elif signal == -1:\n",
    "        exchange.create_market_sell_order(symbol, 0.001)\n",
    "        logging.info(f\"Sold at {df['close'].iloc[-1]}\")\n",
    "\n",
    "# Risk Management\n",
    "def check_risk(position_size, max_exposure=0.1):\n",
    "    if position_size > max_exposure:\n",
    "        logging.warning(\"Exposure exceeds limit!\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Run strategies\n",
    "    market_making_strategy('BTC/USDT')\n",
    "    lstm_trend_strategy('BTC/USDT', '1h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e90c1-e599-4cb3-8b2a-7f178bce50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate returns\n",
    "window = 60\n",
    "np.random.seed(42)\n",
    "stock_returns = np.random.normal(0.0005, 0.01, window)\n",
    "etf_returns = np.random.normal(0.0004, 0.008, window)\n",
    "\n",
    "# Standardize\n",
    "stock_returns_standardised = (stock_returns - stock_returns.mean())/stock_returns.std()\n",
    "etf_returns_standardised = (etf_returns - etf_returns.mean())/etf_returns.std()\n",
    "\n",
    "# Run regression of stock returns on ETF returns\n",
    "etf_returns_with_const = sm.add_constant(etf_returns_standardised)\n",
    "model = sm.OLS(stock_returns_standardised, etf_returns_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# Calculate the residuals from the regression (idiosyncratic returns)\n",
    "residuals = results.resid\n",
    "\n",
    "# Fit an AR(1) model to the residuals\n",
    "ar_model = AutoReg(residuals, lags=1)\n",
    "ar_results = ar_model.fit()\n",
    "\n",
    "# Obtain the autocorrelation coefficients 'a' and 'b' from the AR(1) model\n",
    "a = ar_results.params[0] \n",
    "b = ar_results.params[1] \n",
    "\n",
    "# Calculate the signal\n",
    "s_score = -a * np.sqrt(1 - b**2) / ((1 - b) * np.sqrt(np.var(residuals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61954bd-d840-4c1f-b02b-3c6d4a0a1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TR(data)\n",
    "\n",
    "    # Calculate True Range (TR)\n",
    "    data['TR'] = np.maximum(data['High'] - data['Low'], \n",
    "                            np.maximum(abs(data['High'] - data['Close'].shift(1)), \n",
    "                                       abs(data['Low'] - data['Close'].shift(1))))\n",
    "def ADX(data):\n",
    "\n",
    "    # Calculate +DM and -DM\n",
    "    data['+DM'] = np.where((data['High'] - data['High'].shift(1)) > (data['Low'].shift(1) - data['Low']), \n",
    "                           np.maximum(data['High'] - data['High'].shift(1), 0), 0)\n",
    "    data['-DM'] = np.where((data['Low'].shift(1) - data['Low']) > (data['High'] - data['High'].shift(1)), \n",
    "                           np.maximum(data['Low'].shift(1) - data['Low'], 0), 0)\n",
    "\n",
    "    # Smooth TR, +DM, and -DM using a 14-period average\n",
    "    data['TR14'] = data['TR'].rolling(window=14).sum()\n",
    "    data['+DM14'] = data['+DM'].rolling(window=14).sum()\n",
    "    data['-DM14'] = data['-DM'].rolling(window=14).sum()\n",
    "\n",
    "    # Calculate +DI and -DI\n",
    "    data['+DI'] = 100 * (data['+DM14'] / data['TR14'])\n",
    "    data['-DI'] = 100 * (data['-DM14'] / data['TR14'])\n",
    "\n",
    "    # Calculate DX and ADX\n",
    "    data['DX'] = 100 * abs(data['+DI'] - data['-DI']) / (data['+DI'] + data['-DI'])\n",
    "    data['ADX'] = data['DX'].rolling(window=14).mean()\n",
    "\n",
    "    # Display the result\n",
    "    print(data[['High', 'Low', 'Close', 'ADX']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea7387-12bc-4bb3-b0de-4236258f8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMA, MMA, SMA signals\n",
    "data.long_sell = ((data.close_price > data.ma6) & (data.close_price > data.ma20) & (data.close_price > data.ma40) & (data.close_price > data.ma120)).astype(int)  # 1=bull, 0=bear\n",
    "data.long_buy = ((data.close_price < data.ma6) & (data.close_price < data.ma20) & (data.close_price < data.ma40) & (data.close_price < data.ma120)).astype(int)  # 1=bull, 0=bear\n",
    "\n",
    "data.short_sell = (data.close_price > data.ma6) #& (data.Close < data.ma120))\n",
    "data.short_buy = (data.close_price < data.ma6) #& (data.Close < data.ma20))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27ac81-4498-4f9e-89be-8bc72e3909e7",
   "metadata": {},
   "source": [
    "\n",
    "**up**\\\n",
    "linear trend is positive\\\n",
    "if pressure is negative -> returns is negative on the same day.\\\n",
    "if buy_orders are less than 50% than pres2 == 1 -> price will drop. \\\n",
    "if RSI > 70 = overbought\\\n",
    "if d% > 80 = overbought \\\n",
    "if x < lma & x > sma\n",
    "MACD > 0\n",
    "ROC > 0\n",
    "if increase OBV and price, then up\n",
    "AD+ > AD- and ADX > 25 strong\n",
    "momentum = 1\n",
    "\n",
    "**Down**\\\n",
    "linear trend is negative\\\n",
    "if pressure is positive -> returns is positive on the same day.\\\n",
    "if buy_orders are more than 50% than pres2 == 0 -> price will increase.\\\n",
    "if RSI < 30 = oversold\\\n",
    "if d% < 20 = oversold\\\n",
    "if x > lma & x < sma?\n",
    "MACD < 0\n",
    "ROC < 0\n",
    "if decrease OBV and price, then down\n",
    "AD+ < AD- and ADX > 25 strong\n",
    "momentum = -1\n",
    "\n",
    "\n",
    "if k% x d% change of momentum?\n",
    "if lma x sma change of momentum?\n",
    "if MACD x signal change of momentum?\n",
    "if diverging OBV and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8d02a-422a-4162-b420-8d5c3131125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "# 1.  Load & resample minute data  --> hourly df with close, volume\n",
    "df = raw.resample('60T').agg({'open':'first','high':'max','low':'min',\n",
    "                              'close':'last','volume':'sum'}).dropna()\n",
    "\n",
    "# 2.  VWTS momentum\n",
    "lookback = 24\n",
    "returns  = df['close'].pct_change()\n",
    "vwts_num = (df['volume']*returns).rolling(lookback).sum()\n",
    "vwts_den = df['volume'].rolling(lookback).sum()\n",
    "df['vwts'] = vwts_num / vwts_den\n",
    "\n",
    "# 3.  Vol‑scaled MACD\n",
    "ema_fast = df['close'].ewm(span=12).mean()\n",
    "ema_slow = df['close'].ewm(span=26).mean()\n",
    "macd_raw = ema_fast - ema_slow\n",
    "vol_26   = returns.rolling(26).std()\n",
    "df['macd_s'] = macd_raw / vol_26\n",
    "\n",
    "# 4.  Z‑score & combined signal\n",
    "for col in ['vwts','macd_s']:\n",
    "    df[f'z_{col}'] = (df[col] - df[col].rolling(48).mean()) / df[col].rolling(48).std()\n",
    "df['signal_raw'] = 0.5*df['z_vwts'] + 0.5*df['z_macd_s']\n",
    "df['dir']        = np.sign(df['signal_raw'])\n",
    "\n",
    "# 5.  Position sizing\n",
    "target_vol = 0.0126   # 20 % annualised\n",
    "real_vol   = returns.rolling(24).std()\n",
    "df['pos']  = df['dir'] * (target_vol / real_vol).clip(0, 3)\n",
    "\n",
    "# 6.  P&L and performance\n",
    "df['pnl'] = df['pos'].shift()*returns - 0.0002*df['pos'].diff().abs()\n",
    "sharpe    = np.sqrt(24*365) * df['pnl'].mean() / df['pnl'].std()\n",
    "print('Sharpe', sharpe)\n",
    "\n",
    "class agent:\n",
    "    \n",
    "    def __init__(self, cash, n):\n",
    "        \n",
    "        self.i = 0\n",
    "        self.account = []\n",
    "        self.buy_amounts = []\n",
    "        self.buy_prices = []\n",
    "        for i in range(n):\n",
    "            self.account.append(cash/n)\n",
    "            self.buy_amounts.append(0)\n",
    "            self.buy_prices.append(0)\n",
    "        \n",
    "        \n",
    "    def buy(self, buy, size):\n",
    "        \n",
    "        for i in range(len(self.account)):\n",
    "            if self.account[i] != 0:\n",
    "                self.buy_amounts[i] = (self.account[i]/buy) * size\n",
    "                self.buy_prices[i] = buy\n",
    "                self.account[i] = 0\n",
    "                break\n",
    "\n",
    "    def sell(self, sell, size):\n",
    "        \n",
    "        for i in range(len(self.account)):\n",
    "            if self.account[i] == 0:\n",
    "                self.account[i] = self.buy_amounts[i] * sell * size\n",
    "                self.buy_amounts[i] = 0\n",
    "                self.buy_prices[i] = 0\n",
    "                break\n",
    "        \n",
    "    def pnl(self):\n",
    "        return(sum(self.account))\n",
    "    \n",
    "cash = 100\n",
    "A = agent(cash, 1)\n",
    "buy = False\n",
    "roi_lst = []\n",
    "\n",
    "for i in range(1,len(data)-1):\n",
    "    \n",
    "    #if market_state[i] == \"mean-reversion\":\n",
    "    \n",
    "    if data[\"state\"][i] < 0:\n",
    "        A.buy(data.close_price[i+1], 1)\n",
    "        b_p = data.close_price[i+1]\n",
    "\n",
    "\n",
    "    if data[\"state\"][i] < 0 and data.close_price[i] > 1.03 * b_p:\n",
    "        A.sell(data.close_price[i+1], 1)\n",
    "    \n",
    "    #elif market_state[i] == \"up\":\n",
    "        \n",
    "        #if data.up_buy[i] == 1:\n",
    "            #A.buy(data.close_price[i+1])\n",
    "\n",
    "\n",
    "        #if data.up_sell[i] == 1:\n",
    "            #A.sell(data.close_price[i+1])\n",
    "    \n",
    "    \n",
    "    #elif market_state[i] == \"down\":\n",
    "        \n",
    "        #if data.up_buy[i] == 1:\n",
    "            #A.buy(data.close_price[i+1])\n",
    "\n",
    "\n",
    "        #if data.up_sell[i] == 1:\n",
    "            #A.sell(data.close_price[i+1])\n",
    "\n",
    "    \n",
    "    \n",
    "    # last moment sells - just to see position values\n",
    "    if i > len(data)-5:\n",
    "        for _ in range(10):\n",
    "            A.sell(data.close_price[i+1], 1)\n",
    "    \n",
    "    roi = (sum(A.account) / cash - 1) * 100\n",
    "    roi_lst.append(roi)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(\"end ROI: \", round(roi,2), \"% and max ROI:\", round(max(roi_lst),2), \"%\")\n",
    "\n",
    "sum(abs(data.returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7a2f8-ba1a-404f-99f6-9745705d1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prices)): if adx[i-1] < 25 and adx[i] > 25 and pdi[i] > ndi[i]: if signal != 1: buy_price.append(prices[i]) sell_price.append(np.nan) signal = 1 indicator.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0) elif adx[i-1] < 25 and adx[i] > 25 and ndi[i] > pdi[i]: if signal != -1: buy_price.append(np.nan) sell_price.append(prices[i]) signal = -1 indicator.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0)\n",
    "\n",
    "for i in range(len(data)): if data['macd'][i] > data['signal'][i]: if signal != 1: buy_price.append(prices[i]) sell_price.append(np.nan) signal = 1 indicator.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0) elif data['macd'][i] < data['signal'][i]: if signal != -1: buy_price.append(np.nan) sell_price.append(prices[i]) signal = -1 indicator.append(signal) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0) else: buy_price.append(np.nan) sell_price.append(np.nan) indicator.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f303b-bccd-4232-afa3-1580396381ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aligns with “build capital measures (SA-CCR/IMM) for FX & IR products” (Quantile ad) and XVA/CCR roles. You can fork the open-source sa-ccr-python implementation as ground truth\n",
    "\n",
    "https://github.com/sa-ccr/sa-ccr-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976f2ce-588c-48a3-8842-6937b9e957ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA['STATE'] = \"\"\n",
    "interval_size = 96\n",
    "m_i = 5\n",
    "m_i2 = 2\n",
    "for i in range(len(DATA)):\n",
    "\n",
    "    l_state = \"\"\n",
    "    m_state = \"\"\n",
    "    s_state = \"\"\n",
    "    st = \"\"\n",
    "    \n",
    "    if i <= m_i*interval_size:\n",
    "        DATA['STATE'][i] = \"\"\n",
    "\n",
    "    elif i > m_i*interval_size:\n",
    "        \n",
    "        \n",
    "        # long state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) > 0.05:\n",
    "            l_state = \"lu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) < 0:\n",
    "            l_state = \"ld\"\n",
    "\n",
    "\n",
    "        # medium state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) > 0.02:\n",
    "            m_state = \"mu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) < 0:\n",
    "            m_state = \"md\"\n",
    "\n",
    "            \n",
    "        # short state check    \n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) > 0.01:\n",
    "            s_state = \"su\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) < -0.05:\n",
    "            s_state = \"sd\"\n",
    "        \n",
    "        \n",
    "        DATA['STATE'][i] = l_state + m_state + s_state\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59ca01-5b36-44c6-8798-901607292ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['STATE'] == \"lumusu\"].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "long_signal_dates = DATA[DATA['STATE'] == \"ldmdsd\"].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b7de8-e024-44f5-b35f-dacaf042593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(DATA)):\n",
    "    \n",
    "    #if i > 1920:\n",
    "\n",
    "        #if np.std(DATA['Close'][i-(20*interval_size):i]) > 0.03 * np.mean(DATA['Close'][i-(20*interval_size):i]):\n",
    "            #if np.std(DATA['Close'][i-(10*interval_size):i]) > 0.03 * np.mean(DATA['Close'][i-(10*interval_size):i]): \n",
    "                #if np.std(DATA['Close'][i-(2*interval_size):i]) > 0.015 * np.mean(DATA['Close'][i-(2*interval_size):i]):\n",
    "                    #DATA['STATE'][i] = DATA['STATE'][i] + \"ST\"\n",
    "            \n",
    "            \n",
    "        #if np.std(DATA['returns'][i-(20*interval_size):i]) > 0.007:\n",
    "            #if np.std(DATA['returns'][i-(10*interval_size):i]) > 0.007:\n",
    "                #if np.std(DATA['returns'][i-(2*interval_size):i]) > 0.007:\n",
    "                    #DATA['STATE'][i] = DATA['STATE'][i] + \"ST\"\n",
    "\n",
    "DATA['CONV_H'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if np.sum(DATA['h_returns'][i-700:i]) > 0 and np.sum(DATA['h_returns'][i-700:i-500]) > np.sum(DATA['h_returns'][i-500:i-250]) and np.sum(DATA['h_returns'][i-250:i-125]) > np.sum(DATA['h_returns'][i-125:i-50]) and np.sum(DATA['h_returns'][i-125:i-50]) > np.sum(DATA['h_returns'][i-50:i]):\n",
    "        if np.sum(DATA['l_returns'][i-700:i]) > 0 and np.sum(DATA['l_returns'][i-700:i-500]) > np.sum(DATA['l_returns'][i-500:i-250]) and np.sum(DATA['l_returns'][i-250:i-125]) > np.sum(DATA['l_returns'][i-125:i-50]) and np.sum(DATA['l_returns'][i-125:i-50]) > np.sum(DATA['l_returns'][i-50:i]):\n",
    "            DATA['CONV_H'][i] = 2\n",
    "#\n",
    "DATA['CONV_L'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if np.sum(DATA['l_returns'][i-700:i]) < 0 and np.sum(DATA['l_returns'][i-700:i-500]) < np.sum(DATA['l_returns'][i-500:i-250]) and np.sum(DATA['l_returns'][i-250:i-125]) < np.sum(DATA['l_returns'][i-125:i-50]) and np.sum(DATA['l_returns'][i-125:i-50]) < np.sum(DATA['l_returns'][i-50:i]):\n",
    "        if np.sum(DATA['h_returns'][i-700:i]) < 0 and np.sum(DATA['h_returns'][i-700:i-500]) < np.sum(DATA['h_returns'][i-500:i-250]) and np.sum(DATA['h_returns'][i-250:i-125]) < np.sum(DATA['h_returns'][i-125:i-50]) and np.sum(DATA['h_returns'][i-125:i-50]) < np.sum(DATA['h_returns'][i-50:i]):\n",
    "            DATA['CONV_L'][i] = -2\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['CONV_H'] == 2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "long_signal_dates = DATA[DATA['CONV_L'] == -2].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd80d30-5bd8-4ce6-b900-3815bd06daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA['MPN'] = 0\n",
    "for i in range(700, len(DATA)):\n",
    "    if (np.mean(DATA['PN_counter'][i-96:i]) > 0 and sum(DATA['returns'][i-96:i]) > 0) or (np.mean(DATA['PN_counter'][i-48:i]) > 0.05 and sum(DATA['returns'][i-48:i]) > 0.05):\n",
    "        DATA['MPN'][i] = 1\n",
    "    elif (np.mean(DATA['PN_counter'][i-96:i]) < 0 and sum(DATA['returns'][i-96:i]) < 0) or (np.mean(DATA['PN_counter'][i-48:i]) < -0.01 and sum(DATA['returns'][i-48:i]) < -0.01):\n",
    "        DATA['MPN'][i] = -1\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(23, 13))\n",
    "ax1.set_facecolor('black')  \n",
    "\n",
    "long_signal_dates = DATA[DATA['MPN'] == 1].index\n",
    "for date in long_signal_dates:\n",
    "    ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='green', fontsize=14, ha='center')\n",
    "    \n",
    "#long_signal_dates = DATA[DATA['MPN'] == -1].index\n",
    "#for date in long_signal_dates:\n",
    "    #ax1.annotate('x', xy=(date, DATA.loc[date, 'Close']), color='red', fontsize=14, ha='center')\n",
    "    \n",
    "ax1.plot(DATA.index, DATA.Open, color = \"white\", linewidth = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7062286-b0b5-467d-950c-c0315030fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MA_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and data['value'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and data['MA20'][i] < data['MA40'][i]:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and data['value'][i] > data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and data['MA20'][i] > data['MA40'][i]:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "    \n",
    "\n",
    "def Q_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and sum(data['returns'][i-20:i]) < data['Q1'][i]:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and sum(data['returns'][i-20:i]) > data['Q2'][i]:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "    \n",
    "\n",
    "def PN_signal(data):\n",
    "    signal = []\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        # buy signal\n",
    "        if buy == False and np.mean(data['PN_counter'][i-30:i]) > 0:\n",
    "            signal.append(1)\n",
    "            buy = True\n",
    "        \n",
    "        # sell signal\n",
    "        elif buy == True and np.mean(data['PN_counter'][i-30:i]) < 0:\n",
    "            signal.append(-1)\n",
    "            buy = False\n",
    "            \n",
    "        else:\n",
    "            signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def C_signal(data, interval_size = 1):\n",
    "    signal = []\n",
    "    l_state = None\n",
    "    s_state = None\n",
    "    sqr = None\n",
    "    buy = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i <= 50 * interval_size:\n",
    "            signal.append(0)\n",
    "        \n",
    "        elif i > 50 * interval_size:\n",
    "        \n",
    "        \n",
    "            # long state checks\n",
    "            if np.mean(data['PN_counter'][i-(20 * interval_size):i]) > 0.01:\n",
    "                l_state = \"up\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(20 * interval_size):i]) < -0.01:\n",
    "                l_state = \"down\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(20 * interval_size):i]) < 0.01 and np.mean(data['PN_counter'][i-(20 * interval_size):i]) > -0.01:\n",
    "                l_state = \"flat\"\n",
    "\n",
    "            else:\n",
    "                l_state = None\n",
    "\n",
    "\n",
    "            # short state checks\n",
    "            if np.mean(data['PN_counter'][i-(7 * interval_size):i]) > 0.02:\n",
    "                s_state = \"up\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(7 * interval_size):i]) < -0.02:\n",
    "                s_state = \"down\"\n",
    "\n",
    "            elif np.mean(data['PN_counter'][i-(7 * interval_size):i]) < 0.02 and np.mean(data['PN_counter'][i-(7 * interval_size):i]) > -0.02:\n",
    "                s_state = \"flat\"\n",
    "\n",
    "            else:\n",
    "                s_state = None\n",
    "\n",
    "\n",
    "\n",
    "            #special events\n",
    "\n",
    "            if data['Close'][i] > np.quantile(data['Close'][i-(50 * interval_size):i], 0.99) or \\\n",
    "            data['Open'][i] > np.quantile(data['Open'][i-(50 * interval_size):i], 0.99):\n",
    "                s_state = \"down\"\n",
    "                l_state = \"down\"\n",
    "            \n",
    "            elif data['Close'][i] > np.quantile(data['Close'][i-(50 * interval_size):i], 0.01) or \\\n",
    "            data['Open'][i] > np.quantile(data['Open'][i-(50 * interval_size):i], 0.01):\n",
    "                s_state = \"up\"\n",
    "                l_state = \"up\"\n",
    "                \n",
    "            #np.mean(data['PN_counter'][i-20:i]) > 0 and np.mean(data['PN_counter'][i-5:i]) > 0 \n",
    "            #np.mean(data['PN_counter'][i-20:i]) < 0 and np.mean(data['PN_counter'][i-5:i]) < 0\n",
    "\n",
    "\n",
    "            #data['Open'][i-2] > data['Open'][i-1] and data['Open'][i-1] < data['Open'][i]\n",
    "            #data['Open'][i-2] < data['Open'][i-1] and data['Open'][i-1] > data['Open'][i]\n",
    "\n",
    "            # buy signal\n",
    "            if buy == False and \\\n",
    "            l_state == \"up\" and s_state == \"up\":\n",
    "                signal.append(1)\n",
    "                buy = True\n",
    "                \n",
    "            elif buy == False and \\\n",
    "            l_state == \"down\" and s_state == \"up\":\n",
    "                signal.append(1)\n",
    "                buy = True\n",
    "\n",
    "            \n",
    "            # sell signal\n",
    "            elif buy == True and \\\n",
    "            l_state == \"up\" and s_state == \"down\" :\n",
    "                signal.append(-1)\n",
    "                buy = False\n",
    "                \n",
    "            elif buy == True and \\\n",
    "            l_state == \"down\" and s_state == \"down\" :\n",
    "                signal.append(-1)\n",
    "                buy = False\n",
    "\n",
    "            else:\n",
    "                signal.append(0)\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "#buy\n",
    "data['Open'][i] < data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and \\\n",
    "#sell\n",
    "data['Open'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and \\\n",
    "\n",
    "def sharpe(data):\n",
    "    return (((val-100)/100)/(np.std(data['Close'])/np.mean(data['Close'])))\n",
    "\n",
    "def sharpe2(data):\n",
    "    s = data.iloc[0,0]\n",
    "    std = np.std(data.iloc[:,0])\n",
    "    s = (data.iloc[-1,0] - s) / std\n",
    "    return s\n",
    "    \n",
    "\n",
    "def strategy(data, signal, fee):\n",
    "\n",
    "    strategy_df = pd.DataFrame()\n",
    "    position = np.zeros(len(data))\n",
    "    buy = False\n",
    "    \n",
    "    for i in range(len(signal)):\n",
    "        \n",
    "        if signal[i] == 1 and buy == False:\n",
    "            position[i-1] = 1\n",
    "            buy = True\n",
    "\n",
    "        if signal[i] == -2 and buy == False:\n",
    "            position[i-1] = -2\n",
    "            buy = True\n",
    "            \n",
    "        if signal[i] == -1 and buy == True:\n",
    "            position[i-1] = -1\n",
    "            buy = False\n",
    "\n",
    "        if signal[i] == 0:\n",
    "            position[i-1] = 0\n",
    "\n",
    "\n",
    "    strategy_df['Open'] = data['Open']\n",
    "    strategy_df['High'] = data['High']\n",
    "    strategy_df['Low'] = data['Low']\n",
    "    strategy_df['Close'] = data['Close']\n",
    "    strategy_df['Volume'] = data['Volume']    \n",
    "    strategy_df['position'] = position\n",
    "    strategy_df['acc'] = position\n",
    "    \n",
    "    acc = 1\n",
    "    #val = 1\n",
    "    strategy_df['acc'][0] = acc\n",
    "    buy_price = strategy_df['Close'][0]\n",
    "    buy = False\n",
    "    short = False\n",
    "    \n",
    "    for i in range(1,len(strategy_df)):\n",
    "        \n",
    "        tck = acc / strategy_df['Close'][i]\n",
    "\n",
    "        if strategy_df['position'][i] == 1 and buy == False and short == False:\n",
    "            buy_price = strategy_df['Close'][i]\n",
    "            acc = acc * (1-fee)\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            #val = val * (acc / buy_price) * (1-fee)\n",
    "            strategy_df['acc'][i] = tck * strategy_df['Close'][i]\n",
    "            buy = True\n",
    "        \n",
    "        elif strategy_df['position'][i] == -1 and buy == True:\n",
    "            sell_price = strategy_df['Close'][i]\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            dp = (sell_price - buy_price) * tck #/ buy_price\n",
    "            acc = (acc + dp) * (1-fee)\n",
    "            strategy_df['acc'][i] = acc\n",
    "            buy = False\n",
    "            \n",
    "        if strategy_df['position'][i] == -2 and buy == False and short == False:\n",
    "            buy_price = strategy_df['Close'][i]\n",
    "            acc = acc * (1-fee)\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            #val = val * (acc / buy_price) * (1-fee)\n",
    "            strategy_df['acc'][i] = tck * strategy_df['Close'][i]\n",
    "            short = True\n",
    "            #sit\n",
    "            \n",
    "        elif strategy_df['position'][i] == -1 and short == True:\n",
    "            sell_price = strategy_df['Close'][i]\n",
    "            tck = acc / strategy_df['Close'][i]\n",
    "            dp = (buy_price - sell_price) * tck #/ buy_price\n",
    "            acc = (acc + dp) * (1-fee)\n",
    "            strategy_df['acc'][i] = acc\n",
    "            short = False\n",
    "\n",
    "        else:\n",
    "            if buy == True:\n",
    "                strategy_df['acc'][i] = acc #+ (strategy_df['Close'][i] - buy_price) / buy_price\n",
    "            elif buy == False:\n",
    "                strategy_df['acc'][i] = acc #+ (strategy_df['Close'][i] - sell_price)*tck #/ sell_price\n",
    "    \n",
    "    return strategy_df\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def C_signal(data):\n",
    "    \n",
    "    buy = False\n",
    "    signal = [] # 1 long, -2 short, -1 close position\n",
    "    market_state = \"None\"\n",
    "    temp_state = \"None\"\n",
    "    prev_state = \"None\"\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "            \n",
    "        \n",
    "            if data['STATE'][i] == \"lumusu\" or data['STATE'][i] == \"lumusuST\":\n",
    "                market_state = \"UP\"\n",
    "                j = i\n",
    "                \n",
    "            elif data['STATE'][i]  == \"ldmdsd\" or data['STATE'][i]  == \"ldmdsdST\":\n",
    "                market_state = \"DOWN\"\n",
    "                j = i\n",
    "                \n",
    "            elif data['STATE'][i]  == \"ee\" or data['STATE'][i]  == \"eeST\":\n",
    "                market_state = \"EE\"\n",
    "                j = i\n",
    "                \n",
    "            elif i > j + 4*74 and data['STATE'][i] not in ['lumusu','lumusuST','ldmdsd','ldmdsdST','ee','eeST']:\n",
    "                market_state = \"FLAT\"\n",
    "                j = i\n",
    "            \n",
    "            if market_state != prev_state:\n",
    "                prev_state = temp_state\n",
    "            \n",
    "            \n",
    "            if market_state == \"UP\":\n",
    "                if buy == False and np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(1)\n",
    "                    \n",
    "                elif buy == True and np.mean(data['Open'][i-96:i-50]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(-1)\n",
    "                    \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "                \n",
    "                \n",
    "            if market_state == \"DOWN\":\n",
    "                if buy == True and np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                    signal.append(-1)\n",
    "                    buy = False\n",
    "                    \n",
    "                #elif buy == False and np.mean(data['Open'][i-96:i-76]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                    #signal.append(-2)\n",
    "                    #buy = True\n",
    "                    \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "                \n",
    "            \n",
    "            if market_state == \"EE\":\n",
    "                signal.append(0)\n",
    "                #if prev_state == \"UP\" and buy == True:\n",
    "                    #signal.append(-1)\n",
    "                    #buy = False\n",
    "                        \n",
    "                #elif prev_state == \"DOWN\" and buy == False:\n",
    "                    #signal.append(1)\n",
    "                    #buy = True\n",
    "                    \n",
    "                #else:\n",
    "                    #signal.append(0)\n",
    "                \n",
    "                \n",
    "            if market_state == \"FLAT\":\n",
    "                \n",
    "                if buy == False and data['Close'][i] > data['MA5'][i] and data['MA5'][i] < data['MA20'][i] and data['MA20'][i] < data['MA40'][i]:\n",
    "                    if np.mean(data['Open'][i-96:i-50]) > np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) > np.mean(data['Open'][i-48:i-38]):\n",
    "                        signal.append(1)\n",
    "                        buy = True\n",
    "                \n",
    "                elif buy == True and data['Close'][i] > data['MA5'][i] and data['MA5'][i] > data['MA20'][i] and data['MA20'][i] > data['MA40'][i]:\n",
    "                    if np.mean(data['Open'][i-96:i-50]) < np.mean(data['Open'][i-48:i-38]) and np.mean(data['Open'][i-6:i]) < np.mean(data['Open'][i-48:i-38]):\n",
    "                        signal.append(-1)\n",
    "                        buy = False\n",
    "                        \n",
    "                else:\n",
    "                    signal.append(0)\n",
    "            \n",
    "            temp_state = market_state\n",
    "                        \n",
    "    return(signal)\n",
    "\n",
    "R = np.zeros(len(DATA))\n",
    "R2 = RSI(DATA['Close'])\n",
    "for i in range(20, len(DATA)-1):\n",
    "    R[i] = R2[i]\n",
    "DATA['RSI'] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25925f06-dced-47f7-a933-5c88207cf312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UP_COND(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(3 * interval_size):i]) > 0.01 and sum(data['returns'][i-(3 * interval_size):i]) > 0.01 and data['acceleration'][i] > 1/100000000:\n",
    "        return True\n",
    "\n",
    "def UP_COND2(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(20 * interval_size):i]) > 0.005 and sum(data['returns'][i-(20 * interval_size):i]) > 0.005:\n",
    "        return True\n",
    "\n",
    "def O_TANG(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i]) > np.mean(data['Close'][i-(2 * interval_size):i]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i]) < np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "\n",
    "def ma_cond1(data, i):\n",
    "    if data['Close'][i] > data['MA5'][i] and data['Close'][i] < data['MA20'][i] and data['Close'][i] < data['MA40'][i]:\n",
    "        return True\n",
    "\n",
    "if np.mean(data['Close'][i-(3 * interval_size):i-(1 * interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]) and data['Close'][i] < np.mean(data['Close'][i-(1 * interval_size):i]) \n",
    "\n",
    "def UP_SELL(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(3 * interval_size):i]) < -0.01 and np.mean(data['returns'][i-(3 * interval_size):i]) < 0:\n",
    "        return True\n",
    "\n",
    "def DOWN_COND(data, i, interval_size):\n",
    "    if np.mean(data['PN_counter'][i-(20 * interval_size):i]) < 0 and sum(data['returns'][i-(20 * interval_size):i]) < 0:\n",
    "        if np.mean(data['PN_counter'][i-(3 * interval_size):i]) < -0.01 and sum(data['returns'][i-(3 * interval_size):i]) < -0.01:\n",
    "            return True\n",
    "\n",
    "if np.mean(data['Close'][i-(3 * interval_size):i-(2 * interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]):\n",
    "            if np.mean(data['Close'][i-(2 * interval_size):i-(1 * interval_size)]) > np.mean(data['Close'][i-(1 * interval_size):i]):\n",
    "                #if np.mean(data['Close'][i-(1 * interval_size):i]) < np.mean(data['Close'][i-(3 * interval_size):i-(2 * interval_size)]):\n",
    "\n",
    "def MA_SELL_COND(data, i):\n",
    "    if data['Close'][i] < data['MA5'][i] and data['Close'][i] > data['MA20'][i] and data['Close'][i] > data['MA40'][i]:\n",
    "        return True\n",
    "\n",
    "def IO_TANG(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i]) < np.mean(data['Close'][i-(2 * interval_size):i]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i]) > np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "\n",
    "def IO_TANG2(data, i, interval_size):\n",
    "    if np.mean(data['Close'][i-(3 * interval_size):i-(2*interval_size)]) < np.mean(data['Close'][i-(2 * interval_size):i-(1*interval_size)]):\n",
    "        if np.mean(data['Close'][i-(2 * interval_size):i-(1*interval_size)]) > np.mean(data['Close'][i-20:i]):\n",
    "            return True\n",
    "\n",
    "# combining signals\n",
    "\n",
    "def A_signal(data, interval_size = 1):\n",
    "    signal = np.zeros(len(data))\n",
    "    buy = False\n",
    "    buy_price = 0\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i > 100: #* interval_size:\n",
    "        \n",
    "            #if data['STATE'][i] == \"lumusu\":\n",
    "\n",
    "                #if np.mean(data['PN_counter'][i-(10 * interval_size):i]) > 0.01 and np.mean(data['PN_counter'][i-(3 * interval_size):i]) > -0.02 and sum(data['returns'][i-(20 * interval_size):i]) > 0.005:\n",
    "\n",
    "            if buy == False and ma_cond1(data,i) == True:#UP_COND(data, i, interval_size) == True and  UP_COND2(data,i, interval_size) == True and O_TANG(data, i, interval_size) == True:\n",
    "                signal[i] = 1\n",
    "                buy = True\n",
    "                buy_price = data['Close'][i]\n",
    "\n",
    "                #elif buy == False and data['Close'][i-24] == min(data['Close'][200:i]):\n",
    "                    #signal[i] = 1\n",
    "                    #buy = True\n",
    "                    #buy_price = data['Close'][i]\n",
    "\n",
    "                #if buy == False and DOWN_COND(data, i , interval_size) == True:\n",
    "                    #signal[i] = -1\n",
    "                    #buy = True\n",
    "                    #buy_price = data['Close'][i]\n",
    "\n",
    "\n",
    "            if buy == True and MA_SELL_COND(data,i) == True and (DATA['Close'][i] > 1.05 * buy_price or DATA['Close'][i] < 0.98 * buy_price):#UP_SELL(data,i,interval_size):\n",
    "                #if IO_TANG(data,i,interval_size) == True or IO_TANG2(data,i, interval_size) == True:\n",
    "                signal[i] = -1\n",
    "                buy = False\n",
    "\n",
    "            #elif data['Close'][i] > 1.5 * buy_price and buy == True:\n",
    "                #signal[i] = -1\n",
    "                #buy = False\n",
    "\n",
    "            elif data['Close'][i] < 0.95 * buy_price and buy == True:\n",
    "                signal[i] = -1\n",
    "                buy = False\n",
    "    \n",
    "    return(signal)\n",
    "\n",
    "sig_ = A_signal(DATA,96)\n",
    "strategy_df = strategy(DATA, sig_, 0.005)\n",
    "\n",
    "#price curve\n",
    "#PRICE_plot(DATA)\n",
    "# stochastic oscillator plot\n",
    "#STO_plot(DATA_S)\n",
    "# Average directional index plot\n",
    "#ADX_plot(DATA_AD)\n",
    "\n",
    "#MACD_plot(DATA.Close, DATA_MACD['macd'], DATA_MACD['signal'], DATA_MACD['hist'])\n",
    "\n",
    "print(np.std(strategy_df.acc),(strategy_df['acc'][len(strategy_df)-1]-1))\n",
    "\n",
    "TRADE_plot(strategy_df)\n",
    "\n",
    "SUM_plot(DATA, strategy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fea5b-531f-4164-b521-d843c6015fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "from binance.enums import *\n",
    "import numpy as np\n",
    "\n",
    "quantile = 0.05\n",
    "inter = \"1day\"\n",
    "\n",
    "api_key = 'vBrHMZUCDxlfNiAreBj02aUrymSFMGyl1AwJNAP0O7qVlvh07Drq7qQwfQlbYGeS'\n",
    "api_secret = 'SdzCkiFE5zNjkSvptRVpxQBxlUWZWYrjnRGLp5tzhzJiat79vymOH127zaKHnnCh'\n",
    "\n",
    "from binance import Client, ThreadedWebsocketManager, ThreadedDepthCacheManager\n",
    "client = Client(api_key, api_secret)\n",
    "\n",
    "today = date.today() \n",
    "today = datetime(today.year,today.month,today.day) + timedelta(days = 1) \n",
    "day7 = datetime(today.year,today.month,today.day) + timedelta(days = -1820) \n",
    "todays = today.strftime(\"%d-%m-%Y\")\n",
    "day7 = day7.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "tick = \"ADAUSDT\"\n",
    "    \n",
    "klines = client.get_historical_klines(tick, Client.KLINE_INTERVAL_15MINUTE, day7, todays)\n",
    "DATA = pd.DataFrame(klines, columns = ['Open time', 'Open','High','Low','Close','Volume','Close time','Quote asset volume','Number of trades','Taker buy base asset volume','Taker buy quote asset volume', 'Ignore'])\n",
    "\n",
    "DATA .Open = DATA.Open.astype(float)\n",
    "DATA .High = DATA.High.astype(float)\n",
    "DATA .Low = DATA.Low.astype(float)\n",
    "DATA .Close = DATA.Close.astype(float)\n",
    "DATA .Volume = DATA.Volume.astype(float)\n",
    "DATA  = DATA [['Open','High','Low','Close','Volume']]\n",
    "\n",
    "\n",
    "# Base data, Open, Low, High, Close, Volume, date points\n",
    "#DATA = sdata('BA','2021-09-01','2024-09-03')\n",
    "interval = 4*24\n",
    "\n",
    "#Stochastic oscillator of DATA\n",
    "DATA_S = STO(DATA, N = 20, M = 3)\n",
    "\n",
    "#ADX of DATA\n",
    "DATA_AD = ADX(DATA['High'], DATA['Low'], DATA['Close'], 20)\n",
    "\n",
    "#MACD of DATA\n",
    "DATA_MACD = MACD(DATA['Close'], 26, 12, 9)\n",
    "\n",
    "#SuperTrend of DATA\n",
    "#DATA_SUP = SuperTrend(DATA['High'], DATA['Low'], DATA['Close'], 20, 2)\n",
    "\n",
    "DATA['value'] = (DATA['High'] + DATA['Low']) / 2\n",
    "DATA.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "DATA = DATA.fillna(0)\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['Close'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['c_returns'] = a\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['High'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['h_returns'] = a\n",
    "\n",
    "a = list(TMRW.FINANCE.returns(DATA['Low'])['returns'])\n",
    "a.insert(0,0)\n",
    "DATA['l_returns'] = a\n",
    "\n",
    "ret = []\n",
    "for i in range(len(DATA)):\n",
    "    ret.append((DATA['c_returns'][i]+DATA['h_returns'][i]+DATA['l_returns'][i])/3)       \n",
    "DATA['returns'] = ret\n",
    "\n",
    "# skal smoothes\n",
    "a = list(TMRW.FINANCE.returns(DATA['returns'])['returns'])\n",
    "if len(a) != len(DATA):\n",
    "    for i in range(len(DATA) - len(a)):\n",
    "        a.insert(0,0)\n",
    "DATA['acceleration'] = a\n",
    "DATA.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "#\n",
    "DATA['MR_10'] = DATA['returns'].rolling(window=10).mean().values.flatten()\n",
    "DATA['MR_20'] = DATA['returns'].rolling(window=20).mean().values.flatten()\n",
    "DATA['MR_50'] = DATA['returns'].rolling(window=50).mean().values.flatten()\n",
    "DATA['MR_100'] = DATA['returns'].rolling(window=100).mean().values.flatten()\n",
    "DATA['MR_150'] = DATA['returns'].rolling(window=150).mean().values.flatten()\n",
    "\n",
    "#\n",
    "DATA['AC_48'] = DATA['value'].rolling(window=8).apply(lambda x: x.autocorr())\n",
    "DATA['AC_128'] = DATA['value'].rolling(window=28).apply(lambda x: x.autocorr())\n",
    "DATA['AC_480'] = DATA['value'].rolling(window=80).apply(lambda x: x.autocorr())\n",
    "\n",
    "# pos/neg counter\n",
    "pos_neg = []\n",
    "for i in range(len(DATA)):\n",
    "    if DATA['returns'][i] >= 0:\n",
    "        pos_neg.append(1)\n",
    "    elif DATA['returns'][i] < 0:\n",
    "        pos_neg.append(-1)        \n",
    "DATA['PN_counter'] = pos_neg\n",
    "\n",
    "\n",
    "DATA['Lower'] = list(TMRW.FINANCE.BB(DATA['Open'],20, std = 3.5)['Lower'])\n",
    "DATA['Upper'] = list(TMRW.FINANCE.BB(DATA['Open'],20, std = 3.5)['Upper'])\n",
    "\n",
    "# moving averages pris\n",
    "DATA['MA5'] = TMRW.FINANCE.twa(DATA['value'], 5*interval) # 5 day\n",
    "DATA['MA20'] = TMRW.FINANCE.twa(DATA['value'], 20*interval) # 20 day\n",
    "DATA['MA40'] = TMRW.FINANCE.twa(DATA['value'], 40*interval) # 40 day\n",
    "\n",
    "interval = 20\n",
    "\n",
    "Q95 = np.zeros(len(DATA))\n",
    "Q90 = np.zeros(len(DATA))\n",
    "Q10 = np.zeros(len(DATA))\n",
    "Q5 = np.zeros(len(DATA))\n",
    "for i in range(interval,len(DATA)):\n",
    "    Q95[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.98)\n",
    "    Q90[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.9)\n",
    "    Q10[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.1)\n",
    "    Q5[i] = np.quantile(DATA['returns'][i-interval:i-5], 0.02)\n",
    "\n",
    "DATA['Q98'] = Q95\n",
    "DATA['Q90'] = Q90\n",
    "DATA['Q10'] = Q10\n",
    "DATA['Q2'] = Q5\n",
    "\n",
    "UD3 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "for j in range(5, len(DATA)):\n",
    "    string = \"\"\n",
    "    for k in range(5):\n",
    "        if DATA['PN_counter'][j-(5-k)] == 1:\n",
    "            string = string + \"U\"\n",
    "        elif DATA['PN_counter'][j-(5-k)] == -1:\n",
    "            string = string + \"D\"\n",
    "\n",
    "    if j > 2:\n",
    "        UD3.append(string[2:5])\n",
    "\n",
    "    if j > 4:\n",
    "        UD5.append(string)\n",
    "\n",
    "DATA['UD3'] = UD3\n",
    "DATA['UD5'] = UD5\n",
    "\n",
    "UD3_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD3_U\")\n",
    "UD3_U.index = UD3_U['Unnamed: 0']\n",
    "UD3_U = UD3_U.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD5_U = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD5_U\")\n",
    "UD5_U.index = UD5_U['Unnamed: 0']\n",
    "UD5_U = UD5_U.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD3_D = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD3_D\")\n",
    "UD3_D.index = UD3_D['Unnamed: 0']\n",
    "UD3_D = UD3_D.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "UD5_D = pd.read_excel(open('C:/Users/Markb/OneDrive/Skrivebord/MarkovP.xlsx', 'rb'),sheet_name=\"UD5_D\")\n",
    "UD5_D.index = UD5_D['Unnamed: 0']\n",
    "UD5_D = UD5_D.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\n",
    "for i in range(len(DATA)):\n",
    "    DATA_ = DATA [['Open','High','Low','Close','Volume']]\n",
    "    if len(DATA_) < 1:\n",
    "        next\n",
    "    else:\n",
    "        DATA_['MA5'] = twa(DATA_['Close'],5)\n",
    "        DATA_['MA20'] = twa(DATA_['Close'],20)\n",
    "        DATA_['MA40'] = twa(DATA_['Close'],40)\n",
    "        DATA_['returns'] = returns(DATA_['Close'])\n",
    "        X = False\n",
    "        pos_neg = []\n",
    "        for j in range(len(DATA_)):\n",
    "            if DATA_['returns'][j] >= 0:\n",
    "                pos_neg.append(1)\n",
    "            elif DATA_['returns'][j] < 0:\n",
    "                pos_neg.append(-1)\n",
    "            else:\n",
    "                pos_neg.append(0)\n",
    "        DATA_['PN_counter'] = pos_neg\n",
    "        DATA_['MPN'] = 0\n",
    "        for j in range(19, len(DATA_)):\n",
    "            DATA_['MPN'][j] = np.mean(DATA_['PN_counter'][j-20:j])\n",
    "\n",
    "\n",
    "        UD3 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        UD5 = [\"\",\"\",\"\",\"\",\"\"]\n",
    "        for j in range(5, len(DATA_)):\n",
    "            string = \"\"\n",
    "            for k in range(5):\n",
    "                if DATA_['PN_counter'][j-(5-k)] == 1:\n",
    "                    string = string + \"U\"\n",
    "                elif DATA_['PN_counter'][j-(5-k)] == -1:\n",
    "                    string = string + \"D\"\n",
    "\n",
    "            if j > 2:\n",
    "                UD3.append(string[2:5])\n",
    "\n",
    "            if j > 4:\n",
    "                UD5.append(string)\n",
    "\n",
    "        DATA_['UD3'] = UD3\n",
    "        DATA_['UD5'] = UD5\n",
    "        \n",
    "        for j in range(10, len(DATA_)):\n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['UP'][DATA_['UD3'][j-1]] = UD3_U['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD3_U['DOWN'][DATA_['UD3'][j-1]] = UD3_U['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                   \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['UP'][DATA_['UD3'][j-1]] = UD3_D['UP'][DATA_['UD3'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD3_D['DOWN'][DATA_['UD3'][j-1]] = UD3_D['DOWN'][DATA_['UD3'][j-1]] + 1\n",
    "                 \n",
    "                \n",
    "            \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['UP'][DATA_['UD5'][j-1]] = UD5_U['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] < DATA_['MA20'][j] and DATA_['Close'][j] < DATA_['MA40'][j] and DATA_['Close'][j] > DATA_['MA5'][j]:\n",
    "                UD5_U['DOWN'][DATA_['UD5'][j-1]] = UD5_U['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                  \n",
    "            if DATA_['PN_counter'][j] == 1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['UP'][DATA_['UD5'][j-1]] = UD5_D['UP'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "            if DATA_['PN_counter'][j] == -1 and DATA_['Close'][j] > DATA_['MA20'][j] and DATA_['Close'][j] > DATA_['MA40'][j] and DATA_['Close'][j] < DATA_['MA5'][j]:\n",
    "                UD5_D['DOWN'][DATA_['UD5'][j-1]] = UD5_D['DOWN'][DATA_['UD5'][j-1]] + 1\n",
    "                \n",
    "                \n",
    "UD3_U['ACC'] = 0\n",
    "UD3_D['ACC'] = 0\n",
    "UD5_U['ACC'] = 0\n",
    "UD5_D['ACC'] = 0\n",
    "UD3_U['N'] = 0\n",
    "UD3_D['N'] = 0\n",
    "UD5_U['N'] = 0\n",
    "UD5_D['N'] = 0\n",
    "\n",
    "for i in range(5, len(DATA)-1):\n",
    "    \n",
    "    if DATA['Close'][i] < DATA['MA20'][i] and DATA['Close'][i] < DATA['MA40'][i] and DATA['Close'][i] > DATA['MA5'][i]: # expected to U\n",
    "        \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD3_U['UP'][DATA['UD3'][i]] > UD3_U['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_U['ACC'][DATA['UD3'][i]] = UD3_U['ACC'][DATA['UD3'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD3_U['UP'][DATA['UD3'][i]] < UD3_U['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_U['ACC'][DATA['UD3'][i]] = UD3_U['ACC'][DATA['UD3'][i]] + 1\n",
    "            \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD5_U['UP'][DATA['UD5'][i]] > UD5_U['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_U['ACC'][DATA['UD5'][i]] = UD5_U['ACC'][DATA['UD5'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD5_U['UP'][DATA['UD5'][i]] < UD5_U['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_U['ACC'][DATA['UD5'][i]] = UD5_U['ACC'][DATA['UD5'][i]] + 1\n",
    "            \n",
    "        UD3_U['N'][DATA['UD3'][i]] = UD3_U['N'][DATA['UD3'][i]] + 1\n",
    "        UD5_U['N'][DATA['UD5'][i]] = UD5_U['N'][DATA['UD5'][i]] + 1\n",
    "    \n",
    "    elif DATA['Close'][i] > DATA['MA20'][i] and DATA['Close'][i] > DATA['MA40'][i] and DATA['Close'][i] < DATA['MA5'][i]: # expected to D\n",
    "        \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD3_D['UP'][DATA['UD3'][i]] > UD3_D['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_D['ACC'][DATA['UD3'][i]] = UD3_D['ACC'][DATA['UD3'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD3_D['UP'][DATA['UD3'][i]] < UD3_D['DOWN'][DATA['UD3'][i]]:\n",
    "            UD3_D['ACC'][DATA['UD3'][i]] = UD3_D['ACC'][DATA['UD3'][i]] + 1\n",
    "            \n",
    "        if DATA['PN_counter'][i+1] == 1 and UD5_D['UP'][DATA['UD5'][i]] > UD5_D['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_D['ACC'][DATA['UD5'][i]] = UD5_D['ACC'][DATA['UD5'][i]] + 1\n",
    "        \n",
    "        elif DATA['PN_counter'][i+1] == -1 and UD5_D['UP'][DATA['UD5'][i]] < UD5_D['DOWN'][DATA['UD5'][i]]:\n",
    "            UD5_D['ACC'][DATA['UD5'][i]] = UD5_D['ACC'][DATA['UD5'][i]] + 1\n",
    "            \n",
    "        UD3_D['N'][DATA['UD3'][i]] = UD3_D['N'][DATA['UD3'][i]] + 1\n",
    "        UD5_D['N'][DATA['UD5'][i]] = UD5_D['N'][DATA['UD5'][i]] + 1\n",
    "\n",
    "\n",
    "DATA['STATE'] = \"\"\n",
    "interval_size = 96\n",
    "m_i = 5\n",
    "m_i2 = 2\n",
    "for i in range(len(DATA)):\n",
    "\n",
    "    l_state = \"\"\n",
    "    m_state = \"\"\n",
    "    s_state = \"\"\n",
    "    st = \"\"\n",
    "    \n",
    "    if i <= m_i*interval_size:\n",
    "        DATA['STATE'][i] = \"\"\n",
    "\n",
    "    elif i > m_i*interval_size:\n",
    "        \n",
    "        \n",
    "        # long state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) > 0.05:\n",
    "            l_state = \"lu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i*interval_size):i]) < 0:\n",
    "            l_state = \"ld\"\n",
    "\n",
    "\n",
    "        # medium state checks\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) > 0.02:\n",
    "            m_state = \"mu\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-(m_i2*interval_size):i]) < 0:\n",
    "            m_state = \"md\"\n",
    "\n",
    "            \n",
    "        # short state check    \n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) > 0.01:\n",
    "            s_state = \"su\"\n",
    "\n",
    "        if np.mean(DATA['PN_counter'][i-48:i]) < -0.05:\n",
    "            s_state = \"sd\"\n",
    "        \n",
    "        \n",
    "        DATA['STATE'][i] = l_state + m_state + s_state\n",
    "                    \n",
    "\n",
    "\n",
    "for i in range(len(UD3_U)):\n",
    "    s = UD3_U.iloc[i, 0] + UD3_U.iloc[i, 1]\n",
    "    UD3_U.iloc[i, 0] = UD3_U.iloc[i, 0] / s\n",
    "    UD3_U.iloc[i, 1] = UD3_U.iloc[i, 1] / s\n",
    "    UD3_U.iloc[i, 2] = UD3_U.iloc[i, 2] / UD3_U.iloc[i, 3]\n",
    "    \n",
    "    s = UD3_D.iloc[i, 0] + UD3_D.iloc[i, 1]\n",
    "    UD3_D.iloc[i, 0] = UD3_D.iloc[i, 0] / s\n",
    "    UD3_D.iloc[i, 1] = UD3_D.iloc[i, 1] / s\n",
    "    UD3_D.iloc[i, 2] = UD3_D.iloc[i, 2] / UD3_D.iloc[i, 3]\n",
    "    \n",
    "\n",
    "for i in range(len(UD5_U)):\n",
    "    s = UD5_U.iloc[i, 0] + UD5_U.iloc[i, 1]\n",
    "    UD5_U.iloc[i, 0] = UD5_U.iloc[i, 0] / s\n",
    "    UD5_U.iloc[i, 1] = UD5_U.iloc[i, 1] / s\n",
    "    UD5_U.iloc[i, 2] = UD5_U.iloc[i, 2] / UD5_U.iloc[i, 3]\n",
    "    \n",
    "    s = UD5_D.iloc[i, 0] + UD5_D.iloc[i, 1]\n",
    "    UD5_D.iloc[i, 0] = UD5_D.iloc[i, 0] / s\n",
    "    UD5_D.iloc[i, 1] = UD5_D.iloc[i, 1] / s\n",
    "    UD5_D.iloc[i, 2] = UD5_D.iloc[i, 2] / UD5_D.iloc[i, 3]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ce4c1-d61b-4d4c-8dbc-8a456eaca35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TMRW.FINANCE\n",
    "\n",
    "Created on Mon Apr 15 22:44:02 2024\n",
    "\n",
    "Version 1.0.0 bundled on Fri Aug 09 12:00:00 2024\n",
    "\n",
    "@Author: Mark Daniel Balle Brezina\n",
    "@Contributors: Magnus Faber, Jonathan Emil Balle Brezina\n",
    "\"\"\"\n",
    "import TMRW\n",
    "import TMRW.FINANCE as tf\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, date, timedelta, timezone\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import linregress\n",
    "from scipy import integrate\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def sdata(x, y, z):\n",
    "    # x is a stock symbol\n",
    "    # y is a start date\n",
    "    # z is an end date\n",
    "    # kan bruges på commodities, currencies og aktier\n",
    "    \n",
    "    #MSFT er en aktier der kan bruges\n",
    "    #USDEUR=X er en currency der kan bruges\n",
    "    #GC=F er guld priser\n",
    "    \n",
    "    st = pd.DataFrame()\n",
    "    t = yf.Ticker(x)\n",
    "    st = t.history(start=y, end=z)\n",
    "    st.index = pd.to_datetime(st.index).tz_localize(None)\n",
    "    return(st)\n",
    "\n",
    "###############################################################################\n",
    "#                        Moving averages and stds\n",
    "#                        volume weighted averages and stds\n",
    "#                        bollinger bands?\n",
    "###############################################################################\n",
    "\n",
    "# Time-weighted Moving Average\n",
    "def twa(data, window = 20, typ = \"sma\", x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving average\n",
    "    \"\"\"\n",
    "    if typ == \"sma\": # simple moving average\n",
    "        if len(data) < window+1:\n",
    "            raise ValueError(\"not enough data\")\n",
    "            \n",
    "        if x == True:\n",
    "            \n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "                if i < window:\n",
    "                    df.append(data[i])\n",
    "                else:\n",
    "                    df.append(np.mean(data[i-window:i]))\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 0]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "        elif x == False:\n",
    "\n",
    "            df = []\n",
    "            for i in range(len(data)):\n",
    "\n",
    "                ma = np.zeros(window)\n",
    "\n",
    "                for j in range(1,window):\n",
    "\n",
    "                    if i < window:\n",
    "                        ma[j] = data[j]\n",
    "\n",
    "\n",
    "                    if i >= window:\n",
    "\n",
    "                        ma[j] = np.mean(data[i-j:i])\n",
    "\n",
    "                df.append(ma)\n",
    "\n",
    "            df = pd.DataFrame(df, index = data.index)\n",
    "            df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "            \n",
    "            return(df)\n",
    "    \n",
    "    elif typ == \"ema\": # exponential moving average\n",
    "\n",
    "        df = pd.DataFrame(index=data.index, dtype=np.float64)\n",
    "        df.loc[:,0] = data.ewm(span=window, min_periods=0, adjust=True, ignore_na=False).mean().values.flatten() # ????\n",
    "    \n",
    "        return(df)\n",
    "      \n",
    "#Time-Price Moving STD\n",
    "def twstd(data, window = 20, x = True):\n",
    "    \"\"\"\n",
    "    time-weighted moving standard deviation of prices\n",
    "    also called volatility\n",
    "    \"\"\"\n",
    "    if len(data) < window+1:\n",
    "        raise ValueError(\"not enough data\")\n",
    "\n",
    "    if x == True:\n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "            if i < window:\n",
    "                df.append(data[i])\n",
    "            else:\n",
    "                df.append(np.std(data[i-window:i]))\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 0]\n",
    "        \n",
    "        return(df)\n",
    "        \n",
    "    elif x == False:  \n",
    "        \n",
    "        df = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            mstd = np.zeros(window)\n",
    "\n",
    "            for j in range(1,window):\n",
    "\n",
    "                if i >= j:\n",
    "\n",
    "                    mstd[j] = np.std(data[i-j:i])\n",
    "\n",
    "            df.append(mstd)\n",
    "\n",
    "        df = pd.DataFrame(df, index = data.index)\n",
    "        df = df.iloc[0:len(df), 1:len(df.columns)]\n",
    "    \n",
    "        return(df)\n",
    "   \n",
    "    \n",
    "# Volume-weighted Moving Average\n",
    "def vwa(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving average\n",
    "    \"\"\"\n",
    "    \n",
    "    vwa = np.zeros(window)\n",
    "    vwa = list(vwa)\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if i < window:\n",
    "            \n",
    "            vwa[i] = data[i]\n",
    "            \n",
    "        else:\n",
    "    \n",
    "            v_sum = sum(weights[i-window:i])\n",
    "            v_w_avg = 0\n",
    "            for j in range(window):\n",
    "                v_w_avg = v_w_avg + data[i-j] * (weights[i-j] / v_sum)\n",
    "\n",
    "            vwa.append(v_w_avg)\n",
    "    \n",
    "    df = pd.DataFrame(vwa)\n",
    "    return(df)\n",
    "\n",
    "#Volume-weighted Moving STD\n",
    "def vwstd(data, weights, window = 20):\n",
    "    \"\"\"\n",
    "    volume-weighted moving standard deviations\n",
    "    \"\"\"\n",
    "    \n",
    "    v_w_avg = 0\n",
    "    vws = np.zeros(window)\n",
    "    vws = list(vws)\n",
    "    for i in range(window, len(data)):\n",
    "    \n",
    "        v_sum = sum(weights[i-window:i])\n",
    "        v_w_std = 0\n",
    "        for j in range(window):\n",
    "            v_w_std = v_w_std + ((data[i-j] - np.mean(data[i-window:i]))**2 )* (weights[i-j] / v_sum)\n",
    "\n",
    "        vws.append(np.sqrt(v_w_std))\n",
    "    \n",
    "    df = pd.DataFrame(vws)\n",
    "    return(df)\n",
    "\n",
    "###############################################################################\n",
    "#                        Technical indicators\n",
    "#                        RSI, Stochastic Oscillator\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def RSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    delta = data.diff().dropna() # Close_now - Close_yesterday\n",
    "    delta = delta.reset_index(drop = True)\n",
    "\n",
    "    u = pd.DataFrame(np.zeros(len(delta))) # make an array of 0s for the up returns\n",
    "    u = u[0]\n",
    "    d = u.copy() # make an array of 0s for the down returns   \n",
    "\n",
    "    u[delta > 0] = delta[delta > 0] # for all the days where delta is up, transfer them to U\n",
    "    d[delta < 0] = -delta[delta < 0] # for all the days where delta is down, transfer them to D\n",
    "\n",
    "    u[u.index[window-1]] = np.mean( u[:window] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    d[d.index[window-1]] = np.mean( d[:window] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(window-1)]) #drop the days before the window opens\n",
    "\n",
    "    RS = pd.DataFrame.ewm(u, com=window-1, adjust=False).mean() / pd.DataFrame.ewm(d, com=window-1, adjust=False).mean() # EMA(up) / EMA(down)\n",
    "\n",
    "    RSI_ = 100 - (100 / (1 + RS))\n",
    "    return(RSI_)\n",
    "\n",
    "def FRSI(data, window = 20):\n",
    "    \"\"\"\n",
    "    inverse fisher transform on RSI = 0.1*(rsi-50)\n",
    "    fisher rsi = (np.exp(2*rsi)-1) / (np.exp(2*rsi)+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    RSI_ = 0.1 * (RSI(data, window) - 50)\n",
    "    F_RSI = (np.exp(2*RSI_)-1) / (np.exp(2*RSI_)+1)\n",
    "    return(F_RSI)\n",
    "\n",
    "def BB(data, window = 20, std = 2.5):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    mean_lst = np.zeros(len(data))\n",
    "    std_lst = np.zeros(len(data))\n",
    "    for i in range(0, window):\n",
    "        mean_lst[i] = data[i]\n",
    "        std_lst[i] = 0.05 * data[i]\n",
    "    \n",
    "    for i in range(window,len(data)):\n",
    "        mean_lst[i] = np.mean(data[i-window:i])\n",
    "        std_lst[i] = np.std(data[i-window:i])\n",
    "        \n",
    "    up = mean_lst + std * std_lst\n",
    "    down = mean_lst - std * std_lst\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Upper'] = up\n",
    "    df['Lower'] = down\n",
    "    return(df)\n",
    "\n",
    "def STO(data, N=14, M=3):\n",
    "    assert 'Low' in data.columns\n",
    "    assert 'High' in data.columns\n",
    "    assert 'Close' in data.columns\n",
    "    \n",
    "    data_ = pd.DataFrame()\n",
    "    data_['low_N'] = data['Low'].rolling(N).min()\n",
    "    data_['high_N'] = data['High'].rolling(N).max()\n",
    "    data_['K'] = 100 * (data['Close'] - data_['low_N']) / \\\n",
    "        (data_['high_N'] - data_['low_N']) # The stochastic oscillator\n",
    "    data_['D'] = data_['K'].rolling(M).mean() # the slow and smoothed K\n",
    "    return data_\n",
    "\n",
    "def ADX(high, low, close, lookback):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.rolling(lookback).mean()\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha = 1/lookback).mean() / atr)\n",
    "    minus_di = abs(100 * (minus_dm.ewm(alpha = 1/lookback).mean() / atr))\n",
    "    dx = (abs(plus_di - minus_di) / abs(plus_di + minus_di)) * 100\n",
    "    adx = ((dx.shift(1) * (lookback - 1)) + dx) / lookback\n",
    "    adx_smooth = adx.ewm(alpha = 1/lookback).mean()\n",
    "    df['+DI'] = plus_di\n",
    "    df['-DI'] = minus_di\n",
    "    df['ADX'] = adx_smooth\n",
    "    \n",
    "    return df\n",
    "\n",
    "def MACD(price, slow, fast, smooth):\n",
    "    exp1 = price.ewm(span = fast, adjust = False).mean()\n",
    "    exp2 = price.ewm(span = slow, adjust = False).mean()\n",
    "    macd = pd.DataFrame(exp1 - exp2).rename(columns = {'Close':'macd'})\n",
    "    signal = pd.DataFrame(macd.ewm(span = smooth, adjust = False).mean()).rename(columns = {'macd':'signal'})\n",
    "    hist = pd.DataFrame(macd['macd'] - signal['signal']).rename(columns = {0:'hist'})\n",
    "    frames =  [macd, signal, hist]\n",
    "    df = pd.concat(frames, join = 'inner', axis = 1)\n",
    "    return df\n",
    "\n",
    "def SuperTrend(high, low, close, lookback, multiplier):\n",
    "    # ATR\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    tr1 = pd.DataFrame(high - low)\n",
    "    tr2 = pd.DataFrame(abs(high - close.shift(1)))\n",
    "    tr3 = pd.DataFrame(abs(low - close.shift(1)))\n",
    "    frames = [tr1, tr2, tr3]\n",
    "    tr = pd.concat(frames, axis = 1, join = 'inner').max(axis = 1)\n",
    "    atr = tr.ewm(lookback).mean()\n",
    "    \n",
    "    # H/L AVG AND BASIC UPPER & LOWER BAND\n",
    "    \n",
    "    hl_avg = (high + low) / 2\n",
    "    upper_band = (hl_avg + multiplier * atr).dropna()\n",
    "    lower_band = (hl_avg - multiplier * atr).dropna()\n",
    "    \n",
    "    # FINAL UPPER BAND\n",
    "    \n",
    "    final_bands = pd.DataFrame(columns = ['upper', 'lower'])\n",
    "    final_bands.iloc[:,0] = [x for x in upper_band - upper_band]\n",
    "    final_bands.iloc[:,1] = final_bands.iloc[:,0]\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i,0] = 0\n",
    "        else:\n",
    "            if (upper_band[i] < final_bands.iloc[i-1,0]) | (close[i-1] > final_bands.iloc[i-1,0]):\n",
    "                final_bands.iloc[i,0] = upper_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,0] = final_bands.iloc[i-1,0]\n",
    "    \n",
    "    # FINAL LOWER BAND\n",
    "    \n",
    "    for i in range(len(final_bands)):\n",
    "        if i == 0:\n",
    "            final_bands.iloc[i, 1] = 0\n",
    "        else:\n",
    "            if (lower_band[i] > final_bands.iloc[i-1,1]) | (close[i-1] < final_bands.iloc[i-1,1]):\n",
    "                final_bands.iloc[i,1] = lower_band[i]\n",
    "            else:\n",
    "                final_bands.iloc[i,1] = final_bands.iloc[i-1,1]\n",
    "    \n",
    "    # SUPERTREND\n",
    "    \n",
    "    supertrend = pd.DataFrame(columns = [f'supertrend_{lookback}'])\n",
    "    supertrend.iloc[:,0] = [x for x in final_bands['upper'] - final_bands['upper']]\n",
    "    \n",
    "    for i in range(len(supertrend)):\n",
    "        if i == 0:\n",
    "            supertrend.iloc[i, 0] = 0\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] < final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 0] and close[i] > final_bands.iloc[i, 0]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] > final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 1]\n",
    "        elif supertrend.iloc[i-1, 0] == final_bands.iloc[i-1, 1] and close[i] < final_bands.iloc[i, 1]:\n",
    "            supertrend.iloc[i, 0] = final_bands.iloc[i, 0]\n",
    "    \n",
    "    supertrend = supertrend.set_index(upper_band.index)\n",
    "    supertrend = supertrend.dropna()[1:]\n",
    "    \n",
    "    # ST UPTREND/DOWNTREND\n",
    "    \n",
    "    upt = []\n",
    "    dt = []\n",
    "    close = close.iloc[len(close) - len(supertrend):]\n",
    "\n",
    "    for i in range(len(supertrend)):\n",
    "        if close[i] > supertrend.iloc[i, 0]:\n",
    "            upt.append(supertrend.iloc[i, 0])\n",
    "            dt.append(np.nan)\n",
    "        elif close[i] < supertrend.iloc[i, 0]:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(supertrend.iloc[i, 0])\n",
    "        else:\n",
    "            upt.append(np.nan)\n",
    "            dt.append(np.nan)\n",
    "            \n",
    "    st, upt, dt = pd.Series(supertrend.iloc[:, 0]), pd.Series(upt), pd.Series(dt)\n",
    "    upt.index, dt.index = supertrend.index, supertrend.index\n",
    "    \n",
    "    df['ST'] = st\n",
    "    df['UPT'] = upt\n",
    "    df['DT'] = dt\n",
    "    \n",
    "    return df\n",
    "\n",
    "def PRICE_plot(data, MA = False):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 3))\n",
    "    #ax = plt.gca()\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    ax1.plot(data.index, data.Close, color = \"deepskyblue\")\n",
    "    #ax1.set_xticks([])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def STO_plot(data):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    ax1.plot(data.index, data.D, color = \"deepskyblue\")\n",
    "    ax1.plot(data.index, np.repeat(80,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    ax1.plot(data.index, np.repeat(20,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    plt.show()\n",
    "\n",
    "def ADX_plot(data, components = False):    \n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    if components == False:    \n",
    "        ax1.plot(data.index, data['ADX'], color = \"deepskyblue\")\n",
    "    \n",
    "    elif components == True:\n",
    "        ax1.plot(data.index, data['-DI'], color = \"r\")\n",
    "        ax1.plot(data.index, data['+DI'], color = \"g\")\n",
    "    \n",
    "    ax1.plot(data.index, np.repeat(40,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    ax1.plot(data.index, np.repeat(20,len(data)), color = \"white\", linestyle = \"dotted\")\n",
    "    plt.show()\n",
    "\n",
    "def MACD_plot(prices, macd, signal, hist):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 1))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "\n",
    "    for i in range(len(prices)):\n",
    "        if str(hist[i])[0] == '-':\n",
    "            ax1.bar(prices.index[i], hist[i], color = 'gold')\n",
    "        else:\n",
    "            ax1.bar(prices.index[i], hist[i], color = 'deepskyblue')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def TRADE_plot(strategy):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(23, 12))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    ax1.plot(strategy.index, strategy['Close'], linewidth = 2, color = \"deepskyblue\")\n",
    "    \n",
    "    buy_price = strategy_df[strategy_df['position'] == 1]\n",
    "    sell_price = strategy_df[strategy_df['position'] == -1]\n",
    "    short_price = strategy_df[strategy_df['position'] == -2]\n",
    "    \n",
    "    ax1.plot(buy_price.index, buy_price['Close'], marker = 'o', color = 'lime', markersize = 8, linewidth = 0)\n",
    "    ax1.plot(sell_price.index, sell_price['Close'], marker = 'X', color = 'tomato', markersize = 8, linewidth = 0)\n",
    "    ax1.plot(short_price.index, short_price['Close'], marker = 'X', color = 'purple', markersize = 8, linewidth = 0)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# very useful   \n",
    "def SUM_plot(data, strategy):\n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(23, 12))\n",
    "    ax1.set_facecolor('dimgrey')\n",
    "    \n",
    "    rets = data.Close / data.Close[0]\n",
    "    \n",
    "    \n",
    "    #ax1.plot(rets.index, rets, color = 'deepskyblue', linewidth = 2)\n",
    "    #ax1.plot(data.index, rets, color = 'deepskyblue', linewidth = 2)\n",
    "    ax1.plot(strategy.index, np.repeat(1, len(strategy)), color = 'r', linewidth = 2)\n",
    "    ax1.plot(strategy.index, strategy.acc, color = 'w', linewidth = 2)\n",
    "    plt.grid()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b7f22-9d29-4af1-a2d1-8c96b2dbb95f",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517af357-9f0e-4dce-90a7-d6ab59dc9775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0aff072-4ee7-4549-b75b-d1e89198468a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.79221005947977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('full_timeline_summary.xlsx', sheet_name='signals')\n",
    "df = df[[\"Date Signal\", \"Ticker\", \"Side\", \"Price\", \"Proportion\"]]\n",
    "\n",
    "delta = 0\n",
    "j = i\n",
    "\n",
    "for i in range(len(df)):\n",
    "    d1 = df[\"Date Signal\"][i].strftime(\"%Y-%m-%d\")\n",
    "    d2 = df[\"Date Signal\"][i] + timedelta(days=1)\n",
    "    d2 = d2.strftime(\"%Y-%m-%d\")\n",
    "    a = get_data(df[\"Ticker\"][i], d1, d2)\n",
    "    if df[\"Price\"][i] < a['Low'][0] or df[\"Price\"][i] > a['High'][0]:\n",
    "        diff = max(abs(df[\"Price\"][i]-a[\"Low\"][0]), abs(df[\"Price\"][i] - a[\"High\"][0]))\n",
    "        if diff > delta:\n",
    "            delta = diff\n",
    "            j = i\n",
    "\n",
    "        \n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "838a2850-9156-4b8b-8925-2594b96659f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117167.14330690293"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('full_timeline_summary.xlsx', sheet_name='signals')\n",
    "df = df[[\"Date Signal\", \"Ticker\", \"Side\", \"Price\", \"Proportion\"]]\n",
    "\n",
    "ret = pd.DataFrame(index = df[\"Ticker\"].unique(), columns = [\"Price\", \"Amount\"])\n",
    "for i in range(len(ret)):\n",
    "    ret.iloc[i,0] = 0\n",
    "    ret.iloc[i,1] = 0\n",
    "\n",
    "acc_ret = 1\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    if df[\"Side\"][i] == \"buy\":\n",
    "        ret.loc[df[\"Ticker\"][i]][0] = df[\"Price\"][i]\n",
    "        ret.loc[df[\"Ticker\"][i]][1] = df[\"Proportion\"][i]\n",
    "        \n",
    "        #print(ret.loc[df[\"Ticker\"][i]][0], ret.loc[df[\"Ticker\"][i]][1])\n",
    "        \n",
    "    elif df[\"Side\"][i] == \"sell\" and ret.loc[df[\"Ticker\"][i]][1] > 0:\n",
    "        returns = (df[\"Price\"][i] - ret.loc[df[\"Ticker\"][i]][0]) / ret.loc[df[\"Ticker\"][i]][0]\n",
    "        \n",
    "        acc_ret = acc_ret *(1 + returns * ret.loc[df[\"Ticker\"][i]][1])\n",
    "        ret.loc[df[\"Ticker\"][i]][0] = 0\n",
    "        ret.loc[df[\"Ticker\"][i]][1] = 0\n",
    "        \n",
    "acc_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d40259c-c92b-4dde-8aa4-5f3847b67623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(x, y, z):\n",
    "    # x is a stock symbol\n",
    "    # y is a start date\n",
    "    # z is an end date\n",
    "    # kan bruges på commodities, currencies og aktier\n",
    "    \n",
    "    #MSFT er en aktier der kan bruges\n",
    "    #USDEUR=X er en currency der kan bruges\n",
    "    #GC=F er guld priser\n",
    "    \n",
    "    st = pd.DataFrame()\n",
    "    t = yf.Ticker(x)\n",
    "    st = t.history(start=y, end=z)\n",
    "    st.index = pd.to_datetime(st.index).tz_localize(None)\n",
    "    return(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "16883805-1d4b-4034-9d77-5edd1f253ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe(ret, vol):\n",
    "    \n",
    "    return((ret-0.05)/vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ad55571-8183-4e96-b11b-2b57aa8bbefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CAGR(vend,vstart,years):\n",
    "    return( (vend/vstart)**(1/years) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f04736d3-039b-4b84-a076-689b8baa8f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3434613699957545"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAGR(3203258898.0586734,100000,8.595238)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed66d12-3448-4425-8d5d-e0bc04eb3568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e1f03d81-1dda-4f31-a14d-4699dc10cf35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0714285714285716"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharpe(1.34,0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "67e325e9-5466-4fce-9535-b375a7656236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.116279069767442"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharpe(3.11,0.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27414c6f-bbed-405b-be76-0395ad433ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def cum_ret(rets):\n",
    "    summ = 0\n",
    "    for i in range(len(rets)):\n",
    "        summ = summ + np.log(1 + rets[i])\n",
    "    ret = np.exp(summ) - 1\n",
    "    return(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5fe185-c680-4ed3-ac70-2e496ad07f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('full_timeline_summary.xlsx', sheet_name='performance')\n",
    "df = df[[\"Date\", \"Net Worth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "376569a6-ac57-4da8-b7cd-8db12bab6601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Brezina\\AppData\\Local\\Temp\\ipykernel_17372\\3602338428.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Returns\"][0] = 0\n"
     ]
    }
   ],
   "source": [
    "df[\"Returns\"] = (df[\"Net Worth\"] - df[\"Net Worth\"].shift(1)) / df[\"Net Worth\"].shift(1)\n",
    "df[\"Returns\"][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f677903d-31e6-4e8e-8302-aa3dec4a8f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.595238095238095"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)/252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af3cf20a-ac41-441e-85b6-175ef481c1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32032.588980586734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32032.58898058747"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets = list(df[\"Returns\"][2:])\n",
    "\n",
    "acc = 1\n",
    "for i in range(len(rets)):\n",
    "    acc = acc * (1+rets[i])\n",
    "    \n",
    "\n",
    "print(acc-1)\n",
    "\n",
    "\n",
    "cum_ret(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40837ee-005f-49c6-b11c-47e81b495257",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Signal</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Side</th>\n",
       "      <th>Price</th>\n",
       "      <th>Proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-19 22:00:00</td>\n",
       "      <td>SHW</td>\n",
       "      <td>buy</td>\n",
       "      <td>87.22</td>\n",
       "      <td>0.0805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-24 22:00:00</td>\n",
       "      <td>ALNY</td>\n",
       "      <td>buy</td>\n",
       "      <td>37.62</td>\n",
       "      <td>0.0653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-25 22:00:00</td>\n",
       "      <td>IRM</td>\n",
       "      <td>buy</td>\n",
       "      <td>21.55</td>\n",
       "      <td>0.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-25 22:00:00</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>buy</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.6109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-31 22:00:00</td>\n",
       "      <td>ALNY</td>\n",
       "      <td>sell</td>\n",
       "      <td>39.99</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>2025-08-25 22:00:00</td>\n",
       "      <td>MTDR</td>\n",
       "      <td>sell</td>\n",
       "      <td>49.39</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2025-08-26 22:00:00</td>\n",
       "      <td>CF</td>\n",
       "      <td>sell</td>\n",
       "      <td>86.63</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>2025-08-26 22:00:00</td>\n",
       "      <td>ENB</td>\n",
       "      <td>buy</td>\n",
       "      <td>47.44</td>\n",
       "      <td>0.0538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>2025-08-27 22:00:00</td>\n",
       "      <td>AXP</td>\n",
       "      <td>sell</td>\n",
       "      <td>322.46</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>2025-08-27 22:00:00</td>\n",
       "      <td>CF</td>\n",
       "      <td>buy</td>\n",
       "      <td>86.35</td>\n",
       "      <td>0.0856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2819 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Signal Ticker  Side   Price  Proportion\n",
       "0    2017-01-19 22:00:00    SHW   buy   87.22      0.0805\n",
       "1    2017-01-24 22:00:00   ALNY   buy   37.62      0.0653\n",
       "2    2017-01-25 22:00:00    IRM   buy   21.55      0.0250\n",
       "3    2017-01-25 22:00:00   NVDA   buy    2.65      0.6109\n",
       "4    2017-01-31 22:00:00   ALNY  sell   39.99      1.0000\n",
       "...                  ...    ...   ...     ...         ...\n",
       "2814 2025-08-25 22:00:00   MTDR  sell   49.39      1.0000\n",
       "2815 2025-08-26 22:00:00     CF  sell   86.63      1.0000\n",
       "2816 2025-08-26 22:00:00    ENB   buy   47.44      0.0538\n",
       "2817 2025-08-27 22:00:00    AXP  sell  322.46      1.0000\n",
       "2818 2025-08-27 22:00:00     CF   buy   86.35      0.0856\n",
       "\n",
       "[2819 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('full_timeline_summary.xlsx', sheet_name='signals')\n",
    "df = df[[\"Date Signal\", \"Ticker\", \"Side\", \"Price\", \"Proportion\"]]\n",
    "\n",
    "ret = pd.DataFrame(index = df[\"Ticker\"].unique(), columns = [\"Price\", \"Amount\"])\n",
    "for i in range(len(ret)):\n",
    "    ret.iloc[i,0] = 0\n",
    "    ret.iloc[i,1] = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    if df[\"Side\"][i] == \"buy\":\n",
    "        ret.loc[df[\"Ticker\"][i]][0] = df[\"Price\"][i]\n",
    "        ret.loc[df[\"Ticker\"][i]][1] = ret.loc[df[\"Ticker\"][i]][1] + df[\"Proportion\"][i] * (1-0.0035)\n",
    "        \n",
    "        #print(ret.loc[df[\"Ticker\"][i]][0], ret.loc[df[\"Ticker\"][i]][1])\n",
    "        \n",
    "    elif df[\"Side\"][i] == \"sell\" and ret.loc[df[\"Ticker\"][i]][1] > 0:\n",
    "        returns = (df[\"Price\"][i] - ret.loc[df[\"Ticker\"][i]][0]) / ret.loc[df[\"Ticker\"][i]][0]\n",
    "        \n",
    "        acc_ret = acc_ret *(1 + returns * ret.loc[df[\"Ticker\"][i]][1])\n",
    "        ret.loc[df[\"Ticker\"][i]][0] = 0\n",
    "        ret.loc[df[\"Ticker\"][i]][1] = 0\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d679253-163e-4a02-9fca-19784c39a1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SHW</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALNY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDXX</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EQIX</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDAQ</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RYAAY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSBC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Price Amount\n",
       "SHW       0      0\n",
       "ALNY      0      0\n",
       "IRM       0      0\n",
       "NVDA      0      0\n",
       "IDXX      0      0\n",
       "...     ...    ...\n",
       "EQIX      0      0\n",
       "NDAQ      0      0\n",
       "RYAAY     0      0\n",
       "GOOGL     0      0\n",
       "HSBC      0      0\n",
       "\n",
       "[86 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f21131-efdf-42e9-98bd-824cc12f4dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
